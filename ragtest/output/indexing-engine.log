15:49:08,654 graphrag.index.cli INFO Logging enabled at C:\Other\CSCI 566\graphrag\ragtest\output\indexing-engine.log
15:49:08,654 graphrag.index.cli INFO Starting pipeline run for: 20241030-154908, dryrun=False
15:49:08,654 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Other\\CSCI 566\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Other\\CSCI 566\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Other\\CSCI 566\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
15:49:08,672 graphrag.index.create_pipeline_config INFO skipping workflows 
15:49:08,672 graphrag.index.run.run INFO Running pipeline
15:49:08,672 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Other\CSCI 566\graphrag\ragtest\output
15:49:08,672 graphrag.index.input.load_input INFO loading input from root_dir=input
15:49:08,672 graphrag.index.input.load_input INFO using file storage for input
15:49:08,685 graphrag.index.storage.file_pipeline_storage INFO search C:\Other\CSCI 566\graphrag\ragtest\input for files matching .*\.txt$
15:49:08,685 graphrag.index.input.text INFO found text files from input, found [("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
15:49:08,685 graphrag.index.input.text INFO Found 1 files, loading 1
15:49:08,701 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
15:49:08,701 graphrag.index.run.run INFO Final # of rows loaded: 1
15:49:08,858 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
15:49:08,858 datashaper.workflow.workflow INFO executing verb orderby
15:49:08,874 datashaper.workflow.workflow INFO executing verb zip
15:49:08,874 datashaper.workflow.workflow INFO executing verb aggregate_override
15:49:08,889 datashaper.workflow.workflow INFO executing verb chunk
15:49:09,78 datashaper.workflow.workflow INFO executing verb select
15:49:09,78 datashaper.workflow.workflow INFO executing verb unroll
15:49:09,93 datashaper.workflow.workflow INFO executing verb rename
15:49:09,93 datashaper.workflow.workflow INFO executing verb genid
15:49:09,93 datashaper.workflow.workflow INFO executing verb unzip
15:49:09,109 datashaper.workflow.workflow INFO executing verb copy
15:49:09,109 datashaper.workflow.workflow INFO executing verb filter
15:49:09,125 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
15:49:09,313 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
15:49:09,313 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
15:49:09,360 datashaper.workflow.workflow INFO executing verb entity_extract
15:49:09,360 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
15:49:09,486 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=0, RPM=0
15:49:09,486 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
15:49:15,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:15,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.342999999993481. input_tokens=2936, output_tokens=397
15:49:17,169 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:17,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.609000000054948. input_tokens=2936, output_tokens=480
15:49:17,326 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:17,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.765999999945052. input_tokens=2936, output_tokens=485
15:49:17,457 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:17,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.905999999959022. input_tokens=2935, output_tokens=493
15:49:17,982 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:17,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.469000000040978. input_tokens=2936, output_tokens=495
15:49:18,29 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:18,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.484000000054948. input_tokens=2936, output_tokens=527
15:49:18,328 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:18,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.765999999945052. input_tokens=2937, output_tokens=565
15:49:18,328 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:18,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.812999999965541. input_tokens=2936, output_tokens=558
15:49:18,360 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:18,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.781000000075437. input_tokens=2936, output_tokens=567
15:49:19,216 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:19,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.670999999972992. input_tokens=2936, output_tokens=590
15:49:19,722 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:19,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.170999999972992. input_tokens=2935, output_tokens=661
15:49:19,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:19,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.436999999918044. input_tokens=2936, output_tokens=664
15:49:20,117 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:20,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.562000000034459. input_tokens=2936, output_tokens=674
15:49:20,169 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:20,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.640999999945052. input_tokens=2934, output_tokens=679
15:49:20,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:20,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.952999999979511. input_tokens=2936, output_tokens=686
15:49:20,742 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:20,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.187000000034459. input_tokens=2935, output_tokens=716
15:49:20,868 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:20,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.35999999998603. input_tokens=2936, output_tokens=735
15:49:21,773 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:21,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.233999999938533. input_tokens=2937, output_tokens=785
15:49:21,869 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:21,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.35999999998603. input_tokens=2934, output_tokens=821
15:49:22,277 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:22,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.766000000061467. input_tokens=2936, output_tokens=818
15:49:22,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:22,382 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:22,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:22,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.952999999979511. input_tokens=2936, output_tokens=820
15:49:22,649 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:22,649 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.108999999938533. input_tokens=2935, output_tokens=840
15:49:22,720 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:22,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.155999999959022. input_tokens=2936, output_tokens=863
15:49:22,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:22,837 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:23,594 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:23,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.09299999999348. input_tokens=2936, output_tokens=866
15:49:23,614 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:23,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.422000000020489. input_tokens=2936, output_tokens=498
15:49:23,725 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:23,725 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:23,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:23,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.358999999938533. input_tokens=2936, output_tokens=893
15:49:24,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:24,215 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:24,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:24,389 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:24,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:24,415 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.078000000095926. input_tokens=2936, output_tokens=557
15:49:24,523 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:24,523 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:24,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:24,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.047000000020489. input_tokens=2936, output_tokens=583
15:49:25,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:25,323 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:25,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:25,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.156000000075437. input_tokens=2936, output_tokens=573
15:49:25,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:25,959 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:26,643 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:26,643 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:26,952 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:26,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.625. input_tokens=2936, output_tokens=683
15:49:27,15 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:27,15 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.983999999938533. input_tokens=2936, output_tokens=700
15:49:27,127 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:27,127 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:27,443 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:27,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.077999999979511. input_tokens=2935, output_tokens=612
15:49:27,491 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:27,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.594000000040978. input_tokens=34, output_tokens=264
15:49:27,620 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:27,620 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:27,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:27,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.125. input_tokens=2935, output_tokens=645
15:49:27,972 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:27,972 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:28,4 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:28,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.015999999945052. input_tokens=2936, output_tokens=798
15:49:28,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:28,416 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:28,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:28,917 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:29,226 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:29,226 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:29,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:29,540 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:30,329 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:30,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.218999999924563. input_tokens=2937, output_tokens=711
15:49:30,441 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:30,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.5. input_tokens=34, output_tokens=303
15:49:30,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:30,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.062999999965541. input_tokens=34, output_tokens=285
15:49:30,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:30,789 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:30,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:30,805 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:31,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:31,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.437999999965541. input_tokens=2936, output_tokens=834
15:49:31,796 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:31,796 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:31,812 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:31,812 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.219000000040978. input_tokens=2937, output_tokens=560
15:49:31,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:31,906 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:31,925 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:31,925 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:31,953 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:31,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.0. input_tokens=34, output_tokens=305
15:49:32,0 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:32,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.672000000020489. input_tokens=2936, output_tokens=937
15:49:32,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:32,79 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:32,133 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:32,135 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:32,182 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:32,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.312000000034459. input_tokens=2936, output_tokens=729
15:49:32,292 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:32,292 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:32,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:32,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.594000000040978. input_tokens=2935, output_tokens=741
15:49:32,628 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:32,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.10999999998603. input_tokens=2935, output_tokens=849
15:49:32,728 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:32,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.297000000020489. input_tokens=34, output_tokens=335
15:49:32,846 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:32,846 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:33,238 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:33,239 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:33,461 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:33,461 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:33,667 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:33,667 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:33,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:33,698 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:33,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:33,857 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:34,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:34,19 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:34,464 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:34,465 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:34,548 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:34,549 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:34,551 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:34,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.547000000020489. input_tokens=34, output_tokens=432
15:49:34,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:34,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.297000000020489. input_tokens=2936, output_tokens=800
15:49:35,2 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:35,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 8.15700000000652. input_tokens=2935, output_tokens=560
15:49:35,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:35,117 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:35,206 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:35,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.23499999998603. input_tokens=2936, output_tokens=1020
15:49:35,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:35,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.359000000054948. input_tokens=2934, output_tokens=1023
15:49:35,648 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:35,648 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:35,748 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:35,764 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.280999999959022. input_tokens=2936, output_tokens=869
15:49:35,911 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:35,911 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:35,992 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:35,992 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:36,175 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:36,175 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:36,286 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:36,286 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:36,386 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:36,396 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:36,588 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:36,588 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:36,829 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:36,829 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:37,564 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:37,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.843999999924563. input_tokens=2937, output_tokens=1130
15:49:37,697 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:37,697 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:37,750 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:37,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.312999999965541. input_tokens=34, output_tokens=326
15:49:37,868 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:37,868 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:38,2 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:38,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.640999999945052. input_tokens=34, output_tokens=250
15:49:38,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:38,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.577999999979511. input_tokens=34, output_tokens=306
15:49:38,251 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:38,255 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:38,372 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:38,372 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:39,136 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:39,136 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:39,231 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:39,231 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:39,344 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:39,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.14000000001397. input_tokens=34, output_tokens=344
15:49:39,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:39,427 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:39,460 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:39,461 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:39,477 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:39,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.51500000001397. input_tokens=34, output_tokens=386
15:49:39,818 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:39,818 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:40,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:40,60 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.18700000003446. input_tokens=2936, output_tokens=1007
15:49:40,185 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:40,185 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:40,206 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:40,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.437999999965541. input_tokens=34, output_tokens=364
15:49:40,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:40,331 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:40,393 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:40,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.76500000001397. input_tokens=34, output_tokens=394
15:49:40,806 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:40,806 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:41,102 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:41,102 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:41,351 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:41,352 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:41,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:41,502 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:41,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:41,573 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:41,732 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:41,732 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:41,946 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:41,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.015999999945052. input_tokens=34, output_tokens=336
15:49:42,54 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:42,54 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:42,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:42,85 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:42,117 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:42,117 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.109000000054948. input_tokens=34, output_tokens=294
15:49:42,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:42,368 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:42,384 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:42,399 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:42,509 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:42,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.172000000020489. input_tokens=34, output_tokens=683
15:49:42,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:42,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.140999999945052. input_tokens=34, output_tokens=290
15:49:42,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:42,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.187999999965541. input_tokens=34, output_tokens=435
15:49:42,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:42,761 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:42,902 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:42,902 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:43,170 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:43,170 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 5.60999999998603. input_tokens=34, output_tokens=367
15:49:43,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:43,531 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:43,657 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:43,657 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:44,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:44,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 12.625. input_tokens=2936, output_tokens=870
15:49:44,239 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:44,239 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:44,285 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:44,289 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:44,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:44,531 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:44,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:44,705 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:45,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:45,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.484000000054948. input_tokens=34, output_tokens=270
15:49:45,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:45,492 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:45,752 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:45,757 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:45,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:45,942 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:46,370 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:46,370 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:46,417 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:46,417 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:46,433 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:46,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.375. input_tokens=34, output_tokens=343
15:49:47,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:47,95 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:47,162 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:47,162 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:47,490 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:47,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.312000000034459. input_tokens=34, output_tokens=265
15:49:47,810 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:47,814 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:47,922 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:47,922 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:48,276 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:48,276 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:48,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:48,736 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:49,408 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:49,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.906000000075437. input_tokens=34, output_tokens=443
15:49:49,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:49,583 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:49,833 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:49,833 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:50,241 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:50,241 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:50,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:50,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.922000000020489. input_tokens=34, output_tokens=320
15:49:50,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:50,402 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:50,876 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:50,876 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:51,447 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:51,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.062999999965541. input_tokens=34, output_tokens=559
15:49:51,562 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:51,563 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:51,690 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:51,691 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:51,827 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:51,827 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:51,943 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:51,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.827999999979511. input_tokens=2936, output_tokens=661
15:49:52,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:52,55 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:52,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:52,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 8.390999999945052. input_tokens=34, output_tokens=336
15:49:52,627 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:52,627 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:52,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:52,736 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:53,22 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:53,22 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:53,460 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:53,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.015999999945052. input_tokens=34, output_tokens=486
15:49:53,574 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:53,575 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:53,971 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:53,979 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:54,122 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:54,122 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:54,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:54,656 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:54,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:54,820 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:55,322 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:55,323 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:55,644 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:55,644 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:55,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:55,978 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:56,645 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:56,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.17200000002049. input_tokens=2935, output_tokens=788
15:49:56,780 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:56,781 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:56,868 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:56,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 3.437000000034459. input_tokens=34, output_tokens=248
15:49:57,289 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:57,293 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:57,322 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:57,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 7.093999999924563. input_tokens=34, output_tokens=414
15:49:57,434 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:57,434 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:57,830 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:57,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.343999999924563. input_tokens=2936, output_tokens=736
15:49:58,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:58,46 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:58,146 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:58,147 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:58,232 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:58,233 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:58,400 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:58,401 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:58,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:58,843 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:58,914 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:58,914 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:59,693 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:49:59,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 7.312000000034459. input_tokens=34, output_tokens=517
15:49:59,710 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:59,710 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:59,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:59,801 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:59,814 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:59,814 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:49:59,980 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:49:59,980 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:00,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:00,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.015999999945052. input_tokens=2935, output_tokens=774
15:50:00,534 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:00,534 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:00,873 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:00,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 9.51500000001397. input_tokens=2936, output_tokens=666
15:50:00,996 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:00,996 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:01,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:01,377 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:01,417 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:01,417 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:01,862 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:01,862 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:01,952 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:01,952 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:02,126 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:02,126 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:02,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:02,357 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:02,861 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:02,862 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:04,245 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:04,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 4.015999999945052. input_tokens=34, output_tokens=281
15:50:04,292 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:04,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.437999999965541. input_tokens=2936, output_tokens=565
15:50:04,355 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:04,355 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:04,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:04,418 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:04,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:04,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 7.920999999972992. input_tokens=34, output_tokens=591
15:50:04,771 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:04,772 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:05,5 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:05,6 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:05,22 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:05,22 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:05,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:05,413 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:05,696 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:05,696 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:05,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:05,908 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:06,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:06,90 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:06,336 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:06,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.5. input_tokens=2936, output_tokens=648
15:50:06,751 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:06,751 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:06,905 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:06,905 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:07,249 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:07,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 6.375. input_tokens=34, output_tokens=463
15:50:07,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:07,402 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:08,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:08,167 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:08,180 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:08,180 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:08,606 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:08,606 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:08,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:08,959 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:09,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:09,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 5.23499999998603. input_tokens=34, output_tokens=380
15:50:09,564 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:09,564 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:09,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:09,826 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:09,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:09,886 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:10,97 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:10,105 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:11,62 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:11,62 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:12,106 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:12,106 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:14,392 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:14,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 14.905999999959022. input_tokens=2936, output_tokens=1092
15:50:14,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:14,515 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:14,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:14,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.312999999965541. input_tokens=2935, output_tokens=384
15:50:14,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:14,876 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:15,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:15,123 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:16,130 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:16,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 4.76500000001397. input_tokens=34, output_tokens=295
15:50:16,284 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:16,288 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:16,317 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:16,317 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:16,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:16,501 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:16,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:16,872 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:17,137 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:17,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.655999999959022. input_tokens=2936, output_tokens=818
15:50:17,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:17,529 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:17,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:17,622 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:17,644 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:17,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.312000000034459. input_tokens=2936, output_tokens=710
15:50:17,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:17,790 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:18,304 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:18,304 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:18,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:18,305 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:18,796 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:18,797 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:18,979 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:18,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 10.641000000061467. input_tokens=2936, output_tokens=724
15:50:19,68 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:19,69 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:19,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:19,97 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:19,943 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:19,944 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:20,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:20,174 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:20,871 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:20,872 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:21,191 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:21,192 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:21,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:21,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 27.140999999945052. input_tokens=34, output_tokens=1829
15:50:21,689 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:21,690 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:21,984 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:21,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.734000000054948. input_tokens=2936, output_tokens=911
15:50:22,227 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:22,228 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 18.0. input_tokens=2936, output_tokens=1100
15:50:22,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:22,337 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:23,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:23,533 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:23,715 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:23,715 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:24,2 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:24,3 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:25,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:25,140 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:25,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:25,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 5.031000000075437. input_tokens=34, output_tokens=235
15:50:25,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:25,284 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:25,299 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:25,299 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:25,400 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:25,401 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.688000000081956. input_tokens=2936, output_tokens=465
15:50:25,716 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:25,716 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:26,329 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:26,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 7.437999999965541. input_tokens=2936, output_tokens=414
15:50:26,443 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:26,444 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:26,712 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:26,712 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:26,984 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:26,985 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:27,14 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:27,15 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:27,43 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:27,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 13.60999999998603. input_tokens=2936, output_tokens=676
15:50:27,662 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:27,663 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:27,907 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:27,907 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:28,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:28,413 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:28,428 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:28,428 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:28,513 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:28,515 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:28,628 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:28,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 12.812999999965541. input_tokens=2936, output_tokens=887
15:50:28,744 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:28,745 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:28,919 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:28,920 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:29,183 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:29,184 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:29,265 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:29,267 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:30,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:30,76 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:30,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:30,191 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:30,825 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:30,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 8 retries took 5.937999999965541. input_tokens=34, output_tokens=418
15:50:30,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:30,940 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:30,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:30,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.327999999979511. input_tokens=2936, output_tokens=846
15:50:31,81 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:31,82 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:31,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:31,303 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:31,364 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:31,365 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:31,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:31,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.35999999998603. input_tokens=2936, output_tokens=761
15:50:31,690 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:31,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 5.375. input_tokens=34, output_tokens=356
15:50:31,779 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:31,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.797000000020489. input_tokens=2936, output_tokens=583
15:50:31,901 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:31,902 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:32,602 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:32,602 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:33,114 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:33,114 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:33,138 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:33,139 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:33,432 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:33,433 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:33,561 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:33,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.155999999959022. input_tokens=2936, output_tokens=577
15:50:34,29 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:34,30 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:35,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:35,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.202999999979511. input_tokens=34, output_tokens=510
15:50:35,365 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:35,365 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:35,425 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:35,425 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:35,425 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 192960, Requested 7390. Please try again in 105ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:50:35,438 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'udley thought for a moment. It looked like hard work. Finally he said\nslowly, "So I\'ll have thirty ... thirty..."\n\n"Thirty-nine, sweetums," said Aunt Petunia.\n\n"Oh." Dudley sat down heavily and grabbed the nearest parcel. "All right\nthen."\n\nUncle Vernon chuckled. "Little tyke wants his money\'s worth, just like\nhis father. \'Atta boy, Dudley!" He ruffled Dudley\'s hair.\n\nAt that moment the telephone rang and Aunt Petunia went to answer it\nwhile Harry and Uncle Vernon watched Dudley unwrap the racing bike, a\nvideo camera, a remote control airplane, sixteen new computer games, and\na VCR. He was ripping the paper off a gold wristwatch when Aunt Petunia\ncame back from the telephone looking both angry and worried.\n\n"Bad news, Vernon," she said. "Mrs. Figg\'s broken her leg. She can\'t\ntake him." She jerked her head in Harry\'s direction.\n\nDudley\'s mouth fell open in horror, but Harry\'s heart gave a leap. Every\nyear on Dudley\'s birthday, his parents took him and a friend out for the\nday, to adventure parks, hamburger restaurants, or the movies. Every\nyear, Harry was left behind with Mrs. Figg, a mad old lady who lived two\nstreets away. Harry hated it there. The whole house smelled of cabbage\nand Mrs. Figg made him look at photographs of all the cats she\'d ever\nowned.\n\n"Now what?" said Aunt Petunia, looking furiously at Harry as though he\'d\nplanned this. Harry knew he ought to feel sorry that Mrs. Figg had\nbroken her leg, but it wasn\'t easy when he reminded himself it would be\na whole year before he had to look at Tibbles, Snowy, Mr. Paws, and\nTufty again.\n\n"We could phone Marge," Uncle Vernon suggested.\n\n"Don\'t be silly, Vernon, she hates the boy."\n\nThe Dursleys often spoke about Harry like this, as though he wasn\'t\nthere -- or rather, as though he was something very nasty that couldn\'t\nunderstand them, like a slug.\n\n"What about what\'s-her-name, your friend -- Yvonne?"\n\n"On vacation in Majorca," snapped Aunt Petunia.\n\n"You could just leave me here," Harry put in hopefully (he\'d be able to\nwatch what he wanted on television for a change and maybe even have a go\non Dudley\'s computer).\n\nAunt Petunia looked as though she\'d just swallowed a lemon.\n\n"And come back and find the house in ruins?" she snarled.\n\n"I won\'t blow up the house," said Harry, but they weren\'t listening.\n\n"I suppose we could take him to the zoo," said Aunt Petunia slowly, "...\nand leave him in the car...."\n\n"That car\'s new, he\'s not sitting in it alone...."\n\nDudley began to cry loudly. In fact, he wasn\'t really crying -- it had\nbeen years since he\'d really cried -- but he knew that if he screwed up\nhis face and wailed, his mother would give him anything he wanted.\n\n"Dinky Duddydums, don\'t cry, Mummy won\'t let him spoil your special\nday!" she cried, flinging her arms around him.\n\n"I... don\'t... want... him... t-t-to come!" Dudley yelled between huge,\npretend sobs. "He always sp- spoils everything!" He shot Harry a nasty\ngrin through the gap in his mother\'s arms.\n\nJust then, the doorbell rang -- "Oh, good Lord, they\'re here!" said Aunt\nPetunia frantically -- and a moment later, Dudley\'s best friend, Piers\nPolkiss, walked in with his mother. Piers was a scrawny boy with a face\nlike a rat. He was usually the one who held people\'s arms behind their\nbacks while Dudley hit them. Dudley stopped pretending to cry at once.\n\nHalf an hour later, Harry, who couldn\'t believe his luck, was sitting in\nthe back of the Dursleys\' car with Piers and Dudley, on the way to the\nzoo for the first time in his life. His aunt and uncle hadn\'t been able\nto think of anything else to do with him, but before they\'d left, Uncle\nVernon had taken Harry aside.\n\n"I\'m warning you," he had said, putting his large purple face right up\nclose to Harry\'s, "I\'m warning you now, boy -- any funny business,\nanything at all -- and you\'ll be in that cupboard from now until\nChristmas."\n\n"I\'m not going to do anything," said Harry, "honestly..\n\nBut Uncle Vernon didn\'t believe him. No one ever did.\n\nThe problem was, strange things often happened around Harry and it was\njust no good telling the Dursleys he didn\'t make them happen.\n\nOnce, Aunt Petunia, tired of Harry coming back from the barbers looking\nas though he hadn\'t been at all, had taken a pair of kitchen scissors\nand cut his hair so short he was almost bald except for his bangs, which\nshe left "to hide that horrible scar." Dudley had laughed himself silly\nat Harry, who spent a sleepless night imagining school the next day,\nwhere he was already laughed at for his baggy clothes and taped glasses.\nNext morning, however, he had gotten up to find his hair exactly as it\nhad been before Aunt Petunia had sheared it off He had been given a week\nin his cupboard for this, even though he had tried to explain that he\ncouldn\'t explain how it had grown back so quickly.\n\nAnother time, Aunt Petun'}
15:50:35,471 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:35,471 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:35,727 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:35,727 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:35,970 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:35,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 8 retries took 13.84299999999348. input_tokens=2936, output_tokens=802
15:50:36,107 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:36,107 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:36,541 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:36,542 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:37,783 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:37,783 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:37,784 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 196376, Requested 7347. Please try again in 1.116s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:50:37,785 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'wide open. Once, he thought he saw a burst of fire at the end of a\npassage and twisted around to see if it was a dragon, but too late - -\nthey plunged even deeper, passing an underground lake where huge\nstalactites and stalagmites grew from the ceiling and floor.\n\nI never know," Harry called to Hagrid over the noise of the cart,\n"what\'s the difference between a stalagmite and a stalactite?"\n\n"Stalagmite\'s got an \'m\' in it," said Hagrid. "An\' don\' ask me questions\njust now, I think I\'m gonna be sick."\n\nHe did look very green, and when the cart stopped at last beside a small\ndoor in the passage wall, Hagrid got out and had to lean against the\nwall to stop his knees from trembling.\n\nGriphook unlocked the door. A lot of green smoke came billowing out, and\nas it cleared, Harry gasped. Inside were mounds of gold coins. Columns\nof silver. Heaps of little bronze Knuts.\n\n"All yours," smiled Hagrid.\n\nAll Harry\'s -- it was incredible. The Dursleys couldn\'t have known about\nthis or they\'d have had it from him faster than blinking. How often had\nthey complained how much Harry cost them to keep? And all the time there\nhad been a small fortune belonging to him, buried deep under London.\n\nHagrid helped Harry pile some of it into a bag.\n\n"The gold ones are Galleons," he explained. "Seventeen silver Sickles to\na Galleon and twenty-nine Knuts to a Sickle, it\'s easy enough. Right,\nthat should be enough fer a couple o\' terms, we\'ll keep the rest safe\nfor yeh." He turned to Griphook. "Vault seven hundred and thirteen now,\nplease, and can we go more slowly?"\n\n"One speed only," said Griphook.\n\nThey were going even deeper now and gathering speed. The air became\ncolder and colder as they hurtled round tight corners. They went\nrattling over an underground ravine, and Harry leaned over the side to\ntry to see what was down at the dark bottom, but Hagrid groaned and\npulled him back by the scruff of his neck.\n\nVault seven hundred and thirteen had no keyhole.\n\n"Stand back," said Griphook importantly. He stroked the door gently with\none of his long fingers and it simply melted away.\n\n"If anyone but a Gringotts goblin tried that, they\'d be sucked through\nthe door and trapped in there," said Griphook.\n\n"How often do you check to see if anyone\'s inside?" Harry asked.\n\n"About once every ten years," said Griphook with a rather nasty grin.\n\nSomething really extraordinary had to be inside this top security vault,\nHarry was sure, and he leaned forward eagerly, expecting to see fabulous\njewels at the very least -- but at first he thought it was empty. Then\nhe noticed a grubby little package wrapped up in brown paper lying on\nthe floor. Hagrid picked it up and tucked it deep inside his coat. Harry\nlonged to know what it was, but knew better than to ask.\n\n"Come on, back in this infernal cart, and don\'t talk to me on the way\nback, it\'s best if I keep me mouth shut," said Hagrid.\n\nOne wild cart ride later they stood blinking in the sunlight outside\nGringotts. Harry didn\'t know where to run first now that he had a bag\nfull of money. He didn\'t have to know how many Galleons there were to a\npound to know that he was holding more money than he\'d had in his whole\nlife -- more money than even Dudley had ever had.\n\n"Might as well get yer uniform," said Hagrid, nodding toward Madam\nMalkin\'s Robes for All Occasions. "Listen, Harry, would yeh mind if I\nslipped off fer a pick-me-up in the Leaky Cauldron? I hate them\nGringotts carts." He did still look a bit sick, so Harry entered Madam\nMalkin\'s shop alone, feeling nervous.\n\nMadam Malkin was a squat, smiling witch dressed all in mauve.\n\n"Hogwarts, clear?" she said, when Harry started to speak. "Got the lot\nhere -- another young man being fitted up just now, in fact. "\n\nIn the back of the shop, a boy with a pale, pointed face was standing on\na footstool while a second witch pinned up his long black robes. Madam\nMalkin stood Harry on a stool next to him) slipped a long robe over his\nhead, and began to pin it to the right length.\n\n"Hello," said the boy, "Hogwarts, too?"\n\n"Yes," said Harry.\n\n"My father\'s next door buying my books and mother\'s up the street\nlooking at wands," said the boy. He had a bored, drawling voice. "Then\nI\'m going to drag them off to took at racing brooms. I don\'t see why\nfirst years can\'t have their own. I think I\'ll bully father into getting\nme one and I\'ll smuggle it in somehow."\n\nHarry was strongly reminded of Dudley.\n\n"Have you got your own broom?" the boy went on.\n\n"No," said Harry.\n\n"Play Quidditch at all?"\n\n"No," Harry said again, wondering what on earth Quidditch could be.\n\n"I do -- Father says it\'s a crime if I\'m not picked to play for my\nhouse, and I must say, I agree. Know what house you\'ll be in yet?"\n\n"No," said Harry, feeling'}
15:50:37,907 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:37,911 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:37,946 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:37,947 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:38,121 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:38,123 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:38,262 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:38,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.577999999979511. input_tokens=34, output_tokens=435
15:50:38,537 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:38,538 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:38,539 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 197253, Requested 7456. Please try again in 1.412s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:50:38,543 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'rain. Great drops beat on the roof of the car. Dud ley\nsniveled.\n\n"It\'s Monday," he told his mother. "The Great Humberto\'s on tonight. I\nwant to stay somewhere with a television. "\n\nMonday. This reminded Harry of something. If it was Monday -- and you\ncould usually count on Dudley to know the days the week, because of\ntelevision -- then tomorrow, Tuesday, was Harry\'s eleventh birthday. Of\ncourse, his birthdays were never exactly fun -- last year, the Dursleys\nhad given him a coat hanger and a pair of Uncle Vernon\'s old socks.\nStill, you weren\'t eleven every day.\n\nUncle Vernon was back and he was smiling. He was also carrying a long,\nthin package and didn\'t answer Aunt Petunia when she asked what he\'d\nbought.\n\n"Found the perfect place!" he said. "Come on! Everyone out!"\n\nIt was very cold outside the car. Uncle Vernon was pointing at what\nlooked like a large rock way out at sea. Perched on top of the rock was\nthe most miserable little shack you could imagine. One thing was\ncertain, there was no television in there.\n\n"Storm forecast for tonight!" said Uncle Vernon gleefully, clapping his\nhands together. "And this gentleman\'s kindly agreed to lend us his\nboat!"\n\nA toothless old man came ambling up to them, pointing, with a rather\nwicked grin, at an old rowboat bobbing in the iron-gray water below\nthem.\n\n"I\'ve already got us some rations," said Uncle Vernon, "so all aboard!"\n\nIt was freezing in the boat. Icy sea spray and rain crept down their\nnecks and a chilly wind whipped their faces. After what seemed like\nhours they reached the rock, where Uncle Vernon, slipping and sliding,\nled the way to the broken-down house.\n\nThe inside was horrible; it smelled strongly of seaweed, the wind\nwhistled through the gaps in the wooden walls, and the fireplace was\ndamp and empty. There were only two rooms.\n\nUncle Vernon\'s rations turned out to be a bag of chips each and four\nbananas. He tried to start a fire but the empty chip bags just smoked\nand shriveled up.\n\n"Could do with some of those letters now, eh?" he said cheerfully.\n\nHe was in a very good mood. Obviously he thought nobody stood a chance\nof reaching them here in a storm to deliver mail. Harry privately\nagreed, though the thought didn\'t cheer him up at all.\n\nAs night fell, the promised storm blew up around them. Spray from the\nhigh waves splattered the walls of the hut and a fierce wind rattled the\nfilthy windows. Aunt Petunia found a few moldy blankets in the second\nroom and made up a bed for Dudley on the moth-eaten sofa. She and Uncle\nVernon went off to the lumpy bed next door, and Harry was left to find\nthe softest bit of floor he could and to curl up under the thinnest,\nmost ragged blanket.\n\nThe storm raged more and more ferociously as the night went on. Harry\ncouldn\'t sleep. He shivered and turned over, trying to get comfortable,\nhis stomach rumbling with hunger. Dudley\'s snores were drowned by the\nlow rolls of thunder that started near midnight. The lighted dial of\nDudley\'s watch, which was dangling over the edge of the sofa on his fat\nwrist, told Harry he\'d be eleven in ten minutes\' time. He lay and\nwatched his birthday tick nearer, wondering if the Dursleys would\nremember at all, wondering where the letter writer was now.\n\nFive minutes to go. Harry heard something creak outside. He hoped the\nroof wasn\'t going to fall in, although he might be warmer if it did.\nFour minutes to go. Maybe the house in Privet Drive would be so full of\nletters when they got back that he\'d be able to steal one somehow.\n\nThree minutes to go. Was that the sea, slapping hard on the rock like\nthat? And (two minutes to go) what was that funny crunching noise? Was\nthe rock crumbling into the sea?\n\nOne minute to go and he\'d be eleven. Thirty seconds... twenty ... ten...\nnine -- maybe he\'d wake Dudley up, just to annoy him -- three... two...\none...\n\nBOOM.\n\nThe whole shack shivered and Harry sat bolt upright, staring at the\ndoor. Someone was outside, knocking to come in.\n\n\nCHAPTER FOUR\n\nTHE KEEPER OF THE KEYS\n\nBOOM. They knocked again. Dudley jerked awake. "Where\'s the cannon?" he\nsaid stupidly.\n\nThere was a crash behind them and Uncle Vernon came skidding into the\nroom. He was holding a rifle in his hands -- now they knew what had been\nin the long, thin package he had brought with them.\n\n"Who\'s there?" he shouted. "I warn you -- I\'m armed!"\n\nThere was a pause. Then --\n\nSMASH!\n\nThe door was hit with such force that it swung clean off its hinges and\nwith a deafening crash landed flat on the floor.\n\nA giant of a man was standing in the doorway. His face was almost\ncompletely hidden by a long, shaggy mane of hair and a wild, tangled\nbeard, but you could make out his eyes, glinting like black beetles\nunder all the hair.\n\nThe giant squeezed his way into the hut, stooping so that his head just\nbrushed the ceiling. He bent down, picked up the door, and fitted it\neasily back into its frame. The noise of the storm outside'}
15:50:38,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:38,555 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:38,666 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:38,667 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:38,806 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:38,807 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:39,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:39,56 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:39,59 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:39,59 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:39,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:39,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 10.641000000061467. input_tokens=2936, output_tokens=715
15:50:39,314 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:39,314 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:39,314 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 198785, Requested 7415. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:50:39,315 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'upset or angry... chased by Dudley\'s\ngang, he had somehow found himself out of their reach... dreading going\nto school with that ridiculous haircut, he\'d managed to make it grow\nback... and the very last time Dudley had hit him, hadn\'t he got his\nrevenge, without even realizing he was doing it? Hadn\'t he set a boa\nconstrictor on him?\n\nHarry looked back at Hagrid, smiling, and saw that Hagrid was positively\nbeaming at him.\n\n"See?" said Hagrid. "Harry Potter, not a wizard -- you wait, you\'ll be\nright famous at Hogwarts."\n\nBut Uncle Vernon wasn\'t going to give in without a fight.\n\n"Haven\'t I told you he\'s not going?" he hissed. "He\'s going to Stonewall\nHigh and he\'ll be grateful for it. I\'ve read those letters and he needs\nall sorts of rubbish -- spell books and wands and --"\n\n"If he wants ter go, a great Muggle like you won\'t stop him," growled\nHagrid. "Stop Lily an\' James Potter\' s son goin\' ter Hogwarts! Yer mad.\nHis name\'s been down ever since he was born. He\'s off ter the finest\nschool of witchcraft and wizardry in the world. Seven years there and he\nwon\'t know himself. He\'ll be with youngsters of his own sort, fer a\nchange, an\' he\'ll be under the greatest headmaster Hogwarts ever had\nAlbus Dumbled--"\n\n"I AM NOT PAYING FOR SOME CRACKPOT OLD FOOL To TEACH HIM MAGIC TRICKS!"\nyelled Uncle Vernon.\n\nBut he had finally gone too far. Hagrid seized his umbrella and whirled\nit over his head, "NEVER," he thundered, "- INSULT- ALBUS- DUMBLEDORE-\nIN- FRONT- OF- ME!"\n\nHe brought the umbrella swishing down through the air to point at Dudley\n-- there was a flash of violet light, a sound like a firecracker, a\nsharp squeal, and the next second, Dudley was dancing on the spot with\nhis hands clasped over his fat bottom, howling in pain. When he turned\nhis back on them, Harry saw a curly pig\'s tail poking through a hole in\nhis trousers.\n\nUncle Vernon roared. Pulling Aunt Petunia and Dudley into the other\nroom, he cast one last terrified look at Hagrid and slammed the door\nbehind them.\n\nHagrid looked down at his umbrella and stroked his beard.\n\n"Shouldn\'ta lost me temper," he said ruefully, "but it didn\'t work\nanyway. Meant ter turn him into a pig, but I suppose he was so much like\na pig anyway there wasn\'t much left ter do."\n\nHe cast a sideways look at Harry under his bushy eyebrows.\n\n"Be grateful if yeh didn\'t mention that ter anyone at Hogwarts," he\nsaid. "I\'m -- er -- not supposed ter do magic, strictly speakin\'. I was\nallowed ter do a bit ter follow yeh an\' get yer letters to yeh an\' stuff\n-- one o\' the reasons I was so keen ter take on the job\n\n"Why aren\'t you supposed to do magic?" asked Harry.\n\n"Oh, well -- I was at Hogwarts meself but I -- er -- got expelled, ter\ntell yeh the truth. In me third year. They snapped me wand in half an\'\neverything. But Dumbledore let me stay on as gamekeeper. Great man,\nDumbledore." "Why were you expelled?"\n\n"It\'s gettin\' late and we\'ve got lots ter do tomorrow," said Hagrid\nloudly. "Gotta get up ter town, get all yer books an\' that."\n\nHe took off his thick black coat and threw it to Harry.\n\n"You can kip under that," he said. "Don\' mind if it wriggles a bit, I\nthink I still got a couple o\' dormice in one o\' the pockets."\n\n\nCHAPTER FIVE\n\nDIAGON ALLEY\n\nHarry woke early the next morning. Although he could tell it was\ndaylight, he kept his eyes shut tight.\n\n"It was a dream, he told himself firmly. "I dreamed a giant called\nHagrid came to tell me I was going to a school for wizards. When I open\nmy eyes I\'ll be at home in my cupboard."\n\nThere was suddenly a loud tapping noise.\n\nAnd there\'s Aunt Petunia knocking on the door, Harry thought, his heart\nsinking. But he still didn\'t open his eyes. It had been such a good\ndream.\n\nTap. Tap. Tap.\n\n"All right," Harry mumbled, "I\'m getting up."\n\nHe sat up and Hagrid\'s heavy coat fell off him. The hut was full of\nsunlight, the storm was over, Hagrid himself was asleep on the collapsed\nsofa, and there was an owl rapping its claw on the window, a newspaper\nheld in its beak.\n\nHarry scrambled to his feet, so happy he felt as though a large balloon\nwas swelling inside him. He went straight to the window and jerked it\nopen. The owl swooped in and dropped the newspaper on top of Hagrid, who\ndidn\'t wake up. The owl then fluttered onto the floor and began to\nattack Hagrid\'s coat.\n\n"Don\'t do that."\n\nHarry tried to wave the owl out of the way, but it snapped its beak\nfiercely at him and carried on savaging the coat.\n\n"Hagrid!" said Harry loudly. "There\'s an owl\n\n"Pay him," Hagrid grunted into the sofa.\n\n"What?"\n\n"He wants payin\' fer deliverin'}
15:50:39,373 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:39,373 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:39,430 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:39,431 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:40,109 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:40,109 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:40,115 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:40,115 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:40,198 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:40,199 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:40,200 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 195853, Requested 7470. Please try again in 996ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:50:40,204 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'as\nKent, Yorkshire, and Dundee have been phoning in to tell me that instead\nof the rain I promised yesterday, they\'ve had a downpour of shooting\nstars! Perhaps people have been celebrating Bonfire Night early -- it\'s\nnot until next week, folks! But I can promise a wet night tonight."\n\nMr. Dursley sat frozen in his armchair. Shooting stars all over Britain?\nOwls flying by daylight? Mysterious people in cloaks all over the place?\nAnd a whisper, a whisper about the Potters...\n\nMrs. Dursley came into the living room carrying two cups of tea. It was\nno good. He\'d have to say something to her. He cleared his throat\nnervously. "Er -- Petunia, dear -- you haven\'t heard from your sister\nlately, have you?"\n\nAs he had expected, Mrs. Dursley looked shocked and angry. After all,\nthey normally pretended she didn\'t have a sister.\n\n"No," she said sharply. "Why?"\n\n"Funny stuff on the news," Mr. Dursley mumbled. "Owls... shooting\nstars... and there were a lot of funny-looking people in town today..."\n\n"So?" snapped Mrs. Dursley.\n\n"Well, I just thought... maybe... it was something to do with... you\nknow... her crowd."\n\nMrs. Dursley sipped her tea through pursed lips. Mr. Dursley wondered\nwhether he dared tell her he\'d heard the name "Potter." He decided he\ndidn\'t dare. Instead he said, as casually as he could, "Their son --\nhe\'d be about Dudley\'s age now, wouldn\'t he?"\n\n"I suppose so," said Mrs. Dursley stiffly.\n\n"What\'s his name again? Howard, isn\'t it?"\n\n"Harry. Nasty, common name, if you ask me."\n\n"Oh, yes," said Mr. Dursley, his heart sinking horribly. "Yes, I quite\nagree."\n\nHe didn\'t say another word on the subject as they went upstairs to bed.\nWhile Mrs. Dursley was in the bathroom, Mr. Dursley crept to the bedroom\nwindow and peered down into the front garden. The cat was still there.\nIt was staring down Privet Drive as though it were waiting for\nsomething.\n\nWas he imagining things? Could all this have anything to do with the\nPotters? If it did... if it got out that they were related to a pair of\n-- well, he didn\'t think he could bear it.\n\nThe Dursleys got into bed. Mrs. Dursley fell asleep quickly but Mr.\nDursley lay awake, turning it all over in his mind. His last, comforting\nthought before he fell asleep was that even if the Potters were\ninvolved, there was no reason for them to come near him and Mrs.\nDursley. The Potters knew very well what he and Petunia thought about\nthem and their kind.... He couldn\'t see how he and Petunia could get\nmixed up in anything that might be going on -- he yawned and turned over\n-- it couldn\'t affect them....\n\nHow very wrong he was.\n\nMr. Dursley might have been drifting into an uneasy sleep, but the cat\non the wall outside was showing no sign of sleepiness. It was sitting as\nstill as a statue, its eyes fixed unblinkingly on the far corner of\nPrivet Drive. It didn\'t so much as quiver when a car door slammed on the\nnext street, nor when two owls swooped overhead. In fact, it was nearly\nmidnight before the cat moved at all.\n\nA man appeared on the corner the cat had been watching, appeared so\nsuddenly and silently you\'d have thought he\'d just popped out of the\nground. The cat\'s tail twitched and its eyes narrowed.\n\nNothing like this man had ever been seen on Privet Drive. He was tall,\nthin, and very old, judging by the silver of his hair and beard, which\nwere both long enough to tuck into his belt. He was wearing long robes,\na purple cloak that swept the ground, and high-heeled, buckled boots.\nHis blue eyes were light, bright, and sparkling behind half-moon\nspectacles and his nose was very long and crooked, as though it had been\nbroken at least twice. This man\'s name was Albus Dumbledore.\n\nAlbus Dumbledore didn\'t seem to realize that he had just arrived in a\nstreet where everything from his name to his boots was unwelcome. He was\nbusy rummaging in his cloak, looking for something. But he did seem to\nrealize he was being watched, because he looked up suddenly at the cat,\nwhich was still staring at him from the other end of the street. For\nsome reason, the sight of the cat seemed to amuse him. He chuckled and\nmuttered, "I should have known."\n\nHe found what he was looking for in his inside pocket. It seemed to be a\nsilver cigarette lighter. He flicked it open, held it up in the air, and\nclicked it. The nearest street lamp went out with a little pop. He\nclicked it again -- the next lamp flickered into darkness. Twelve times\nhe clicked the Put-Outer, until the only lights left on the whole street\nwere two tiny pinpricks in the distance, which were the eyes of the cat\nwatching him. If anyone looked out of their window now, even beady-eyed\nMrs. Dursley, they wouldn\'t be able to see anything that was happening\ndown on the pavement. Dumbledore slipped the Put-Outer back inside his\ncloak and set off'}
15:50:40,258 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:40,259 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:40,408 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:40,408 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:40,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:40,414 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:40,565 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:40,566 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:40,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:40,583 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:41,458 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:41,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 10.702999999979511. input_tokens=2937, output_tokens=755
15:50:41,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:41,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.327999999979511. input_tokens=34, output_tokens=147
15:50:41,668 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:41,669 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:41,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:41,736 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:41,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:41,784 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:42,472 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:42,473 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:42,928 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:42,931 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 13.079000000027008. input_tokens=2936, output_tokens=801
15:50:43,47 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:43,47 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:43,360 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:43,361 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:43,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:43,521 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:43,694 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:43,695 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:43,893 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:43,894 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:44,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:44,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 7.172000000020489. input_tokens=34, output_tokens=397
15:50:44,626 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:44,626 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:45,129 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:45,130 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:45,398 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:45,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.827999999979511. input_tokens=2936, output_tokens=812
15:50:45,558 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:45,559 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:46,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:46,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.672000000020489. input_tokens=2936, output_tokens=721
15:50:46,242 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:46,242 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:46,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:46,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.031000000075437. input_tokens=2936, output_tokens=864
15:50:46,537 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:46,538 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:46,646 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:46,647 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:47,372 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:47,372 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:47,637 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:47,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.187000000034459. input_tokens=34, output_tokens=334
15:50:47,755 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:47,756 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:47,843 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:47,843 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:48,260 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:48,261 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:48,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:48,266 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:48,295 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:48,296 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:48,379 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:48,380 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:48,527 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:48,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.266000000061467. input_tokens=2936, output_tokens=614
15:50:48,684 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:48,685 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:48,907 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:48,908 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:48,963 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:48,963 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:49,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:49,168 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:49,172 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:49,173 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:49,407 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:49,409 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 6.547000000020489. input_tokens=34, output_tokens=422
15:50:49,545 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:49,546 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:49,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:49,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 8.390999999945052. input_tokens=34, output_tokens=552
15:50:49,816 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:49,817 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:50,636 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:50,636 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:50,726 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:50,729 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 16.07799999997951. input_tokens=2936, output_tokens=1019
15:50:50,743 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:50,744 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:50,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:50,849 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:51,163 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:51,164 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:51,181 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:51,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.0. input_tokens=34, output_tokens=437
15:50:51,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:51,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.85999999998603. input_tokens=34, output_tokens=436
15:50:51,303 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:51,304 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:51,308 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:51,309 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:51,386 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:51,386 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:51,491 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:51,492 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:52,282 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:52,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 8 retries took 3.734000000054948. input_tokens=34, output_tokens=281
15:50:52,392 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:52,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 3.030999999959022. input_tokens=34, output_tokens=219
15:50:52,484 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:52,485 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:52,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:52,504 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:52,507 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:52,508 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:52,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:52,665 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:52,843 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:52,843 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:52,855 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:52,856 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:52,937 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:52,938 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:53,462 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:53,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 2.265999999945052. input_tokens=34, output_tokens=161
15:50:53,565 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:53,566 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:53,641 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:53,642 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:53,989 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:53,990 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:54,93 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:54,97 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:54,221 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:54,222 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:54,506 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:54,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 8.10999999998603. input_tokens=2936, output_tokens=614
15:50:54,835 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:54,836 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:54,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:54,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.187000000034459. input_tokens=34, output_tokens=239
15:50:55,63 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:55,64 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:55,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:55,320 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:55,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:55,505 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:55,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:55,656 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 8.125. input_tokens=2936, output_tokens=618
15:50:55,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:55,942 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:55,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:55,952 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:56,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:56,503 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:56,956 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:56,957 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:57,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:57,324 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:58,260 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:58,264 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:58,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:58,410 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:58,418 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:58,418 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:58,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:58,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.296999999904074. input_tokens=34, output_tokens=457
15:50:58,692 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:58,692 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:58,997 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:58,999 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:59,5 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:59,6 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:59,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:59,135 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:59,293 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:59,294 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:59,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:59,669 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.172000000020489. input_tokens=2936, output_tokens=429
15:50:59,732 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:50:59,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 5 retries took 9.312000000034459. input_tokens=2936, output_tokens=781
15:50:59,772 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:59,773 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:59,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:59,844 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:50:59,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:50:59,943 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:00,768 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:00,769 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:00,821 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:00,822 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:01,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:01,600 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:02,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:02,11 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:02,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:02,597 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:03,727 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:03,727 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:03,765 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:03,766 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:04,29 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:04,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 9.141000000061467. input_tokens=2935, output_tokens=759
15:51:04,342 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:04,342 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:04,450 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:04,451 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:04,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:04,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 11.280999999959022. input_tokens=2936, output_tokens=911
15:51:04,821 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:04,821 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:05,295 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:05,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.655999999959022. input_tokens=2936, output_tokens=451
15:51:05,538 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:05,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.890999999945052. input_tokens=2935, output_tokens=798
15:51:06,88 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:06,88 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:06,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:06,151 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:06,467 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:06,468 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:06,677 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:06,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 8.53200000000652. input_tokens=2937, output_tokens=621
15:51:06,926 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:06,927 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:07,608 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:07,610 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:07,625 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:07,625 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:08,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:08,340 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:08,525 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:08,525 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:08,563 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:08,563 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:08,563 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 193926, Requested 7805. Please try again in 519ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:51:08,567 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'of their\nfirst class he took the roll call, and when he reached Harry\'s name he\ngave an excited squeak and toppled out of sight.\n\nProfessor McGonagall was again different. Harry had been quite right to\nthink she wasn\'t a teacher to cross. Strict and clever, she gave them a\ntalking-to the moment they sat down in her first class.\n\n"Transfiguration is some of the most complex and dangerous magic you\nwill learn at Hogwarts," she said. "Anyone messing around in my class\nwill leave and not come back. You have been warned."\n\nThen she changed her desk into a pig and back again. They were all very\nimpressed and couldn\'t wait to get started, but soon realized they\nweren\'t going to be changing the furniture into animals for a long time.\nAfter taking a lot of complicated notes, they were each given a match\nand started trying to turn it into a needle. By the end of the lesson,\nonly Hermione Granger had made any difference to her match; Professor\nMcGonagall showed the class how it had gone all silver and pointy and\ngave Hermione a rare smile.\n\nThe class everyone had really been looking forward to was Defense\nAgainst the Dark Arts, but Quirrell\'s lessons turned out to be a bit of\na joke. His classroom smelled strongly of garlic, which everyone said\nwas to ward off a vampire he\'d met in Romania and was afraid would be\ncoming back to get him one of these days. His turban, he told them, had\nbeen given to him by an African prince as a thank-you for getting rid of\na troublesome zombie, but they weren\'t sure they believed this story.\nFor one thing, when Seamus Finnigan asked eagerly to hear how Quirrell\nhad fought off the zombie, Quirrell went pink and started talking about\nthe weather; for another, they had noticed that a funny smell hung\naround the turban, and the Weasley twins insisted that it was stuffed\nfull of garlic as well, so that Quirrell was protected wherever he went.\n\nHarry was very relieved to find out that he wasn\'t miles behind everyone\nelse. Lots of people had come from Muggle families and, like him, hadn\'t\nhad any idea that they were witches and wizards. There was so much to\nlearn that even people like Ron didn\'t have much of a head start.\n\nFriday was an important day for Harry and Ron. They finally managed to\nfind their way down to the Great Hall for breakfast without getting lost\nonce.\n\n"What have we got today?" Harry asked Ron as he poured sugar on his\nporridge.\n\n"Double Potions with the Slytherins," said Ron. "Snape\'s Head of\nSlytherin House. They say he always favors them -- we\'ll be able to see\nif it\'s true."\n\n"Wish McGonagall favored us, " said Harry. Professor McGonagall was head\nof Gryffindor House, but it hadn\'t stopped her from giving them a huge\npile of homework the day before.\n\nJust then, the mail arrived. Harry had gotten used to this by now, but\nit had given him a bit of a shock on the first morning, when about a\nhundred owls had suddenly streamed into the Great Hall during breakfast,\ncircling the tables until they saw their owners, and dropping letters\nand packages onto their laps.\n\nHedwig hadn\'t brought Harry anything so far. She sometimes flew in to\nnibble his ear and have a bit of toast before going off to sleep in the\nowlery with the other school owls. This morning, however, she fluttered\ndown between the marmalade and the sugar bowl and dropped a note onto\nHarry\'s plate. Harry tore it open at once. It said, in a very untidy\nscrawl:\n\n\nDear Harry,\n\nI know you get Friday afternoons off, so would you like to come and have\na cup of tea with me around three?\n\nI want to hear all about your first week. Send us an answer back with\nHedwig.\n\nHagrid\n\n\nHarry borrowed Ron\'s quill, scribbled Yes, please, see you later on the\nback of the note, and sent Hedwig off again.\n\nIt was lucky that Harry had tea with Hagrid to look forward to, because\nthe Potions lesson turned out to be the worst thing that had happened to\nhim so far.\n\nAt the start-of-term banquet, Harry had gotten the idea that Professor\nSnape disliked him. By the end of the first Potions lesson, he knew he\'d\nbeen wrong. Snape didn\'t dislike Harry -- he hated him.\n\nPotions lessons took place down in one of the dungeons. It was colder\nhere than up in the main castle, and would have been quite creepy enough\nwithout the pickled animals floating in glass jars all around the walls.\n\nSnape, like Flitwick, started the class by taking the roll call, and\nlike Flitwick, he paused at Harry\'s name.\n\n"Ah, Yes," he said softly, "Harry Potter. Our new -- celebrity."\n\nDraco Malfoy and his friends Crabbe and Goyle sniggered behind their\nhands. Snape finished calling the names and looked up at the class. His\neyes were black like Hagrid\'s, but they had none of Hagrid\'s warmth.\nThey were cold and empty and made you think of dark tunnels.\n\n"You are here to learn the subtle science and exact art of\npotionmaking," he began. He spoke in barely more than a whisper, but\nthey caught every word -- like Professor McGonagall, Snape had y caught\nevery word -- like Professor McGonagall, Snape had'}
15:51:08,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:08,713 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:09,83 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:09,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 3.375. input_tokens=34, output_tokens=266
15:51:09,193 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:09,194 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:09,194 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:09,194 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:09,273 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:09,274 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:09,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:09,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 8.094000000040978. input_tokens=2937, output_tokens=657
15:51:09,509 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:09,509 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:09,537 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:09,537 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:09,970 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:09,970 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:10,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:10,84 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:10,267 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:10,268 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:10,509 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:10,509 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:10,904 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:10,907 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.875. input_tokens=2936, output_tokens=518
15:51:11,27 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:11,31 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:11,142 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:11,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 7.672000000020489. input_tokens=2936, output_tokens=594
15:51:11,619 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:11,621 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:11,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:11,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 6 retries took 12.921999999904074. input_tokens=2936, output_tokens=983
15:51:11,900 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:11,900 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:11,947 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:11,947 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:12,712 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:12,712 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:13,147 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:13,150 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:13,733 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:13,733 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:13,804 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:13,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.125. input_tokens=34, output_tokens=624
15:51:13,883 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:13,883 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:13,914 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:13,915 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:14,480 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:14,481 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:14,979 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:14,979 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:15,417 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:15,421 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:16,201 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:16,202 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:16,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:16,352 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:16,712 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:16,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.155999999959022. input_tokens=2936, output_tokens=971
15:51:16,904 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:16,905 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:17,533 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:17,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 8.531000000075437. input_tokens=2936, output_tokens=723
15:51:17,650 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:17,650 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:17,777 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:17,778 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:18,192 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:18,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.469000000040978. input_tokens=34, output_tokens=326
15:51:18,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:18,357 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:18,467 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:18,467 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:18,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:18,530 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:18,649 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:18,650 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:19,133 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:19,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 9 retries took 9.827999999979511. input_tokens=2937, output_tokens=817
15:51:19,312 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:19,317 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:19,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:19,383 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:19,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:19,640 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:20,183 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:20,183 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:20,204 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:20,204 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:20,770 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:20,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 9.905999999959022. input_tokens=34, output_tokens=713
15:51:20,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:20,854 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:20,887 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:20,888 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:21,143 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:21,143 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:21,731 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:21,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 9.141000000061467. input_tokens=2936, output_tokens=742
15:51:22,30 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:22,30 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:22,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:22,255 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:22,512 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:22,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.375. input_tokens=2935, output_tokens=952
15:51:22,792 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:22,792 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:22,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:22,838 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:23,401 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:23,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.266000000061467. input_tokens=34, output_tokens=276
15:51:23,813 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:23,814 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:23,996 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:23,997 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:23,997 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 198034, Requested 7653. Please try again in 1.706s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:51:23,999 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'and looked up at the class. His\neyes were black like Hagrid\'s, but they had none of Hagrid\'s warmth.\nThey were cold and empty and made you think of dark tunnels.\n\n"You are here to learn the subtle science and exact art of\npotionmaking," he began. He spoke in barely more than a whisper, but\nthey caught every word -- like Professor McGonagall, Snape had y caught\nevery word -- like Professor McGonagall, Snape had the gift of keeping a\nclass silent without effort. "As there is little foolish wand-waving\nhere, many of you will hardly believe this is magic. I don\'t expect you\nwill really understand the beauty of the softly simmering cauldron with\nits shimmering fumes, the delicate power of liquids that creep through\nhuman veins, bewitching the mind, ensnaring the senses.... I can teach\nyou how to bottle fame, brew glory, even stopper death -- if you aren\'t\nas big a bunch of dunderheads as I usually have to teach."\n\nMore silence followed this little speech. Harry and Ron exchanged looks\nwith raised eyebrows. Hermione Granger was on the edge of her seat and\nlooked desperate to start proving that she wasn\'t a dunderhead.\n\n"Potter!" said Snape suddenly. "What would I get if I added powdered\nroot of asphodel to an infusion of wormwood?"\n\nPowdered root of what to an infusion of what? Harry glanced at Ron, who\nlooked as stumped as he was; Hermione\'s hand had shot into the air.\n\n"I don\'t know, sit," said Harry.\n\nSnape\'s lips curled into a sneer.\n\n"Tut, tut -- fame clearly isn\'t everything."\n\nHe ignored Hermione\'s hand.\n\n"Let\'s try again. Potter, where would you look if I told you to find me\na bezoar?"\n\nHermione stretched her hand as high into the air as it would go without\nher leaving her seat, but Harry didn\'t have the faintest idea what a\nbezoar was. He tried not to look at Malfoy, Crabbe, and Goyle, who were\nshaking with laughter.\n\n"I don\'t know, sit." "Thought you wouldn\'t open a book before coming,\neh, Potter?" Harry forced himself to keep looking straight into those\ncold eyes. He had looked through his books at the Dursleys\', but did\nSnape expect him to remember everything in One Thousand Magical Herbs\nand Fungi?\n\nSnape was still ignoring Hermione\'s quivering hand.\n\n"What is the difference, Potter, between monkshood and wolfsbane?"\n\nAt this, Hermione stood up, her hand stretching toward the dungeon\nceiling.\n\n"I don\'t know," said Harry quietly. "I think Hermione does, though, why\ndon\'t you try her?"\n\nA few people laughed; Harry caught Seamus\'s eye, and Seamus winked.\nSnape, however, was not pleased.\n\n"Sit down," he snapped at Hermione. "For your information, Potter,\nasphodel and wormwood make a sleeping potion so powerful it is known as\nthe Draught of Living Death. A bezoar is a stone taken from the stomach\nof a goat and it will save you from most poisons. As for monkshood and\nwolfsbane, they are the same plant, which also goes by the name of\naconite. Well? Why aren\'t you all copying that down?"\n\nThere was a sudden rummaging for quills and parchment. Over the noise,\nSnape said, "And a point will be taken from Gryffindor House for your\ncheek, Potter."\n\nThings didn\'t improve for the Gryffindors as the Potions lesson\ncontinued. Snape put them all into pairs and set them to mixing up a\nsimple potion to cure boils. He swept around in his long black cloak,\nwatching them weigh dried nettles and crush snake fangs, criticizing\nalmost everyone except Malfoy, whom he seemed to like. He was just\ntelling everyone to look at the perfect way Malfoy had stewed his horned\nslugs when clouds of acid green smoke and a loud hissing filled the\ndungeon. Neville had somehow managed to melt Seamus\'s cauldron into a\ntwisted blob, and their potion was seeping across the stone floor,\nburning holes in people\'s shoes. Within seconds, the whole class was\nstanding on their stools while Neville, who had been drenched in the\npotion when the cauldron collapsed, moaned in pain as angry red boils\nsprang up all over his arms and legs.\n\n"Idiot boy!" snarled Snape, clearing the spilled potion away with one\nwave of his wand. "I suppose you added the porcupine quills before\ntaking the cauldron off the fire?"\n\nNeville whimpered as boils started to pop up all over his nose.\n\n"Take him up to the hospital wing," Snape spat at Seamus. Then he\nrounded on Harry and Ron, who had been working next to Neville.\n\n"You -- Potter -- why didn\'t you tell him not to add the quills? Thought\nhe\'d make you look good if he got it wrong, did you? That\'s another\npoint you\'ve lost for Gryffindor."\n\nThis was so unfair that Harry opened his mouth to argue, but Ron kicked\nhim behind their cauldron.\n\n"Doi* push it," he muttered, "I\'ve heard Snape can turn very nasty."\n\nAs they climbed the steps out of the dungeon an hour later, Harry\'s mind\nwas racing and his spirits were low. He\'d lost two points for Gryffindor\nin his very first week -- why did'}
15:51:24,114 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:24,114 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:24,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:24,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.327999999979511. input_tokens=34, output_tokens=489
15:51:24,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:24,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.812999999965541. input_tokens=34, output_tokens=189
15:51:24,608 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:24,609 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:24,649 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:24,650 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:24,674 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:24,675 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:25,420 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:25,421 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:25,644 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:25,646 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:25,774 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:25,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 8.84299999999348. input_tokens=34, output_tokens=467
15:51:26,91 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:26,91 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:26,229 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:26,233 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 5.719000000040978. input_tokens=34, output_tokens=426
15:51:26,347 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:26,350 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:26,363 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:26,366 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:26,953 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:26,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.5469999999040738. input_tokens=34, output_tokens=281
15:51:27,66 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:27,67 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:27,812 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:27,812 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.297000000020489. input_tokens=34, output_tokens=375
15:51:27,897 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:27,897 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:27,934 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:27,935 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:27,983 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:27,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 12.531000000075437. input_tokens=2042, output_tokens=732
15:51:28,239 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:28,240 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:28,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:28,309 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:28,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:28,601 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:28,646 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:28,649 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:28,754 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:28,758 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:28,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:28,802 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:29,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:29,116 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:29,137 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:29,138 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:29,250 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:29,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 3.0320000000065193. input_tokens=34, output_tokens=246
15:51:29,302 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:29,305 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:29,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:29,377 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:29,421 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:29,425 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:29,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:29,495 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:29,495 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:29,495 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:30,12 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:30,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 5.547000000020489. input_tokens=34, output_tokens=458
15:51:30,183 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:30,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.202999999979511. input_tokens=34, output_tokens=164
15:51:30,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:30,310 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:30,329 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:30,331 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:30,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:30,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.719000000040978. input_tokens=34, output_tokens=321
15:51:31,99 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:31,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 3.952999999979511. input_tokens=34, output_tokens=314
15:51:31,221 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:31,221 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:31,483 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:31,484 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:31,488 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:31,489 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:32,158 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:32,159 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:32,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:32,557 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:32,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:32,968 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:33,633 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:33,634 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:33,641 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:33,641 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:33,927 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:33,927 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:33,967 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:33,967 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:34,224 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:34,227 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:34,865 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:34,865 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:35,137 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:35,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.125. input_tokens=34, output_tokens=318
15:51:35,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:35,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.515999999945052. input_tokens=34, output_tokens=285
15:51:35,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:35,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.155999999959022. input_tokens=34, output_tokens=324
15:51:36,108 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:36,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 6.937000000034459. input_tokens=34, output_tokens=460
15:51:37,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:37,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 3.655999999959022. input_tokens=34, output_tokens=254
15:51:38,128 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:38,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 2.577999999979511. input_tokens=34, output_tokens=166
15:51:38,853 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:38,857 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:38,883 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:38,883 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:38,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:38,933 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:38,933 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 198324, Requested 7461. Please try again in 1.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:51:38,933 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'Two Sixty?" Ron\ngrinned at Harry. "Comets look flashy, but they\'re not in the same\nleague as the Nimbus."\n\n"What would you know about it, Weasley, you couldn\'t afford half the\nhandle," Malfoy snapped back. "I suppose you and your brothers have to\nsave up twig by twig."\n\nBefore Ron could answer, Professor Flitwick appeared at Malfoy\'s elbow.\n\n"Not arguing, I hope, boys?" he squeaked.\n\n"Potter\'s been sent a broomstick, Professor," said Malfoy quickly.\n\n"Yes, yes, that\'s right," said Professor Flitwick, beaming at Harry.\n"Professor McGonagall told me all about the special circumstances,\nPotter. And what model is it?"\n\n"A Nimbus Two Thousand, sit," said Harry, fighting not to laugh at the\nlook of horror on Malfoy\'s face. "And it\'s really thanks to Malfoy here\nthat I\'ve got it," he added.\n\nHarry and Ron headed upstairs, smothering their laughter at Malfoy\'s\nobvious rage and confusion. "Well, it\'s true," Harry chortled as they\nreached the top of the marble staircase, "If he hadn\'t stolen Neville\'s\nRemembrall I wouln\'t be on the team...."\n\n"So I suppose you think that\'s a reward for breaking rules?" came an\nangry voice from just behind them. Hermione was stomping up the stairs,\nlooking disapprovingly at the package in Harry\'s hand.\n\n"I thought you weren\'t speaking to us?" said Harry.\n\n"Yes, don\'t stop now," said Ron, "it\'s doing us so much good."\n\nHermione marched away with her nose in the air.\n\nHarry had a lot of trouble keeping his mind on his lessons that day. It\nkept wandering up to the dormitory where his new broomstick was lying\nunder his bed, or straying off to the Quidditch field where he\'d be\nlearning to play that night. He bolted his dinner that evening without\nnoticing what he was eating, and then rushed upstairs with Ron to unwrap\nthe Nimbus Two Thousand at last.\n\n"Wow," Ron sighed, as the broomstick rolled onto Harry\'s bedspread.\n\nEven Harry, who knew nothing about the different brooms, thought it\nlooked wonderful. Sleek and shiny, with a mahogany handle, it had a long\ntail of neat, straight twigs and Nimbus Two Thousand written in gold\nnear the top.\n\nAs seven o\'clock drew nearer, Harry left the castle and set off in the\ndusk toward the Quidditch field. Held never been inside the stadium\nbefore. Hundreds of seats were raised in stands around the field so that\nthe spectators were high enough to see what was going on. At either end\nof the field were three golden poles with hoops on the end. They\nreminded Harry of the little plastic sticks Muggle\n\nchildren blew bubbles through, except that they were fifty feet high.\n\nToo eager to fly again to wait for Wood, Harry mounted his broomstick\nand kicked off from the ground. What a feeling -- he swooped in and out\nof the goal posts and then sped up and down the field. The Nimbus Two\nThousand turned wherever he wanted at his lightest touch.\n\n"Hey, Potter, come down!\'\n\nOliver Wood had arrived. fie was carrying a large wooden crate under his\narm. Harry landed next to him.\n\n"Very nice," said Wood, his eyes glinting. "I see what McGonagall\nmeant... you really are a natural. I\'m just going to teach you the rules\nthis evening, then you\'ll be joining team practice three times a week."\n\nHe opened the crate. Inside were four different-sized balls.\n\n"Right," said Wood. "Now, Quidditch is easy enough to understand, even\nif it\'s not too easy to play. There are seven players on each side.\nThree of them are called Chasers."\n\n"Three Chasers," Harry repeated, as Wood took out a bright red ball\nabout the size of a soccer ball.\n\n"This ball\'s called the Quaffle," said Wood. "The Chasers throw the\nQuaffle to each other and try and get it through one of the hoops to\nscore a goal. Ten points every time the Quaffle goes through one of the\nhoops. Follow me?"\n\n"The Chasers throw the Quaffle and put it through the hoops to score,"\nHarry recited. "So -- that\'s sort of like basketball on broomsticks with\nsix hoops, isn\'t it?"\n\n"What\'s basketball?" said Wood curiously. "Never mind," said Harry\nquickly.\n\n"Now, there\'s another player on each side who\'s called the Keeper -I\'m\nKeeper for Gryffindor. I have to fly around our hoops and stop the other\nteam from scoring."\n\n"Three Chasers, one Keeper," said Harry, who was determined to remember\nit all. "And they play with the Quaffle. Okay, got that. So what are\nthey for?" He pointed at the three balls left inside the box.\n\n"I\'ll show you now," said Wood. "Take this."\n\nHe handed Harry a small club, a bit like a short baseball bat.\n\n"I\'m going to show you what the Bludgers do," Wood said. "These two are\nthe Bludgers."\n\nHe showed Harry two identical balls, jet black and slightly smaller than\nthe red Quaffle. Harry noticed that they seemed to be straining to\nescape the straps holding them inside the box.\n\n"Stand back," Wood warned Harry. He bent down and freed one of the\nBludgers.\n\nAt once, the black ball rose high in the air and then p'}
15:51:39,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:39,542 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:39,617 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:39,617 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:39,617 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 196018, Requested 7527. Please try again in 1.063s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:51:39,617 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '"I\'m going to show you what the Bludgers do," Wood said. "These two are\nthe Bludgers."\n\nHe showed Harry two identical balls, jet black and slightly smaller than\nthe red Quaffle. Harry noticed that they seemed to be straining to\nescape the straps holding them inside the box.\n\n"Stand back," Wood warned Harry. He bent down and freed one of the\nBludgers.\n\nAt once, the black ball rose high in the air and then pelted straight at\nHarry\'s face. Harry swung at it with the bat to stop it from breaking\nhis nose, and sent it zigzagging away into the air -- it zoomed around\ntheir heads and then shot at Wood, who dived on top of it and managed to\npin it to the ground.\n\n"See?" Wood panted, forcing the struggling Bludger back into the crate\nand strapping it down safely. "The Bludgers rocket around, trying to\nknock players off their brooms. That\'s why you have two Beaters on each\nteam -- the Weasley twins are ours -- it\'s their job to protect their\nside from the Bludgers and try and knock them toward the other team. So\n-- think you\'ve got all that?"\n\n"Three Chasers try and score with the Quaffle; the Keeper guards the\ngoal posts; the Beaters keep the Bludgers away from their team," Harry\nreeled off.\n\n"Very good," said Wood.\n\n"Er -- have the Bludgers ever killed anyone?" Harry asked, hoping he\nsounded offhand.\n\n"Never at Hogwarts. We\'ve had a couple of broken jaws but nothing worse\nthan that. Now, the last member of the team is the\n\nSeeker. That\'s you. And you don\'t have to worry about the Quaffle or the\nBludgers unless they crack my head open."\n\n"Don\'t worry, the Weasleys are more than a match for the Bludgers -- I\nmean, they\'re like a pair of human Bludgers themselves."\n\nWood reached into the crate and took out the fourth and last ball.\nCompared with the Quaffle and the Bludgers, it was tiny, about the size\nof a large walnut. It was bright gold and had little fluttering silver\nwings.\n\n"This," said Wood, "is the Golden Snitch, and it\'s the most important\nball of the lot. It\'s very hard to catch because it\'s so fast and\ndifficult to see. It\'s the Seeker\'s job to catch it. You\'ve got to weave\nin and out of the Chasers, Beaters, Bludgers, and Quaffle to get it\nbefore the other team\'s Seeker, because whichever Seeker catches the\nSnitch wins his team an extra hundred and fifty points, so they\n\nnearly always win. That\'s why Seekers get fouled so much. A game of\nQuidditch only ends when the Snitch is caught, so it can go on for ages\n-- I think the record is three months, they had to keep bringing on\nsubstitutes so the players could get some sleep. "Well, that\'s it -- any\nquestions?"\n\nHarry shook his head. He understood what he had to do all right, it was\ndoing it that was going to be the problem.\n\n"We won\'t practice with the Snitch yet," said Wood, carefully shutting\nit back inside the crate, "it\'s too dark, we might lose it. Let\'s try\nyou out with a few of these."\n\nHe pulled a bag of ordinary golf balls out of his pocket and a few\nminutes later, he and Harry were up in the air, Wood throwing the golf\nballs as hard as he could in every direction for Harry to catch.\n\nHarry didn\'t miss a single one, and Wood was delighted. After half an\nhour, night had really fallen and they couldn\'t carry on.\n\n"That Quidditch cup\'ll have our name on it this year," said Wood happily\nas they trudged back up to the castle. "I wouldn\'t be surprised if you\nturn out better than Charlie Weasley, and he could have played for\nEngland if he hadn\'t gone off chasing dragons."\n\nPerhaps it was because he was now so busy, what with Quidditch practice\nthree evenings a week on top of all his homework, but Harry could hardly\nbelieve it when he realized that he\'d already been at Hogwarts two\nmonths. The castle felt more like home than Privet Drive ever had. His\nlessons, too, were becoming more and more interesting now that they had\nmastered the basics.\n\nOn Halloween morning they woke to the delicious smell of baking pumpkin\nwafting through the corridors. Even better, Professor Flitwick announced\nin Charms that he thought they were ready to start making objects fly,\nsomething they had all been dying to try since they\'d seen him make\nNeville\'s toad zoom around the classroom. Professor Flitwick put the\nclass into pairs to practice. Harry\'s partner was Seamus Finnigan (which\nwas a relief, because Neville had been trying to catch his eye). Ron,\nhowever, was to be working with Hermione Granger. It was hard to tell\nwhether Ron or Hermione was angrier about this. She hadn\'t spoken to\neither of them since the day Harry\'s broomstick had arrived.\n\n"Now, don\'t forget that nice wrist movement we\'ve been practicing!"\nsqueaked Professor Flitwick, perched on top of his pile of books as\nusual. "Swish and flick, remember, swish and flick. And saying the magic\nwords properly is very important, too -- never forget Wizard Baruffio,\nwho said \'s\''}
15:51:39,627 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:39,627 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:40,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:40,445 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:41,529 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:41,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 3.4380000000819564. input_tokens=34, output_tokens=259
15:51:43,87 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:43,87 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:43,87 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 196164, Requested 7378. Please try again in 1.062s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
15:51:43,87 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'mixture of old\nsocks and the kind of public toilet no one seems to clean.\n\nAnd then they heard it -- a low grunting, and the shuffling footfalls of\ngigantic feet. Ron pointed -- at the end of a passage to the left,\nsomething huge was moving toward them. They shrank into the shadows and\nwatched as it emerged into a patch of moonlight.\n\nIt was a horrible sight. Twelve feet tall, its skin was a dull, granite\ngray, its great lumpy body like a boulder with its small bald head\nperched on top like a coconut. It had short legs thick as tree trunks\nwith flat, horny feet. The smell coming from it was incredible. It was\nholding a huge wooden club, which dragged along the floor because its\narms were so long.\n\nThe troll stopped next to a doorway and peered inside. It waggled its\nlong ears, making up its tiny mind, then slouched slowly into the room.\n\n"The keys in the lock," Harry muttered. "We could lock it in."\n\n"Good idea," said Ron nervously.\n\nThey edged toward the open door, mouths dry, praying the troll wasn\'t\nabout to come out of it. With one great leap, Harry managed to grab the\nkey, slam the door, and lock it.\n\n\'Yes!"\n\nFlushed with their victory, they started to run back up the passage, but\nas they reached the corner they heard something that made their hearts\nstop -- a high, petrified scream -- and it was coming from the chamber\nthey\'d just chained up.\n\n"Oh, no," said Ron, pale as the Bloody Baron.\n\n"It\'s the girls\' bathroom!" Harry gasped.\n\n"Hermione!" they said together.\n\nIt was the last thing they wanted to do, but what choice did they have?\nWheeling around, they sprinted back to the door and turned the key,\nfumbling in their panic. Harry pulled the door open and they ran inside.\n\nHermione Granger was shrinking against the wall opposite, looking as if\nshe was about to faint. The troll was advancing on her, knocking the\nsinks off the walls as it went.\n\n"Confuse it!" Harry said desperately to Ron, and, seizing a tap, he\nthrew it as hard as he could against the wall.\n\nThe troll stopped a few feet from Hermione. It lumbered around, blinking\nstupidly, to see what had made the noise. Its mean little eyes saw\nHarry. It hesitated, then made for him instead, lifting its club as it\nwent.\n\n"Oy, pea-brain!" yelled Ron from the other side of the chamber, and he\nthrew a metal pipe at it. The troll didn\'t even seem to notice the pipe\nhitting its shoulder, but it heard the yell and paused again, turning\nits ugly snout toward Ron instead, giving Harry time to run around it.\n\n"Come on, run, run!" Harry yelled at Hermione, trying to pull her toward\nthe door, but she couldn\'t move, she was still flat against the wall,\nher mouth open with terror.\n\nThe shouting and the echoes seemed to be driving the troll berserk. It\nroared again and started toward Ron, who was nearest and had no way to\nescape.\n\nHarry then did something that was both very brave and very stupid: He\ntook a great running jump and managed to fasten his arms around the\ntroll\'s neck from behind. The troll couldn\'t feel Harry hanging there,\nbut even a troll will notice if you stick a long bit of wood up its\nnose, and Harry\'s wand had still been in his hand when he\'d jumped -- it\nhad gone straight up one of the troll\'s nostrils.\n\nHowling with pain, the troll twisted and flailed its club, with Harry\nclinging on for dear life; any second, the troll was going to rip him\noff or catch him a terrible blow with the club.\n\nHermione had sunk to the floor in fright; Ron pulled out his own wand --\nnot knowing what he was going to do he heard himself cry the first spell\nthat came into his head: "Wingardium Leviosa!"\n\nThe club flew suddenly out of the troll\'s hand, rose high, high up into\nthe air, turned slowly over -- and dropped, with a sickening crack, onto\nits owner\'s head. The troll swayed on the spot and then fell flat on its\nface, with a thud that made the whole room tremble.\n\nHarry got to his feet. He was shaking and out of breath. Ron was\nstanding there with his wand still raised, staring at what he had done.\n\nIt was Hermione who spoke first.\n\n"Is it -- dead?"\n\nI don\'t think so," said Harry, I think it\'s just been knocked out."\n\nHe bent down and pulled his wand out of the troll\'s nose. It was covered\nin what looked like lumpy gray glue.\n\n"Urgh -- troll boogers."\n\nHe wiped it on the troll\'s trousers.\n\nA sudden slamming and loud footsteps made the three of them look up.\nThey hadn\'t realized what a racket they had been making, but of course,\nsomeone downstairs must have heard the crashes and the troll\'s roars. A\nmoment later, Professor McGonagall had come bursting into the room,\nclosely followed by Snape, with Quirrell bringing up the rear. Quirrell\ntook one look at the troll, let out a faint whimper, and sat quickly\ndown on a toilet, clutching his heart.\n\nSnape bent over the troll. Professor McGonagall was looking at Ron and\nHarry. Harry had never seen her look so angry. Her lips were white.\nHopes of'}
15:51:43,187 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:43,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 4.859000000054948. input_tokens=34, output_tokens=317
15:51:43,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:43,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 5.48499999998603. input_tokens=34, output_tokens=414
15:51:44,45 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:44,47 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:44,97 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:51:44,97 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:51:45,674 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:45,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 2.60999999998603. input_tokens=34, output_tokens=170
15:51:46,256 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:46,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 7.593999999924563. input_tokens=34, output_tokens=401
15:51:47,200 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:47,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 5.046999999904074. input_tokens=34, output_tokens=320
15:51:48,998 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:48,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 7.797000000020489. input_tokens=34, output_tokens=506
15:51:49,37 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:49,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 4.172000000020489. input_tokens=34, output_tokens=263
15:51:53,453 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:53,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 3.827999999979511. input_tokens=34, output_tokens=253
15:51:55,662 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:55,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 5.219000000040978. input_tokens=34, output_tokens=383
15:51:56,22 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:56,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 8 retries took 7.155999999959022. input_tokens=34, output_tokens=488
15:51:56,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:56,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 7.516000000061467. input_tokens=34, output_tokens=494
15:51:58,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:58,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 4.719000000040978. input_tokens=34, output_tokens=311
15:51:59,178 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:51:59,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 8 retries took 9.625. input_tokens=2936, output_tokens=656
15:52:04,282 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:04,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.094000000040978. input_tokens=34, output_tokens=287
15:52:05,514 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:05,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 11.405999999959022. input_tokens=34, output_tokens=754
15:52:05,526 datashaper.workflow.workflow INFO executing verb merge_graphs
15:52:05,601 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
15:52:05,809 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
15:52:05,810 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
15:52:05,829 datashaper.workflow.workflow INFO executing verb summarize_descriptions
15:52:06,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:06,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7349999999860302. input_tokens=140, output_tokens=32
15:52:06,816 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:06,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9220000000204891. input_tokens=177, output_tokens=47
15:52:06,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:06,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9379999999655411. input_tokens=164, output_tokens=42
15:52:06,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:06,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.984000000054948. input_tokens=168, output_tokens=51
15:52:07,91 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1400000000139698. input_tokens=175, output_tokens=71
15:52:07,195 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2650000000139698. input_tokens=171, output_tokens=69
15:52:07,250 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.312999999965541. input_tokens=177, output_tokens=65
15:52:07,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3440000000409782. input_tokens=169, output_tokens=75
15:52:07,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4369999999180436. input_tokens=229, output_tokens=102
15:52:07,421 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.515999999945052. input_tokens=176, output_tokens=92
15:52:07,477 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6560000000754371. input_tokens=148, output_tokens=25
15:52:07,532 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,532 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.609000000054948. input_tokens=225, output_tokens=81
15:52:07,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.702999999979511. input_tokens=175, output_tokens=101
15:52:07,626 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9529999999795109. input_tokens=171, output_tokens=64
15:52:07,675 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8599999999860302. input_tokens=173, output_tokens=55
15:52:07,744 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8599999999860302. input_tokens=209, output_tokens=102
15:52:07,907 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0. input_tokens=320, output_tokens=135
15:52:07,923 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:07,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0320000000065193. input_tokens=437, output_tokens=161
15:52:08,18 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.077999999979511. input_tokens=189, output_tokens=90
15:52:08,94 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8429999999934807. input_tokens=169, output_tokens=51
15:52:08,249 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.35999999998603. input_tokens=526, output_tokens=171
15:52:08,299 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.35999999998603. input_tokens=342, output_tokens=174
15:52:08,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,415 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5. input_tokens=316, output_tokens=184
15:52:08,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1410000000614673. input_tokens=188, output_tokens=78
15:52:08,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,503 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.312999999965541. input_tokens=202, output_tokens=91
15:52:08,617 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.140999999945052. input_tokens=177, output_tokens=72
15:52:08,634 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.687999999965541. input_tokens=399, output_tokens=220
15:52:08,647 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9679999999934807. input_tokens=167, output_tokens=48
15:52:08,755 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.327999999979511. input_tokens=173, output_tokens=68
15:52:08,770 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0150000000139698. input_tokens=176, output_tokens=64
15:52:08,956 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:08,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8599999999860302. input_tokens=311, output_tokens=141
15:52:09,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.155999999959022. input_tokens=530, output_tokens=246
15:52:09,178 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.547000000020489. input_tokens=191, output_tokens=78
15:52:09,188 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2809999999590218. input_tokens=194, output_tokens=92
15:52:09,263 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.375. input_tokens=839, output_tokens=281
15:52:09,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0150000000139698. input_tokens=172, output_tokens=75
15:52:09,284 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.359000000054948. input_tokens=205, output_tokens=112
15:52:09,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0. input_tokens=314, output_tokens=141
15:52:09,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9529999999795109. input_tokens=165, output_tokens=64
15:52:09,395 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9059999999590218. input_tokens=175, output_tokens=59
15:52:09,477 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5459999999729916. input_tokens=1034, output_tokens=265
15:52:09,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9070000000065193. input_tokens=273, output_tokens=159
15:52:09,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,541 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.625. input_tokens=661, output_tokens=283
15:52:09,589 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0929999999934807. input_tokens=206, output_tokens=83
15:52:09,608 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8440000000409782. input_tokens=157, output_tokens=41
15:52:09,628 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=162, output_tokens=68
15:52:09,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,665 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.734000000054948. input_tokens=1074, output_tokens=282
15:52:09,692 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.155999999959022. input_tokens=259, output_tokens=141
15:52:09,733 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7810000000754371. input_tokens=156, output_tokens=45
15:52:09,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.437000000034459. input_tokens=249, output_tokens=105
15:52:09,903 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,907 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8910000000614673. input_tokens=467, output_tokens=128
15:52:09,956 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,956 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5939999999245629. input_tokens=145, output_tokens=29
15:52:09,995 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:09,997 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3599999999860302. input_tokens=200, output_tokens=86
15:52:10,78 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.327999999979511. input_tokens=167, output_tokens=71
15:52:10,240 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,242 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.172000000020489. input_tokens=167, output_tokens=54
15:52:10,302 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.625. input_tokens=141, output_tokens=14
15:52:10,332 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8279999999795109. input_tokens=163, output_tokens=54
15:52:10,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0619999999180436. input_tokens=198, output_tokens=75
15:52:10,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.5. input_tokens=1317, output_tokens=391
15:52:10,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.297000000020489. input_tokens=228, output_tokens=99
15:52:10,677 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.375. input_tokens=266, output_tokens=112
15:52:10,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1089999999385327. input_tokens=233, output_tokens=84
15:52:10,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:10,874 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9689999999245629. input_tokens=216, output_tokens=57
15:52:11,112 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.922000000020489. input_tokens=302, output_tokens=154
15:52:11,182 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9529999999795109. input_tokens=181, output_tokens=56
15:52:11,204 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=172, output_tokens=86
15:52:11,284 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.672000000020489. input_tokens=264, output_tokens=122
15:52:11,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6089999999385327. input_tokens=258, output_tokens=128
15:52:11,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.047000000020489. input_tokens=188, output_tokens=67
15:52:11,403 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0160000000614673. input_tokens=238, output_tokens=133
15:52:11,408 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.109000000054948. input_tokens=178, output_tokens=69
15:52:11,420 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1570000000065193. input_tokens=297, output_tokens=138
15:52:11,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,503 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.422000000020489. input_tokens=180, output_tokens=92
15:52:11,511 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.827999999979511. input_tokens=254, output_tokens=111
15:52:11,557 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.234000000054948. input_tokens=172, output_tokens=82
15:52:11,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8900000000139698. input_tokens=154, output_tokens=46
15:52:11,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.01500000001397. input_tokens=898, output_tokens=277
15:52:11,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.60999999998603. input_tokens=222, output_tokens=87
15:52:11,741 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.312000000034459. input_tokens=197, output_tokens=100
15:52:11,866 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0160000000614673. input_tokens=167, output_tokens=62
15:52:11,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.952999999979511. input_tokens=343, output_tokens=162
15:52:11,974 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:11,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0929999999934807. input_tokens=206, output_tokens=67
15:52:12,43 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.85999999998603. input_tokens=525, output_tokens=217
15:52:12,74 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.48499999998603. input_tokens=554, output_tokens=200
15:52:12,103 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5619999999180436. input_tokens=462, output_tokens=223
15:52:12,115 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5940000000409782. input_tokens=146, output_tokens=21
15:52:12,186 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.625. input_tokens=227, output_tokens=126
15:52:12,260 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8279999999795109. input_tokens=149, output_tokens=46
15:52:12,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.172000000020489. input_tokens=175, output_tokens=85
15:52:12,403 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=174, output_tokens=67
15:52:12,446 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.327999999979511. input_tokens=210, output_tokens=80
15:52:12,495 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0939999999245629. input_tokens=207, output_tokens=76
15:52:12,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,503 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1570000000065193. input_tokens=167, output_tokens=71
15:52:12,644 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=238, output_tokens=90
15:52:12,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4219999999040738. input_tokens=254, output_tokens=104
15:52:12,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.077999999979511. input_tokens=495, output_tokens=109
15:52:12,764 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.297000000020489. input_tokens=271, output_tokens=112
15:52:12,783 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5780000000959262. input_tokens=173, output_tokens=90
15:52:12,812 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.312000000034459. input_tokens=201, output_tokens=76
15:52:12,861 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.297000000020489. input_tokens=195, output_tokens=94
15:52:12,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2339999999385327. input_tokens=151, output_tokens=50
15:52:12,983 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.875. input_tokens=166, output_tokens=45
15:52:12,997 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:12,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9070000000065193. input_tokens=170, output_tokens=46
15:52:13,57 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.202999999979511. input_tokens=180, output_tokens=63
15:52:13,233 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,233 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.187000000034459. input_tokens=170, output_tokens=67
15:52:13,420 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7190000000409782. input_tokens=193, output_tokens=98
15:52:13,546 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2820000000065193. input_tokens=164, output_tokens=53
15:52:13,576 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.937999999965541. input_tokens=234, output_tokens=120
15:52:13,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5. input_tokens=205, output_tokens=99
15:52:13,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0929999999934807. input_tokens=173, output_tokens=64
15:52:13,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2189999999245629. input_tokens=193, output_tokens=71
15:52:13,758 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.047000000020489. input_tokens=171, output_tokens=60
15:52:13,779 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2810000000754371. input_tokens=161, output_tokens=83
15:52:13,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,808 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.047000000020489. input_tokens=186, output_tokens=73
15:52:13,878 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8129999999655411. input_tokens=147, output_tokens=35
15:52:13,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:13,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7809999999590218. input_tokens=346, output_tokens=146
15:52:14,65 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=209, output_tokens=84
15:52:14,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9220000000204891. input_tokens=149, output_tokens=41
15:52:14,290 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.75. input_tokens=150, output_tokens=40
15:52:14,326 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.875. input_tokens=273, output_tokens=138
15:52:14,336 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4689999999245629. input_tokens=202, output_tokens=111
15:52:14,412 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4380000000819564. input_tokens=168, output_tokens=89
15:52:14,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7190000000409782. input_tokens=175, output_tokens=110
15:52:14,449 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.48499999998603. input_tokens=289, output_tokens=152
15:52:14,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8900000000139698. input_tokens=168, output_tokens=56
15:52:14,574 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=167, output_tokens=39
15:52:14,696 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2650000000139698. input_tokens=180, output_tokens=70
15:52:14,717 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.76500000001397. input_tokens=511, output_tokens=226
15:52:14,859 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.859000000054948. input_tokens=229, output_tokens=104
15:52:14,911 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.125. input_tokens=170, output_tokens=54
15:52:14,927 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,928 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:14,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.562999999965541. input_tokens=270, output_tokens=137
15:52:14,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.172000000020489. input_tokens=201, output_tokens=77
15:52:15,149 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,149 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8429999999934807. input_tokens=175, output_tokens=44
15:52:15,186 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.375. input_tokens=203, output_tokens=94
15:52:15,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.577999999979511. input_tokens=285, output_tokens=159
15:52:15,278 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.875. input_tokens=172, output_tokens=53
15:52:15,432 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.109000000054948. input_tokens=176, output_tokens=60
15:52:15,511 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6400000000139698. input_tokens=188, output_tokens=108
15:52:15,687 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.625. input_tokens=176, output_tokens=104
15:52:15,767 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.797000000020489. input_tokens=289, output_tokens=139
15:52:15,787 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.859000000054948. input_tokens=184, output_tokens=46
15:52:15,862 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2660000000614673. input_tokens=301, output_tokens=151
15:52:15,899 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=298, output_tokens=134
15:52:15,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.155999999959022. input_tokens=178, output_tokens=66
15:52:15,950 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:15,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=204, output_tokens=86
15:52:16,70 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,71 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9220000000204891. input_tokens=173, output_tokens=66
15:52:16,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=188, output_tokens=94
15:52:16,92 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1089999999385327. input_tokens=303, output_tokens=117
15:52:16,132 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.547000000020489. input_tokens=738, output_tokens=301
15:52:16,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3439999999245629. input_tokens=164, output_tokens=92
15:52:16,261 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8279999999795109. input_tokens=169, output_tokens=41
15:52:16,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.625. input_tokens=208, output_tokens=123
15:52:16,363 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1410000000614673. input_tokens=175, output_tokens=80
15:52:16,428 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8440000000409782. input_tokens=209, output_tokens=128
15:52:16,500 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.077999999979511. input_tokens=253, output_tokens=155
15:52:16,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.047000000020489. input_tokens=238, output_tokens=177
15:52:16,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6559999999590218. input_tokens=194, output_tokens=95
15:52:16,603 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.422000000020489. input_tokens=182, output_tokens=101
15:52:16,614 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9220000000204891. input_tokens=166, output_tokens=50
15:52:16,751 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2349999999860302. input_tokens=224, output_tokens=60
15:52:16,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,939 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.859000000054948. input_tokens=178, output_tokens=63
15:52:16,962 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:16,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.015999999945052. input_tokens=217, output_tokens=65
15:52:17,12 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.077999999979511. input_tokens=171, output_tokens=103
15:52:17,65 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.202999999979511. input_tokens=179, output_tokens=77
15:52:17,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8119999999180436. input_tokens=186, output_tokens=108
15:52:17,208 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2969999999040738. input_tokens=319, output_tokens=105
15:52:17,249 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1570000000065193. input_tokens=169, output_tokens=61
15:52:17,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.312000000034459. input_tokens=255, output_tokens=99
15:52:17,270 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4839999999385327. input_tokens=165, output_tokens=55
15:52:17,428 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3599999999860302. input_tokens=208, output_tokens=90
15:52:17,557 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.062000000034459. input_tokens=168, output_tokens=69
15:52:17,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9689999999245629. input_tokens=170, output_tokens=62
15:52:17,581 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.812999999965541. input_tokens=318, output_tokens=123
15:52:17,602 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2339999999385327. input_tokens=167, output_tokens=73
15:52:17,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.859000000054948. input_tokens=176, output_tokens=65
15:52:17,624 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.047000000020489. input_tokens=182, output_tokens=85
15:52:17,767 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.625. input_tokens=171, output_tokens=66
15:52:17,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.577999999979511. input_tokens=167, output_tokens=71
15:52:17,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3599999999860302. input_tokens=168, output_tokens=65
15:52:17,911 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:17,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.297000000020489. input_tokens=176, output_tokens=95
15:52:18,237 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.172000000020489. input_tokens=189, output_tokens=85
15:52:18,284 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2660000000614673. input_tokens=240, output_tokens=122
15:52:18,314 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9690000000409782. input_tokens=240, output_tokens=135
15:52:18,403 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1410000000614673. input_tokens=188, output_tokens=88
15:52:18,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,415 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7970000000204891. input_tokens=164, output_tokens=47
15:52:18,557 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.984000000054948. input_tokens=172, output_tokens=49
15:52:18,570 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.312000000034459. input_tokens=189, output_tokens=99
15:52:18,574 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.625. input_tokens=253, output_tokens=141
15:52:18,663 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,665 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8910000000614673. input_tokens=168, output_tokens=54
15:52:18,780 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:18,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6880000000819564. input_tokens=284, output_tokens=108
15:52:19,42 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8440000000409782. input_tokens=222, output_tokens=112
15:52:19,160 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.75. input_tokens=162, output_tokens=45
15:52:19,165 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8599999999860302. input_tokens=154, output_tokens=44
15:52:19,270 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.672000000020489. input_tokens=204, output_tokens=145
15:52:19,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0469999999040738. input_tokens=170, output_tokens=67
15:52:19,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7809999999590218. input_tokens=164, output_tokens=68
15:52:19,387 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5150000000139698. input_tokens=184, output_tokens=82
15:52:19,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.077999999979511. input_tokens=166, output_tokens=81
15:52:19,549 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9690000000409782. input_tokens=186, output_tokens=116
15:52:19,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9839999999385327. input_tokens=188, output_tokens=67
15:52:19,799 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2190000000409782. input_tokens=163, output_tokens=100
15:52:19,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.39000000001397. input_tokens=523, output_tokens=214
15:52:19,877 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9689999999245629. input_tokens=176, output_tokens=78
15:52:19,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.390999999945052. input_tokens=189, output_tokens=96
15:52:19,982 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=253, output_tokens=112
15:52:19,986 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:19,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.719000000040978. input_tokens=236, output_tokens=115
15:52:20,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,48 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.422000000020489. input_tokens=302, output_tokens=212
15:52:20,97 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6709999999729916. input_tokens=457, output_tokens=215
15:52:20,124 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.077999999979511. input_tokens=167, output_tokens=73
15:52:20,202 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4219999999040738. input_tokens=172, output_tokens=102
15:52:20,207 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.375. input_tokens=163, output_tokens=46
15:52:20,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.687999999965541. input_tokens=206, output_tokens=89
15:52:20,509 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1099999999860302. input_tokens=176, output_tokens=50
15:52:20,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=174, output_tokens=68
15:52:20,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,532 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.375. input_tokens=165, output_tokens=80
15:52:20,536 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8910000000614673. input_tokens=168, output_tokens=51
15:52:20,780 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.844000000040978. input_tokens=647, output_tokens=261
15:52:20,840 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9679999999934807. input_tokens=159, output_tokens=61
15:52:20,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7959999999729916. input_tokens=164, output_tokens=45
15:52:20,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,939 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.984000000054948. input_tokens=160, output_tokens=56
15:52:20,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6099999999860302. input_tokens=185, output_tokens=47
15:52:20,991 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:20,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=232, output_tokens=69
15:52:21,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.687000000034459. input_tokens=198, output_tokens=129
15:52:21,67 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9370000000344589. input_tokens=193, output_tokens=64
15:52:21,82 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5309999999590218. input_tokens=188, output_tokens=95
15:52:21,157 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.172000000020489. input_tokens=202, output_tokens=82
15:52:21,223 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0150000000139698. input_tokens=191, output_tokens=58
15:52:21,332 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8279999999795109. input_tokens=169, output_tokens=53
15:52:21,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2959999999729916. input_tokens=208, output_tokens=107
15:52:21,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.984000000054948. input_tokens=456, output_tokens=293
15:52:21,595 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.327999999979511. input_tokens=236, output_tokens=116
15:52:21,797 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.265999999945052. input_tokens=188, output_tokens=75
15:52:21,833 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0469999999040738. input_tokens=158, output_tokens=60
15:52:21,840 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6400000000139698. input_tokens=204, output_tokens=93
15:52:21,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,849 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3589999999385327. input_tokens=233, output_tokens=163
15:52:21,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:21,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.875. input_tokens=183, output_tokens=51
15:52:22,39 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5160000000614673. input_tokens=176, output_tokens=70
15:52:22,106 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2660000000614673. input_tokens=191, output_tokens=84
15:52:22,124 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.875. input_tokens=172, output_tokens=114
15:52:22,162 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.625. input_tokens=282, output_tokens=104
15:52:22,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2190000000409782. input_tokens=177, output_tokens=77
15:52:22,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.859000000054948. input_tokens=170, output_tokens=55
15:52:22,242 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3910000000614673. input_tokens=200, output_tokens=104
15:52:22,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.172000000020489. input_tokens=174, output_tokens=70
15:52:22,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8129999999655411. input_tokens=177, output_tokens=45
15:52:22,500 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4849999999860302. input_tokens=155, output_tokens=56
15:52:22,537 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0630000000819564. input_tokens=169, output_tokens=83
15:52:22,611 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.797000000020489. input_tokens=356, output_tokens=172
15:52:22,749 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.687999999965541. input_tokens=266, output_tokens=113
15:52:22,920 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8129999999655411. input_tokens=198, output_tokens=41
15:52:22,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:22,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7349999999860302. input_tokens=160, output_tokens=49
15:52:23,15 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,15 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8429999999934807. input_tokens=171, output_tokens=52
15:52:23,33 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.234000000054948. input_tokens=427, output_tokens=151
15:52:23,45 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=178, output_tokens=70
15:52:23,165 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5790000000270084. input_tokens=223, output_tokens=102
15:52:23,314 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7029999999795109. input_tokens=173, output_tokens=40
15:52:23,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7969999999040738. input_tokens=177, output_tokens=47
15:52:23,342 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1089999999385327. input_tokens=165, output_tokens=73
15:52:23,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4839999999385327. input_tokens=201, output_tokens=115
15:52:23,364 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.844000000040978. input_tokens=323, output_tokens=190
15:52:23,369 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.187999999965541. input_tokens=162, output_tokens=60
15:52:23,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.469000000040978. input_tokens=199, output_tokens=104
15:52:23,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.265999999945052. input_tokens=178, output_tokens=108
15:52:23,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.187000000034459. input_tokens=163, output_tokens=75
15:52:23,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,483 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.984000000054948. input_tokens=204, output_tokens=66
15:52:23,593 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5619999999180436. input_tokens=187, output_tokens=104
15:52:23,665 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7190000000409782. input_tokens=169, output_tokens=43
15:52:23,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7809999999590218. input_tokens=159, output_tokens=39
15:52:23,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,874 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.577999999979511. input_tokens=206, output_tokens=81
15:52:23,967 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:23,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9529999999795109. input_tokens=166, output_tokens=64
15:52:24,13 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2650000000139698. input_tokens=223, output_tokens=76
15:52:24,159 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.125. input_tokens=178, output_tokens=67
15:52:24,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8900000000139698. input_tokens=175, output_tokens=63
15:52:24,269 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9220000000204891. input_tokens=169, output_tokens=49
15:52:24,287 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7970000000204891. input_tokens=170, output_tokens=48
15:52:24,288 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.188000000081956. input_tokens=170, output_tokens=88
15:52:24,306 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1400000000139698. input_tokens=162, output_tokens=60
15:52:24,316 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9690000000409782. input_tokens=174, output_tokens=61
15:52:24,318 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3900000000139698. input_tokens=274, output_tokens=89
15:52:24,369 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.015999999945052. input_tokens=170, output_tokens=63
15:52:24,398 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9679999999934807. input_tokens=205, output_tokens=59
15:52:24,507 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.672000000020489. input_tokens=184, output_tokens=74
15:52:24,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6880000000819564. input_tokens=285, output_tokens=168
15:52:24,541 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2190000000409782. input_tokens=254, output_tokens=81
15:52:24,587 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=174, output_tokens=51
15:52:24,614 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.781000000075437. input_tokens=162, output_tokens=87
15:52:24,638 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2339999999385327. input_tokens=194, output_tokens=80
15:52:24,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.593999999924563. input_tokens=379, output_tokens=204
15:52:24,963 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:24,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8429999999934807. input_tokens=205, output_tokens=142
15:52:25,147 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:25,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2650000000139698. input_tokens=195, output_tokens=76
15:52:25,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:25,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9529999999795109. input_tokens=171, output_tokens=68
15:52:25,224 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:25,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5459999999729916. input_tokens=175, output_tokens=60
15:52:25,246 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:25,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2820000000065193. input_tokens=171, output_tokens=61
15:52:25,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:25,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4690000000409782. input_tokens=161, output_tokens=96
15:52:25,313 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:25,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1559999999590218. input_tokens=182, output_tokens=59
15:52:25,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:25,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3440000000409782. input_tokens=175, output_tokens=54
15:52:25,836 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:25,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.515999999945052. input_tokens=236, output_tokens=157
15:52:25,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:25,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6099999999860302. input_tokens=209, output_tokens=111
15:52:26,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:26,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.125. input_tokens=175, output_tokens=88
15:52:26,595 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
15:52:26,796 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
15:52:26,797 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
15:52:26,820 datashaper.workflow.workflow INFO executing verb cluster_graph
15:52:27,224 datashaper.workflow.workflow INFO executing verb select
15:52:27,228 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
15:52:27,465 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
15:52:27,466 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
15:52:27,492 datashaper.workflow.workflow INFO executing verb unpack_graph
15:52:27,622 datashaper.workflow.workflow INFO executing verb rename
15:52:27,628 datashaper.workflow.workflow INFO executing verb select
15:52:27,637 datashaper.workflow.workflow INFO executing verb dedupe
15:52:27,648 datashaper.workflow.workflow INFO executing verb rename
15:52:27,655 datashaper.workflow.workflow INFO executing verb filter
15:52:27,673 datashaper.workflow.workflow INFO executing verb text_split
15:52:27,687 datashaper.workflow.workflow INFO executing verb drop
15:52:27,694 datashaper.workflow.workflow INFO executing verb merge
15:52:27,745 datashaper.workflow.workflow INFO executing verb text_embed
15:52:27,749 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
15:52:27,891 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
15:52:27,891 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
15:52:27,915 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 362 inputs via 362 snippets using 23 batches. max_batch_size=16, max_tokens=8191
15:52:28,461 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,464 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,478 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,478 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,497 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,534 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.625. input_tokens=741, output_tokens=0
15:52:28,584 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,585 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,617 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6720000000204891. input_tokens=423, output_tokens=0
15:52:28,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7179999999934807. input_tokens=699, output_tokens=0
15:52:28,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7190000000409782. input_tokens=1242, output_tokens=0
15:52:28,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.75. input_tokens=1718, output_tokens=0
15:52:28,690 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.75. input_tokens=383, output_tokens=0
15:52:28,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7809999999590218. input_tokens=820, output_tokens=0
15:52:28,711 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,715 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7959999999729916. input_tokens=415, output_tokens=0
15:52:28,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7959999999729916. input_tokens=428, output_tokens=0
15:52:28,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,754 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,755 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,756 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8279999999795109. input_tokens=779, output_tokens=0
15:52:28,774 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8279999999795109. input_tokens=861, output_tokens=0
15:52:28,790 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,795 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8900000000139698. input_tokens=984, output_tokens=0
15:52:28,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9529999999795109. input_tokens=943, output_tokens=0
15:52:28,903 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0. input_tokens=957, output_tokens=0
15:52:28,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0150000000139698. input_tokens=1642, output_tokens=0
15:52:28,949 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:28,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0309999999590218. input_tokens=472, output_tokens=0
15:52:28,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0459999999729916. input_tokens=852, output_tokens=0
15:52:28,986 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:29,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.140999999945052. input_tokens=885, output_tokens=0
15:52:29,94 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:29,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:29,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.202999999979511. input_tokens=1158, output_tokens=0
15:52:29,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2179999999934807. input_tokens=1757, output_tokens=0
15:52:29,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.265999999945052. input_tokens=475, output_tokens=0
15:52:29,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:29,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5459999999729916. input_tokens=807, output_tokens=0
15:52:30,7 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
15:52:30,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.2179999999934807. input_tokens=1037, output_tokens=0
15:52:30,163 datashaper.workflow.workflow INFO executing verb drop
15:52:30,175 datashaper.workflow.workflow INFO executing verb filter
15:52:30,189 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
15:52:30,440 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
15:52:30,444 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
15:52:30,469 datashaper.workflow.workflow INFO executing verb layout_graph
15:52:31,232 datashaper.workflow.workflow INFO executing verb unpack_graph
15:52:31,409 datashaper.workflow.workflow INFO executing verb unpack_graph
15:52:31,763 datashaper.workflow.workflow INFO executing verb filter
15:52:31,800 datashaper.workflow.workflow INFO executing verb drop
15:52:31,809 datashaper.workflow.workflow INFO executing verb select
15:52:31,828 datashaper.workflow.workflow INFO executing verb rename
15:52:31,847 datashaper.workflow.workflow INFO executing verb join
15:52:31,869 datashaper.workflow.workflow INFO executing verb convert
15:52:31,907 datashaper.workflow.workflow INFO executing verb rename
15:52:31,907 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
15:52:32,157 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
15:52:32,157 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
15:52:32,204 datashaper.workflow.workflow INFO executing verb create_final_communities
15:52:32,509 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
15:52:32,721 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
15:52:32,721 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
15:52:32,736 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
15:52:32,771 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
15:52:32,922 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
15:52:32,930 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
15:52:33,120 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
15:52:33,121 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
15:52:33,123 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
15:52:33,156 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
15:52:33,188 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
15:52:33,238 datashaper.workflow.workflow INFO executing verb select
15:52:33,241 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
15:52:33,465 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
15:52:33,467 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
15:52:33,475 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
15:52:33,503 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
15:52:33,538 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
15:52:33,558 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
15:52:33,577 datashaper.workflow.workflow INFO executing verb prepare_community_reports
15:52:33,578 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=5 => 362
15:52:33,603 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=4 => 362
15:52:33,622 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 362
15:52:33,653 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 362
15:52:33,775 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 362
15:52:33,950 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 362
15:52:34,32 datashaper.workflow.workflow INFO executing verb create_community_reports
15:52:45,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:45,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.045999999972992. input_tokens=3505, output_tokens=787
15:52:46,15 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:46,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.96799999999348. input_tokens=9157, output_tokens=890
15:52:55,153 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:55,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.094000000040978. input_tokens=2110, output_tokens=655
15:52:58,949 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:52:58,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.890999999945052. input_tokens=4271, output_tokens=914
15:53:10,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:10,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.875. input_tokens=2215, output_tokens=596
15:53:11,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:11,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.327999999979511. input_tokens=2177, output_tokens=635
15:53:13,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:13,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.266000000061467. input_tokens=2419, output_tokens=784
15:53:14,97 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:14,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.077999999979511. input_tokens=3242, output_tokens=848
15:53:15,397 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:15,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.358999999938533. input_tokens=2900, output_tokens=915
15:53:15,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:15,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.93799999996554. input_tokens=6767, output_tokens=982
15:53:16,880 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:16,881 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:25,35 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:25,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.531000000075437. input_tokens=2226, output_tokens=592
15:53:25,746 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:25,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.28200000000652. input_tokens=2381, output_tokens=629
15:53:26,622 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:26,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.172000000020489. input_tokens=2153, output_tokens=696
15:53:27,256 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:27,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.765999999945052. input_tokens=3042, output_tokens=735
15:53:27,666 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:27,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.203000000095926. input_tokens=2358, output_tokens=756
15:53:27,696 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:27,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.187999999965541. input_tokens=3599, output_tokens=746
15:53:28,33 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:28,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.578000000095926. input_tokens=2916, output_tokens=810
15:53:28,112 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:28,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.60999999998603. input_tokens=2686, output_tokens=748
15:53:28,617 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:28,617 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.172000000020489. input_tokens=3596, output_tokens=803
15:53:28,696 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:28,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.234000000054948. input_tokens=2722, output_tokens=819
15:53:29,184 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:29,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.75. input_tokens=4990, output_tokens=857
15:53:29,263 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:29,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.797000000020489. input_tokens=2636, output_tokens=854
15:53:29,561 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:29,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.140999999945052. input_tokens=4030, output_tokens=850
15:53:29,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:29,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.359000000054948. input_tokens=2506, output_tokens=723
15:53:30,366 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:30,383 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.843999999924563. input_tokens=9792, output_tokens=828
15:53:30,604 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:30,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.141000000061467. input_tokens=6775, output_tokens=840
15:53:30,730 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:30,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.25. input_tokens=2855, output_tokens=780
15:53:30,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:30,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.358999999938533. input_tokens=4714, output_tokens=759
15:53:30,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:30,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.327999999979511. input_tokens=2968, output_tokens=915
15:53:31,441 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:31,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 13.375. input_tokens=2440, output_tokens=682
15:53:31,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:31,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.01500000001397. input_tokens=7013, output_tokens=919
15:53:32,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:32,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.89000000001397. input_tokens=7161, output_tokens=965
15:53:32,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:32,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.31299999996554. input_tokens=5661, output_tokens=862
15:53:34,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:34,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.04700000002049. input_tokens=4071, output_tokens=1058
15:53:35,7 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:35,7 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:35,23 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:35,23 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:35,38 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:35,38 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:35,38 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:35,38 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:35,38 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:35,38 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:35,54 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:35,54 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:36,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:36,264 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:36,624 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:36,624 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:36,750 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:36,750 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:36,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:36,942 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:37,49 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:37,49 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:39,142 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:39,142 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:39,222 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:39,222 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:39,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:39,389 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:39,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:39,531 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:40,956 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:40,956 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.312999999965541. input_tokens=2049, output_tokens=454
15:53:41,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:41,698 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.046999999904074. input_tokens=2062, output_tokens=483
15:53:42,735 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:42,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.047000000020489. input_tokens=2171, output_tokens=566
15:53:43,853 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:43,853 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:43,931 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
15:53:43,947 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
15:53:44,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:44,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.25. input_tokens=2315, output_tokens=744
15:53:45,66 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:45,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.39000000001397. input_tokens=2702, output_tokens=695
15:53:45,207 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:45,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.546999999904074. input_tokens=2372, output_tokens=763
15:53:45,976 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:45,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.311999999918044. input_tokens=3188, output_tokens=813
15:53:46,404 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:46,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.766000000061467. input_tokens=9021, output_tokens=884
15:53:46,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:46,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.280999999959022. input_tokens=4260, output_tokens=874
15:53:47,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:47,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.422000000020489. input_tokens=2874, output_tokens=752
15:53:47,239 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:47,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.577999999979511. input_tokens=4841, output_tokens=908
15:53:47,333 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:47,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.687999999965541. input_tokens=6475, output_tokens=869
15:53:47,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:47,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.844000000040978. input_tokens=4023, output_tokens=876
15:53:47,770 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:47,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.09299999999348. input_tokens=3896, output_tokens=794
15:53:49,233 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:49,233 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.562000000034459. input_tokens=4206, output_tokens=870
15:53:53,304 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:53,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 16.68700000003446. input_tokens=3758, output_tokens=839
15:53:55,109 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:55,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 11.422000000020489. input_tokens=5800, output_tokens=834
15:53:56,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:56,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 12.781000000075437. input_tokens=6587, output_tokens=889
15:53:57,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:53:57,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 18.219000000040978. input_tokens=5324, output_tokens=915
15:54:03,947 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:03,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 11.765999999945052. input_tokens=8933, output_tokens=836
15:54:04,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:04,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 12.094000000040978. input_tokens=8606, output_tokens=884
15:54:12,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:12,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.686999999918044. input_tokens=2064, output_tokens=529
15:54:14,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:14,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.48499999998603. input_tokens=2398, output_tokens=646
15:54:14,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:14,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.968999999924563. input_tokens=2509, output_tokens=695
15:54:15,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:15,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.796999999904074. input_tokens=2580, output_tokens=744
15:54:19,176 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:19,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.28200000000652. input_tokens=4238, output_tokens=959
15:54:19,319 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:19,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.405999999959022. input_tokens=6711, output_tokens=895
15:54:21,969 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:21,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.061999999918044. input_tokens=6947, output_tokens=828
15:54:22,4 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:22,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.07799999997951. input_tokens=6082, output_tokens=850
15:54:24,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
15:54:24,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.5. input_tokens=9437, output_tokens=1028
15:54:24,474 datashaper.workflow.workflow INFO executing verb window
15:54:24,474 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
15:54:24,727 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
15:54:24,727 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
15:54:24,790 datashaper.workflow.workflow INFO executing verb unroll
15:54:24,806 datashaper.workflow.workflow INFO executing verb select
15:54:24,823 datashaper.workflow.workflow INFO executing verb rename
15:54:24,838 datashaper.workflow.workflow INFO executing verb join
15:54:24,869 datashaper.workflow.workflow INFO executing verb aggregate_override
15:54:24,900 datashaper.workflow.workflow INFO executing verb join
15:54:24,932 datashaper.workflow.workflow INFO executing verb rename
15:54:24,948 datashaper.workflow.workflow INFO executing verb convert
15:54:24,979 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
15:54:25,199 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
15:54:25,199 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
15:54:25,262 datashaper.workflow.workflow INFO executing verb rename
15:54:25,262 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
15:54:25,342 graphrag.index.cli INFO All workflows completed successfully.
