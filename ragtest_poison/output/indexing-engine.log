21:35:11,884 graphrag.index.cli INFO Logging enabled at C:\Other\CSCI 566\graphrag\ragtest_poison\output\indexing-engine.log
21:35:11,889 graphrag.index.cli INFO Starting pipeline run for: 20241101-213511, dryrun=False
21:35:11,890 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Other\\CSCI 566\\graphrag\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Other\\CSCI 566\\graphrag\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Other\\CSCI 566\\graphrag\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:35:11,915 graphrag.index.create_pipeline_config INFO skipping workflows 
21:35:11,915 graphrag.index.run.run INFO Running pipeline
21:35:11,916 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Other\CSCI 566\graphrag\ragtest_poison\output
21:35:11,917 graphrag.index.input.load_input INFO loading input from root_dir=input
21:35:11,918 graphrag.index.input.load_input INFO using file storage for input
21:35:11,921 graphrag.index.storage.file_pipeline_storage INFO search C:\Other\CSCI 566\graphrag\ragtest_poison\input for files matching .*\.txt$
21:35:11,923 graphrag.index.input.text INFO found text files from input, found [('1.txt', {}), ('10.txt', {}), ('11.txt', {}), ('12.txt', {}), ('13.txt', {}), ('14.txt', {}), ('15.txt', {}), ('16.txt', {}), ('17.txt', {}), ('18.txt', {}), ('19.txt', {}), ('2.txt', {}), ('20.txt', {}), ('21.txt', {}), ('22.txt', {}), ('23.txt', {}), ('24.txt', {}), ('25.txt', {}), ('26.txt', {}), ('27.txt', {}), ('28.txt', {}), ('29.txt', {}), ('3.txt', {}), ('30.txt', {}), ('31.txt', {}), ('32.txt', {}), ('33.txt', {}), ('34.txt', {}), ('35.txt', {}), ('36.txt', {}), ('37.txt', {}), ('38.txt', {}), ('39.txt', {}), ('4.txt', {}), ('40.txt', {}), ('41.txt', {}), ('42.txt', {}), ('43.txt', {}), ('44.txt', {}), ('45.txt', {}), ('46.txt', {}), ('47.txt', {}), ('48.txt', {}), ('49.txt', {}), ('5.txt', {}), ('50.txt', {}), ('51.txt', {}), ('52.txt', {}), ('53.txt', {}), ('54.txt', {}), ('55.txt', {}), ('56.txt', {}), ('57.txt', {}), ('58.txt', {}), ('59.txt', {}), ('6.txt', {}), ('60.txt', {}), ('61.txt', {}), ('62.txt', {}), ('63.txt', {}), ('64.txt', {}), ('65.txt', {}), ('66.txt', {}), ('67.txt', {}), ('68.txt', {}), ('69.txt', {}), ('7.txt', {}), ('70.txt', {}), ('71.txt', {}), ('72.txt', {}), ('73.txt', {}), ('74.txt', {}), ('75.txt', {}), ('76.txt', {}), ('77.txt', {}), ('78.txt', {}), ('8.txt', {}), ('9.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:35:11,953 graphrag.index.input.text WARNING Warning! Error loading file 19.txt. Skipping...
21:35:11,987 graphrag.index.input.text WARNING Warning! Error loading file 37.txt. Skipping...
21:35:12,15 graphrag.index.input.text WARNING Warning! Error loading file 51.txt. Skipping...
21:35:12,25 graphrag.index.input.text WARNING Warning! Error loading file 59.txt. Skipping...
21:35:12,77 graphrag.index.input.text INFO Found 79 files, loading 75
21:35:12,78 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:35:12,78 graphrag.index.run.run INFO Final # of rows loaded: 75
21:35:12,381 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:35:12,388 datashaper.workflow.workflow INFO executing verb orderby
21:35:12,392 datashaper.workflow.workflow INFO executing verb zip
21:35:12,397 datashaper.workflow.workflow INFO executing verb aggregate_override
21:35:12,406 datashaper.workflow.workflow INFO executing verb chunk
21:35:12,665 datashaper.workflow.workflow INFO executing verb select
21:35:12,669 datashaper.workflow.workflow INFO executing verb unroll
21:35:12,676 datashaper.workflow.workflow INFO executing verb rename
21:35:12,679 datashaper.workflow.workflow INFO executing verb genid
21:35:12,687 datashaper.workflow.workflow INFO executing verb unzip
21:35:12,692 datashaper.workflow.workflow INFO executing verb copy
21:35:12,696 datashaper.workflow.workflow INFO executing verb filter
21:35:12,708 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:35:12,939 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
21:35:12,940 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:35:12,970 datashaper.workflow.workflow INFO executing verb entity_extract
21:35:12,984 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
21:35:13,149 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
21:35:13,149 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
21:35:14,594 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:14,598 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.375. input_tokens=1798, output_tokens=97
21:35:14,618 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:14,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.4069999999992433. input_tokens=1809, output_tokens=97
21:35:14,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:14,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.4690000000009604. input_tokens=1813, output_tokens=89
21:35:14,845 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:14,846 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6559999999990396. input_tokens=1803, output_tokens=126
21:35:15,66 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:15,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.875. input_tokens=1796, output_tokens=144
21:35:15,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:15,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.29700000000048. input_tokens=1809, output_tokens=207
21:35:15,576 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:15,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.3440000000009604. input_tokens=1807, output_tokens=218
21:35:15,647 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:15,649 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.42200000000048. input_tokens=1801, output_tokens=224
21:35:15,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:15,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.4689999999991414. input_tokens=1807, output_tokens=229
21:35:15,715 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:15,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.5. input_tokens=1814, output_tokens=247
21:35:16,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:16,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.686999999999898. input_tokens=1808, output_tokens=398
21:35:17,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:17,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.813000000000102. input_tokens=1821, output_tokens=415
21:35:17,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:17,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.8439999999991414. input_tokens=2936, output_tokens=354
21:35:17,945 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:17,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.688000000000102. input_tokens=2936, output_tokens=510
21:35:18,579 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:18,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.32799999999952. input_tokens=2936, output_tokens=585
21:35:18,638 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:18,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.390000000001237. input_tokens=2936, output_tokens=630
21:35:18,706 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:18,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.4210000000002765. input_tokens=2935, output_tokens=637
21:35:18,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:18,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.515999999999622. input_tokens=2934, output_tokens=637
21:35:19,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.764999999999418. input_tokens=2937, output_tokens=644
21:35:19,171 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:19,173 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:19,298 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.7809999999990396. input_tokens=2935, output_tokens=449
21:35:19,498 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.265000000001237. input_tokens=2936, output_tokens=742
21:35:19,507 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.906000000000859. input_tokens=2936, output_tokens=622
21:35:19,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.281999999999243. input_tokens=2936, output_tokens=699
21:35:19,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.32799999999952. input_tokens=2936, output_tokens=768
21:35:19,659 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:19,660 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:19,731 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:19,732 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:19,760 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.515000000001237. input_tokens=2936, output_tokens=760
21:35:19,793 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.5. input_tokens=2935, output_tokens=788
21:35:19,909 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.29700000000048. input_tokens=2936, output_tokens=602
21:35:19,985 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:19,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.921999999998661. input_tokens=2936, output_tokens=588
21:35:20,22 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:20,22 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:20,109 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:20,110 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:20,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:20,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.890000000001237. input_tokens=2934, output_tokens=824
21:35:20,165 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:20,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.515999999999622. input_tokens=2936, output_tokens=554
21:35:20,549 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:20,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.70299999999952. input_tokens=2935, output_tokens=699
21:35:20,672 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:20,673 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:20,809 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:20,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.233999999998559. input_tokens=2936, output_tokens=639
21:35:21,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:21,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.313000000000102. input_tokens=2937, output_tokens=663
21:35:21,77 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:21,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.406000000000859. input_tokens=2936, output_tokens=612
21:35:21,92 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:21,96 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.843999999999141. input_tokens=2936, output_tokens=929
21:35:21,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:21,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.07799999999952. input_tokens=2936, output_tokens=509
21:35:21,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:21,142 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:21,205 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:21,206 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:21,207 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:21,207 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:21,227 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:21,227 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:21,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:21,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.625. input_tokens=34, output_tokens=179
21:35:21,663 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:21,665 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:21,683 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:21,684 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:21,742 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:21,743 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:21,987 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:21,988 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:22,406 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:22,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.7039999999997235. input_tokens=2936, output_tokens=422
21:35:22,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:22,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.2340000000003783. input_tokens=34, output_tokens=127
21:35:22,673 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:22,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.186999999999898. input_tokens=2936, output_tokens=562
21:35:22,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:22,786 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:22,798 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:22,799 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:22,813 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:22,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.6560000000008586. input_tokens=34, output_tokens=316
21:35:22,949 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:22,950 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:23,64 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:23,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.25. input_tokens=34, output_tokens=248
21:35:23,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:23,81 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:23,166 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:23,168 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:23,182 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:23,184 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:23,213 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:23,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.264999999999418. input_tokens=2936, output_tokens=567
21:35:23,227 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:23,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.640999999999622. input_tokens=2936, output_tokens=534
21:35:23,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:23,349 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:23,395 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:23,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.75. input_tokens=2936, output_tokens=588
21:35:23,495 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:23,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.609999999998763. input_tokens=2936, output_tokens=819
21:35:23,618 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:23,620 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:23,964 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:23,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.82799999999952. input_tokens=34, output_tokens=414
21:35:24,65 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:24,67 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:24,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:24,105 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:24,304 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:24,305 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:24,351 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:24,352 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:24,417 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:24,419 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:24,436 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:24,438 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.0310000000008586. input_tokens=34, output_tokens=236
21:35:24,608 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:24,609 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:24,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:24,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.171999999998661. input_tokens=2935, output_tokens=566
21:35:24,770 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:24,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.0. input_tokens=34, output_tokens=214
21:35:24,815 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:24,817 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:24,887 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:24,888 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:25,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:25,91 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:25,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:25,148 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.390999999999622. input_tokens=2937, output_tokens=644
21:35:25,194 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:25,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.406000000000859. input_tokens=2936, output_tokens=742
21:35:25,297 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:25,297 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:25,303 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:25,303 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:25,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:25,376 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:25,407 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:25,408 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:25,510 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:25,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 1.6869999999998981. input_tokens=34, output_tokens=166
21:35:25,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:25,838 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:25,934 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:25,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.54700000000048. input_tokens=34, output_tokens=287
21:35:26,58 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:26,58 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:26,59 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:26,59 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:26,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:26,96 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:26,164 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:26,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.5159999999996217. input_tokens=34, output_tokens=362
21:35:26,490 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:26,491 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:26,694 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:26,694 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:26,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:26,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.45299999999952. input_tokens=2935, output_tokens=869
21:35:26,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:26,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.514999999999418. input_tokens=34, output_tokens=282
21:35:27,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:27,75 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:27,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:27,274 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:27,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:27,349 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:27,437 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:27,439 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:27,533 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:27,533 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:27,810 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:27,811 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:28,66 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:28,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 7.04700000000048. input_tokens=2936, output_tokens=808
21:35:28,111 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:28,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.813000000000102. input_tokens=2936, output_tokens=947
21:35:28,231 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:28,235 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:28,316 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:28,318 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:28,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:28,323 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:28,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:28,341 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:28,354 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:28,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 7.436999999999898. input_tokens=2936, output_tokens=855
21:35:28,767 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:28,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.2659999999996217. input_tokens=34, output_tokens=344
21:35:28,840 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:28,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.625. input_tokens=34, output_tokens=593
21:35:28,881 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:28,881 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:28,948 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:28,949 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:28,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:28,962 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:29,94 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:29,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.95299999999952. input_tokens=34, output_tokens=408
21:35:29,115 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:29,117 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 4.45299999999952. input_tokens=34, output_tokens=451
21:35:29,153 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:29,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.0. input_tokens=34, output_tokens=324
21:35:29,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:29,282 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:29,289 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:29,289 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:29,450 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:29,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.485000000000582. input_tokens=34, output_tokens=279
21:35:29,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:29,575 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:29,888 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:29,889 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:29,995 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:29,996 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:30,27 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:30,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.9529999999995198. input_tokens=34, output_tokens=201
21:35:30,43 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:30,44 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:30,143 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:30,147 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:30,269 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:30,270 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:30,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:30,331 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:30,354 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:30,355 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:30,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:30,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 1.8119999999998981. input_tokens=34, output_tokens=155
21:35:30,588 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:30,588 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:30,783 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:30,783 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:30,786 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:30,787 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:30,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:30,899 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:31,139 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:31,139 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.7810000000008586. input_tokens=34, output_tokens=258
21:35:31,321 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:31,322 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:31,375 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:31,376 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:31,550 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:31,551 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:31,744 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:31,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 2.1719999999986612. input_tokens=34, output_tokens=208
21:35:32,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:32,24 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:32,137 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:32,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.04700000000048. input_tokens=34, output_tokens=375
21:35:32,157 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:32,157 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:32,247 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:32,247 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:32,256 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:32,257 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:32,357 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:32,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 2.2189999999991414. input_tokens=34, output_tokens=214
21:35:32,477 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:32,478 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:32,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:32,806 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:33,37 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:33,38 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:33,72 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:33,73 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:33,225 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:33,226 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:33,644 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:33,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.5. input_tokens=34, output_tokens=299
21:35:33,733 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:33,734 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:33,767 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:33,768 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:33,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:33,858 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:33,880 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:33,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.140000000001237. input_tokens=34, output_tokens=234
21:35:34,77 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:34,78 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:34,114 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:34,115 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:34,218 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:34,219 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 1.9220000000004802. input_tokens=34, output_tokens=208
21:35:34,297 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:34,298 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:34,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:34,331 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:34,441 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:34,441 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:34,663 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:34,664 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:34,714 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:34,715 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:35,481 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:35,482 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:35,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:35,631 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:36,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:36,432 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:36,798 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:36,799 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:37,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:37,141 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:37,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:37,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 2.25. input_tokens=34, output_tokens=268
21:35:37,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:37,274 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:37,284 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:37,285 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:37,476 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:37,476 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:37,770 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:37,771 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:38,262 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:38,263 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:38,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:38,845 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:38,901 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:38,901 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:38,981 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:38,983 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:38,996 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:38,997 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 5.375. input_tokens=34, output_tokens=667
21:35:39,11 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:39,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.125. input_tokens=34, output_tokens=572
21:35:39,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:39,133 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:39,213 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:39,214 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:39,251 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:39,251 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:39,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:39,332 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:39,507 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:39,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.390000000001237. input_tokens=2935, output_tokens=825
21:35:39,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:39,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 2.7340000000003783. input_tokens=34, output_tokens=289
21:35:39,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:39,879 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:40,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:40,331 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:40,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:40,338 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:40,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:40,406 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:41,397 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:41,398 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:41,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:41,410 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:41,551 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:41,553 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:41,694 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:41,695 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:41,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:41,961 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:42,187 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:42,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.2030000000013388. input_tokens=34, output_tokens=324
21:35:42,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:42,757 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:42,945 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:42,946 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:43,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:43,326 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:43,766 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:43,767 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:43,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:43,821 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:43,962 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:43,963 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:43,983 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:43,984 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 3.0469999999986612. input_tokens=34, output_tokens=291
21:35:44,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:44,102 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:45,527 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:45,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.32799999999952. input_tokens=34, output_tokens=330
21:35:45,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:45,641 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:45,700 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:45,701 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:45,763 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:45,764 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:46,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:46,274 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:46,526 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:46,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 8.484000000000378. input_tokens=2936, output_tokens=950
21:35:46,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:46,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 3.07799999999952. input_tokens=34, output_tokens=330
21:35:47,175 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:47,176 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:47,398 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:47,399 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:47,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:47,501 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:47,611 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:47,611 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:47,661 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:47,662 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:47,875 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:47,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.375. input_tokens=2936, output_tokens=724
21:35:47,888 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:47,888 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:47,956 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:47,959 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:48,11 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:48,12 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:48,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:48,275 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:49,2 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:49,3 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:49,747 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:49,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 4.296999999998661. input_tokens=34, output_tokens=388
21:35:49,865 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:49,866 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:49,925 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:49,926 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:50,468 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:50,469 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:50,547 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:50,547 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:51,537 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:51,538 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:51,688 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:51,689 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:52,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:52,85 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:52,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:52,142 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.625. input_tokens=34, output_tokens=344
21:35:52,192 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:52,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.21900000000096. input_tokens=34, output_tokens=600
21:35:53,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:53,56 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:53,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:53,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.688000000000102. input_tokens=34, output_tokens=393
21:35:53,700 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:53,701 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 5.735000000000582. input_tokens=2937, output_tokens=652
21:35:53,895 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:53,896 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:54,929 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:54,930 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:56,751 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:56,752 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:57,523 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:57,524 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:57,741 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:57,741 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:57,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:57,805 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:58,689 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:58,690 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:35:58,727 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:35:58,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 5.015999999999622. input_tokens=34, output_tokens=551
21:35:59,727 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:35:59,727 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:00,238 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:00,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 1.25. input_tokens=34, output_tokens=99
21:36:00,601 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:00,601 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:00,725 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:00,725 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:01,118 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:01,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 8.375. input_tokens=2936, output_tokens=728
21:36:01,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:01,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 3.360000000000582. input_tokens=34, output_tokens=271
21:36:01,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:01,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 10.328000000001339. input_tokens=2935, output_tokens=904
21:36:02,205 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:02,206 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:03,0 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:03,1 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:03,178 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:03,178 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:03,913 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:03,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.985000000000582. input_tokens=2936, output_tokens=1081
21:36:04,27 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:04,28 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:04,344 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:04,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.625. input_tokens=2936, output_tokens=878
21:36:06,188 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:06,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 5.71900000000096. input_tokens=34, output_tokens=435
21:36:07,539 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:07,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 6.0. input_tokens=34, output_tokens=645
21:36:07,864 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:07,866 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:08,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:08,405 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:09,299 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:09,299 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:09,586 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:09,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.811999999999898. input_tokens=2936, output_tokens=610
21:36:10,99 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:10,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 2.561999999999898. input_tokens=34, output_tokens=235
21:36:10,610 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:10,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 5.563000000000102. input_tokens=2935, output_tokens=570
21:36:10,703 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:10,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 5 retries took 9.156000000000859. input_tokens=2934, output_tokens=953
21:36:11,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:11,555 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:12,480 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:12,481 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:13,318 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:13,319 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:13,391 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:13,392 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:13,510 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:13,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 2.765000000001237. input_tokens=34, output_tokens=248
21:36:14,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:14,276 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:14,722 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:14,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.015999999999622. input_tokens=34, output_tokens=384
21:36:14,752 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:14,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 2.5310000000008586. input_tokens=34, output_tokens=247
21:36:14,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:14,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 8.90599999999904. input_tokens=2936, output_tokens=842
21:36:16,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:16,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 2.5930000000007567. input_tokens=34, output_tokens=261
21:36:17,441 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:17,442 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:17,914 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:17,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.813000000000102. input_tokens=2936, output_tokens=937
21:36:18,207 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:18,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 11.234000000000378. input_tokens=2935, output_tokens=1244
21:36:18,338 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:18,339 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:19,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:19,557 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:19,923 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:19,924 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:20,559 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:20,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 2.6719999999986612. input_tokens=34, output_tokens=278
21:36:21,247 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:21,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.609999999998763. input_tokens=34, output_tokens=455
21:36:22,865 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:22,865 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:22,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:22,996 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 6.625. input_tokens=2936, output_tokens=666
21:36:23,452 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:23,453 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:23,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:24,198 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.45299999999952. input_tokens=2937, output_tokens=915
21:36:25,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:25,940 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:26,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:26,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 7.610000000000582. input_tokens=2935, output_tokens=742
21:36:27,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:27,211 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:27,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:27,446 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.891000000001441. input_tokens=34, output_tokens=436
21:36:28,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:28,989 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:29,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:29,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.7180000000007567. input_tokens=34, output_tokens=282
21:36:30,223 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:30,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 9.359000000000378. input_tokens=2936, output_tokens=938
21:36:30,544 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:30,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 5 retries took 8.718999999999141. input_tokens=2936, output_tokens=872
21:36:30,892 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:30,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 5.96900000000096. input_tokens=2935, output_tokens=388
21:36:31,422 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:31,423 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:33,203 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:33,203 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:33,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:33,602 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:33,602 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 194112, Requested 7584. Please try again in 508ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:36:33,615 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'next lamp flickered into darkness. Twelve times\nhe clicked the Put-Outer, until the only lights left on the whole street\nwere two tiny pinpricks in the distance, which were the eyes of the cat\nwatching him. If anyone looked out of their window now, even beady-eyed\nMrs. Dursley, they wouldn\'t be able to see anything that was happening\ndown on the pavement. Dumbledore slipped the Put-Outer back inside his\ncloak and set off down the street toward number four, where he sat down\non the wall next to the cat. He didn\'t look at it, but after a moment he\nspoke to it.\n\n"Fancy seeing you here, Professor McGonagall."\n\nHe turned to smile at the tabby, but it had gone. Instead he was smiling\nat a rather severe-looking woman who was wearing square glasses exactly\nthe shape of the markings the cat had had around its eyes. She, too, was\nwearing a cloak, an emerald one. Her black hair was drawn into a tight\nbun. She looked distinctly ruffled.\n\n"How did you know it was me?" she asked.\n\n"My dear Professor, I \'ve never seen a cat sit so stiffly."\n\n"You\'d be stiff if you\'d been sitting on a brick wall all day," said\nProfessor McGonagall.\n\n"All day? When you could have been celebrating? I must have passed a\ndozen feasts and parties on my way here."\n\nProfessor McGonagall sniffed angrily.\n\n"Oh yes, everyone\'s celebrating, all right," she said impatiently.\n"You\'d think they\'d be a bit more careful, but no -- even the Muggles\nhave noticed something\'s going on. It was on their news." She jerked her\nhead back at the Dursleys\' dark living-room window. "I heard it. Flocks\nof owls... shooting stars.... Well, they\'re not completely stupid. They\nwere bound to notice something. Shooting stars down in Kent -- I\'ll bet\nthat was Dedalus Diggle. He never had much sense."\n\n"You can\'t blame them," said Dumbledore gently. "We\'ve had precious\nlittle to celebrate for eleven years."\n\n"I know that," said Professor McGonagall irritably. "But that\'s no\nreason to lose our heads. People are being downright careless, out on\nthe streets in broad daylight, not even dressed in Muggle clothes,\nswapping rumors."\n\nShe threw a sharp, sideways glance at Dumbledore here, as though hoping\nhe was going to tell her something, but he didn\'t, so she went on. "A\nfine thing it would be if, on the very day YouKnow-Who seems to have\ndisappeared at last, the Muggles found out about us all. I suppose he\nreally has gone, Dumbledore?"\n\n"It certainly seems so," said Dumbledore. "We have much to be thankful\nfor. Would you care for a lemon drop?"\n\n"A what?"\n\n"A lemon drop. They\'re a kind of Muggle sweet I\'m rather fond of"\n\n"No, thank you," said Professor McGonagall coldly, as though she didn\'t\nthink this was the moment for lemon drops. "As I say, even if\nYou-Know-Who has gone -"\n\n"My dear Professor, surely a sensible person like yourself can call him\nby his name? All this \'You- Know-Who\' nonsense -- for eleven years I\nhave been trying to persuade people to call him by his proper name:\nVoldemort." Professor McGonagall flinched, but Dumbledore, who was\nunsticking two lemon drops, seemed not to notice. "It all gets so\nconfusing if we keep saying \'You-Know-Who.\' I have never seen any reason\nto be frightened of saying Voldemort\'s name.\n\n"I know you haven \'t, said Professor McGonagall, sounding half\nexasperated, half admiring. "But you\'re different. Everyone knows you\'re\nthe only one You-Know- oh, all right, Voldemort, was frightened of."\n\n"You flatter me," said Dumbledore calmly. "Voldemort had powers I will\nnever have."\n\n"Only because you\'re too -- well -- noble to use them."\n\n"It\'s lucky it\'s dark. I haven\'t blushed so much since Madam Pomfrey\ntold me she liked my new earmuffs."\n\nProfessor McGonagall shot a sharp look at Dumbledore and said, "The owls\nare nothing next to the rumors that are flying around. You know what\neveryone\'s saying? About why he\'s disappeared? About what finally\nstopped him?"\n\nIt seemed that Professor McGonagall had reached the point she was most\nanxious to discuss, the real reason she had been waiting on a cold, hard\nwall all day, for neither as a cat nor as a woman had she fixed\nDumbledore with such a piercing stare as she did now. It was plain that\nwhatever "everyone" was saying, she was not going to believe it until\nDumbledore told her it was true. Dumbledore, however, was choosing\nanother lemon drop and did not answer.\n\n"What they\'re saying," she pressed on, "is that last night Voldemort\nturned up in Godric\'s Hollow. He went to find the Potters. The rumor is\nthat Lily and James Potter are -- are -- that they\'re -- dead. "\n\nDumbledore bowed his head. Professor McGonagall gasped.\n\n"Lily and James... I can\'t believe it... I didn\'t want to believe it...\nOh, Albus..."\n\nDumbledore reached out and patted her on the shoulder. "I know... I\nknow..." he said heavily.\n\nProfessor McGonagall\'s voice trembled as she went'}
21:36:33,806 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:33,807 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:34,198 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:34,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.360000000000582. input_tokens=2936, output_tokens=465
21:36:34,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:34,325 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:34,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:34,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.265999999999622. input_tokens=34, output_tokens=361
21:36:34,799 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:34,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 6.843999999999141. input_tokens=2936, output_tokens=760
21:36:35,871 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:35,873 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:35,927 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:35,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 12.186999999999898. input_tokens=2936, output_tokens=1242
21:36:36,58 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:36,58 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:37,127 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:37,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.234000000000378. input_tokens=34, output_tokens=553
21:36:37,399 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:37,401 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.25. input_tokens=34, output_tokens=218
21:36:37,686 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:37,687 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:37,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:37,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.75. input_tokens=2936, output_tokens=638
21:36:38,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:38,29 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:38,109 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:38,109 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:38,955 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:38,956 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:39,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:39,829 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:40,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:40,155 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:40,433 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:40,434 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.0309999999990396. input_tokens=34, output_tokens=313
21:36:41,786 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:41,786 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:42,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:42,463 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:42,598 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:42,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.515999999999622. input_tokens=2936, output_tokens=983
21:36:43,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:43,51 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:43,808 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:43,810 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:44,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:44,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.264999999999418. input_tokens=2936, output_tokens=734
21:36:45,12 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:45,128 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:45,128 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:45,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 7.093000000000757. input_tokens=2936, output_tokens=777
21:36:46,147 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:46,147 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:46,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:46,341 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:47,484 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:47,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.921999999998661. input_tokens=2936, output_tokens=539
21:36:47,606 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:47,607 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:47,890 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:47,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.515999999999622. input_tokens=2936, output_tokens=1034
21:36:48,314 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:48,315 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:48,920 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:48,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 4.343999999999141. input_tokens=34, output_tokens=341
21:36:49,177 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:49,177 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:49,379 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:49,379 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:50,109 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:50,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.390999999999622. input_tokens=2936, output_tokens=1106
21:36:50,392 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:50,394 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.625. input_tokens=2936, output_tokens=1017
21:36:51,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:51,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.593000000000757. input_tokens=2936, output_tokens=655
21:36:51,618 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:51,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.7189999999991414. input_tokens=34, output_tokens=336
21:36:51,846 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:51,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.92200000000048. input_tokens=34, output_tokens=325
21:36:51,985 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:51,986 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:52,298 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:52,299 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:53,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:53,74 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:53,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:53,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.531000000000859. input_tokens=2936, output_tokens=733
21:36:54,145 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:54,145 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:54,177 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:54,179 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:54,663 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:54,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.04700000000048. input_tokens=34, output_tokens=361
21:36:55,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:55,187 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:55,188 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:55,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.610000000000582. input_tokens=34, output_tokens=367
21:36:56,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:56,136 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:56,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:56,446 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:57,94 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:57,95 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:58,408 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:58,409 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 4.688000000000102. input_tokens=34, output_tokens=409
21:36:58,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:58,530 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:58,817 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:58,818 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:59,139 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:36:59,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 8.438000000000102. input_tokens=2936, output_tokens=898
21:36:59,269 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:59,270 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:36:59,873 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:36:59,874 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:00,706 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:00,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 7 retries took 8.639999999999418. input_tokens=2936, output_tokens=925
21:37:00,745 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:00,746 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:02,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:02,77 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:02,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:02,406 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:02,451 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:02,451 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:02,904 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:02,908 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:03,32 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:03,33 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:03,313 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:03,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 4.453000000001339. input_tokens=2936, output_tokens=480
21:37:03,433 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:03,434 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:03,865 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:03,865 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:04,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:04,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.656000000000859. input_tokens=2936, output_tokens=685
21:37:04,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:04,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.890999999999622. input_tokens=2936, output_tokens=1001
21:37:05,21 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:05,23 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:05,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:05,522 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:06,451 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:06,451 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:06,697 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:06,697 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:07,408 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:07,409 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:07,441 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:07,441 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:08,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:08,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 3.2189999999991414. input_tokens=34, output_tokens=285
21:37:08,62 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:08,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 12.21900000000096. input_tokens=34, output_tokens=1182
21:37:08,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:08,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.156999999999243. input_tokens=2936, output_tokens=662
21:37:09,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:09,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 9.07799999999952. input_tokens=2936, output_tokens=681
21:37:09,171 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:09,173 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:10,129 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:10,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 3.140000000001237. input_tokens=34, output_tokens=273
21:37:10,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:10,961 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:11,702 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:11,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 8.546000000000276. input_tokens=2936, output_tokens=713
21:37:11,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:11,820 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:12,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:12,428 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:12,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:12,555 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:12,980 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:12,981 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:13,180 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:13,180 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:13,194 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:13,194 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:14,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:14,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 10.186999999999898. input_tokens=2936, output_tokens=821
21:37:14,375 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:14,375 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:15,474 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:15,474 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:15,648 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:15,649 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:15,843 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:15,963 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:15,963 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:15,997 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 5.42200000000048. input_tokens=34, output_tokens=469
21:37:15,998 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:15,999 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:16,186 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:16,186 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:16,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:16,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.640999999999622. input_tokens=2936, output_tokens=588
21:37:16,601 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:16,602 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:16,862 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:16,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.063000000000102. input_tokens=2935, output_tokens=738
21:37:17,56 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:17,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.0. input_tokens=34, output_tokens=794
21:37:17,448 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:17,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.531000000000859. input_tokens=2936, output_tokens=561
21:37:17,559 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:17,559 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:17,851 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:17,851 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:18,249 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:18,251 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:18,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:18,436 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:18,719 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:18,720 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:19,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:19,613 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:19,735 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:19,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.4069999999992433. input_tokens=34, output_tokens=240
21:37:19,855 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:19,855 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:20,660 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:20,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.79700000000048. input_tokens=34, output_tokens=387
21:37:20,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:20,808 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:21,261 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:21,262 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:21,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:21,343 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:21,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:21,488 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:21,610 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:21,611 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:21,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:21,729 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:21,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 5.984000000000378. input_tokens=2936, output_tokens=696
21:37:22,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 7.79700000000048. input_tokens=2936, output_tokens=828
21:37:22,544 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:22,548 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:22,658 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:22,658 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:22,799 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:22,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 7.95299999999952. input_tokens=2936, output_tokens=899
21:37:23,201 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:23,202 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:23,209 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:23,210 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:23,318 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:23,319 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:23,845 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:23,845 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:24,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:24,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.46900000000096. input_tokens=2937, output_tokens=638
21:37:24,385 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:24,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.640000000001237. input_tokens=34, output_tokens=398
21:37:25,595 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:25,595 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:25,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:25,762 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:25,873 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:25,873 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:25,967 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:25,967 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:26,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:26,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 4.7960000000002765. input_tokens=34, output_tokens=357
21:37:27,68 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:27,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.266000000001441. input_tokens=34, output_tokens=446
21:37:27,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:27,961 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:28,304 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:28,304 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:29,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:29,395 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:29,396 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:29,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:29,522 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:29,602 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:29,603 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:29,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.765999999999622. input_tokens=2936, output_tokens=785
21:37:30,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:30,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.9530000000013388. input_tokens=1815, output_tokens=172
21:37:30,881 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:30,883 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:31,590 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:31,591 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:31,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:31,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.78099999999904. input_tokens=34, output_tokens=515
21:37:32,52 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:32,53 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:32,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:32,216 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.20299999999952. input_tokens=2042, output_tokens=725
21:37:32,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:32,331 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:32,652 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:32,654 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 8.45299999999952. input_tokens=2935, output_tokens=742
21:37:32,769 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:32,770 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:32,782 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:32,782 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:33,350 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:33,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.593999999999141. input_tokens=2937, output_tokens=690
21:37:33,441 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:33,442 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:33,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:33,651 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:33,907 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:33,908 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:34,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:34,117 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:34,566 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:34,566 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:34,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:34,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.125. input_tokens=34, output_tokens=243
21:37:34,888 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:34,890 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5310000000008586. input_tokens=1798, output_tokens=157
21:37:35,3 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:35,4 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:35,684 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:35,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.82799999999952. input_tokens=34, output_tokens=223
21:37:35,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:35,757 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:35,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:35,857 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:36,162 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:36,162 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:36,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:36,340 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:36,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:36,341 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:36,624 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:36,625 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:36,995 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:36,996 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.3130000000001019. input_tokens=1799, output_tokens=118
21:37:37,251 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:37,251 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:37,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:37,429 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 4.858999999998559. input_tokens=34, output_tokens=335
21:37:37,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:37,543 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:38,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:38,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.45299999999952. input_tokens=34, output_tokens=339
21:37:38,406 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:38,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 7.92200000000048. input_tokens=2935, output_tokens=874
21:37:38,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:38,532 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:38,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:38,578 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:39,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:39,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:39,216 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:39,216 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 4.093999999999141. input_tokens=2936, output_tokens=404
21:37:39,518 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:39,519 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:39,734 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:39,734 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:40,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:40,105 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:40,222 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:40,223 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:40,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:40,378 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.390000000001237. input_tokens=34, output_tokens=356
21:37:40,661 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:40,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 2.25. input_tokens=34, output_tokens=209
21:37:40,781 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:40,782 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:40,990 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:40,991 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:41,466 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:41,467 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:41,821 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:41,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.7810000000008586. input_tokens=1804, output_tokens=319
21:37:42,172 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:42,174 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:42,453 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:42,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.139999999999418. input_tokens=34, output_tokens=234
21:37:42,620 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:42,621 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:42,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:42,906 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:42,906 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 198452, Requested 7730. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:37:42,908 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'almost flew, back down the corridor. Filch must have hurried off to look\nfor them somewhere else, because they didn\'t see him anywhere, but they\nhardly cared -- all they wanted to do was put as much space as possible\nbetween them and that monster. They didn\'t stop running until they\nreached the portrait of the Fat Lady on the seventh floor.\n\n"Where on earth have you all been?" she asked, looking at their\nbathrobes hanging off their shoulders and their flushed, sweaty faces.\n\n"Never mind that -- pig snout, pig snout," panted Harry, and the\nportrait swung forward. They scrambled into the common room and\ncollapsed, trembling, into armchairs.\n\nIt was a while before any of them said anything. Neville, indeed, looked\nas if he\'d never speak again.\n\n"What do they think they\'re doing, keeping a thing like that locked up\nin a school?" said Ron finally. "If any dog needs exercise, that one\ndoes."\n\nHermione had got both her breath and her bad temper back again. "You\ndon\'t use your eyes, any of you, do you?" she snapped. "Didn\'t you see\nwhat it was standing on.\n\n"The floor?" Harry suggested. "I wasn\'t looking at its feet, I was too\nbusy with its heads."\n\n"No, not the floor. It was standing on a trapdoor. It\'s obviously\nguarding something."\n\nShe stood up, glaring at them.\n\nI hope you\'re pleased with yourselves. We could all have been killed --\nor worse, expelled. Now, if you don\'t mind, I\'m going to bed."\n\nRon stared after her, his mouth open.\n\n"No, we don\'t mind," he said. "You\'d think we dragged her along,\nwouldn\'t you.\n\nBut Hermione had given Harry something else to think about as he climbed\nback into bed. The dog was guarding something.... What had Hagrid said?\nGringotts was the safest place in the world for something you wanted to\nhide -- except perhaps Hogwarts.\n\nIt looked as though Harry had found out where the grubby littie package\nfrom vault seven hundred and thirteen was.\n\n\nCHAPTER TEN\n\nHALLOWEEN\n\nMalfoy couldn\'t believe his eyes when he saw that Harry and Ron were\nstill at Hogwarts the next day, looking tired but perfectly cheerful.\nIndeed, by the next morning Harry and Ron thought that meeting the\nthree-headed dog had been an excellent adventure, and they were quite\nkeen to have another one. In the meantime, Harry filled Ron in about the\npackage that seemed to have been moved from Gringotts to Hogwarts, and\nthey spent a lot of time wondering what could possibly need such heavy\nprotection. "It\'s either really valuable or really dangerous," said Ron.\n"Or both," said Harry.\n\n\nBut as all they knew for sure about the mysterious object was that it\nwas about two inches long, they didn\'t have much chance of guessing what\nit was without further clues.\n\nNeither Neville nor Hermione showed the slightest interest in what lay\nunderneath the dog and the trapdoor. All Neville cared about was never\ngoing near the dog again.\n\nHermione was now refusing to speak to Harry and Ron, but she was such a\nbossy know-it-all that they saw this as an added bonus. All they really\nwanted now was a way of getting back at Malfoy, and to their great\ndelight, just such a thing arrived in the mail about a week later.\n\nAs the owls flooded into the Great Hall as usual, everyone\'s attention\nwas caught at once by a long, thin package carried by six large screech\nowls. Harry was just as interested as everyone else to see what was in\nthis large parcel, and was amazed when the owls soared down and dropped\nit right in front of him, knocking his bacon to the floor. They had\nhardly fluttered out of the way when another owl dropped a letter on top\nof the parcel.\n\nHarry ripped open the letter first, which was lucky, because it said:\n\n\nDO NOT OPEN THE PARCEL AT THE TABLE.\n\nIt contains your new Nimbus Two Thousand, but I don\'t want everybody\nknowing you\'ve got a broomstick or they\'ll all want one. Oliver Wood\nwill meet you tonight on the Quidditch field at seven o\'clock for your\nfirst training session.\n\nProfessor McGonagall\n\nHarry had difficulty hiding his glee as he handed the note to Ron to\nread.\n\n"A Nimbus Two Thousand!" Ron moaned enviously. "I\'ve never even touched\none."\n\nThey left the hall quickly, wanting to unwrap the broomstick in private\nbefore their first class, but halfway across the entrance hall they\nfound the way upstairs barred by Crabbe and Goyle. Malfoy seized the\npackage from Harry and felt it.\n\n"That\'s a broomstick," he said, throwing it back to Harry with a mixture\nof jealousy and spite on his face. "You\'ll be in for it this time,\nPotter, first years aren\'t allowed them."\n\nRon couldn\'t resist it.\n\n"It\'s not any old broomstick," he said, "it\'s a Nimbus Two Thousand.\nWhat did you say you\'ve got at home, Malfoy, a Comet Two Sixty?" Ron\ngrinned at Harry. "Comets look flashy, but they\'re not in the same\nleague as the Nimbus."\n\n"What would you know about it, Weasley, you couldn\'t afford half the\nhandle," Malfoy snapped back. "I suppose you and your brothers have to\nsave up twig by twig."\n\nBefore Ron could answer, Professor Flitwick appeared at Malfoy\'s elbow.\n\n"Not arguing, I hope, boys?" he squeaked.\n\n"Pot'}
21:37:42,965 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:42,965 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:43,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:43,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0940000000009604. input_tokens=1800, output_tokens=216
21:37:43,34 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:43,35 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:43,142 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:43,143 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:43,145 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:43,146 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:43,580 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:43,580 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:43,833 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:43,834 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:44,691 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:44,692 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:44,695 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:44,695 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:44,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:44,789 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:44,952 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:44,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 5.889999999999418. input_tokens=2936, output_tokens=558
21:37:45,77 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:45,78 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:45,86 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:45,87 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7180000000007567. input_tokens=1814, output_tokens=259
21:37:45,204 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:45,205 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:45,538 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:45,539 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.54700000000048. input_tokens=1807, output_tokens=224
21:37:45,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:45,651 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:45,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:45,886 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:46,5 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:46,5 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:46,380 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:46,380 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:46,765 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:46,765 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:46,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:46,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.92200000000048. input_tokens=34, output_tokens=515
21:37:47,16 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:47,17 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:47,384 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:47,384 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:47,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:47,848 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:48,228 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:48,228 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:48,776 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:48,777 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:48,777 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 150, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 194347, Requested 6862. Please try again in 362ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:37:48,779 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '\'t\ngoing at all the way he wanted.\n\n"Like this," he said irritably. He seized his left ear and pulled. His\nwhole head swung off his neck and fell onto his shoulder as if it was on\na hinge. Someone had obviously tried to behead him, but not done it\nproperly. Looking pleased at the stunned looks on their faces, Nearly\nHeadless Nick flipped his head back onto his neck, coughed, and said,\n"So -- new Gryffindors! I hope you\'re going to help us win the house\nchampionship this year? Gryffindors have never gone so long without\nwinning. Slytherins have got the cup six years in a row! The Bloody\nBaron\'s becoming almost unbearable -- he\'s the Slytherin ghost."\n\nHarry looked over at the Slytherin table and saw a horrible ghost\nsitting there, with blank staring eyes, a gaunt face, and robes stained\nwith silver blood. He was right next to Malfoy who, Harry was pleased to\nsee, didn\'t look too pleased with the seating arrangements.\n\n"How did he get covered in blood?" asked Seamus with great interest.\n\n"I\'ve never asked," said Nearly Headless Nick delicately.\n\nWhen everyone had eaten as much as they could, the remains of the food\nfaded from the plates, leaving them sparkling clean as before. A moment\nlater the desserts appeared. Blocks of ice cream in every flavor you\ncould think of, apple pies, treacle tarts, chocolate eclairs and jam\ndoughnuts, trifle, strawberries, Jell-O, rice pudding -- "\n\nAs Harry helped himself to a treacle tart, the talk turned to their\nfamilies.\n\n"I\'m half-and-half," said Seamus. "Me dad\'s a Muggle. Mom didn\'t tell\nhim she was a witch \'til after they were married. Bit of a nasty shock\nfor him."\n\nThe others laughed.\n\n"What about you, Neville?" said Ron.\n\n"Well, my gran brought me up and she\'s a witch," said Neville, "but the\nfamily thought I was all- Muggle for ages. My Great Uncle Algie kept\ntrying to catch me off my guard and force some magic out of me -- he\npushed me off the end of Blackpool pier once, I nearly drowned -- but\nnothing happened until I was eight. Great Uncle Algie came round for\ndinner, and he was hanging me out of an upstairs window by the ankles\nwhen my Great Auntie Enid offered him a meringue and he accidentally let\ngo. But I bounced -- all the way down the garden and into the road. They\nwere all really pleased, Gran was crying, she was so happy. And you\nshould have seen their faces when I got in here -- they thought I might\nnot be magic enough to come, you see. Great Uncle Algie was so pleased\nhe bought me my toad."\n\nOn Harry\'s other side, Percy Weasley and Hermione were talking about\nlessons ("I do hope they start right away, there\'s so much to learn, I\'m\nparticularly interested in Transfiguration, you know, turning something\ninto something else, of course, it\'s supposed to be very difficult-";\n"You\'ll be starting small, just matches into needles and that sort of\nthing -- ").\n\nHarry, who was starting to feel warm and sleepy, looked up at\n\nthe High Table again. Hagrid was drinking deeply from his goblet.\nProfessor McGonagall was talking to Professor Dumbledore. Professor\nQuirrell, in his absurd turban, was talking to a teacher with greasy\nblack hair, a hooked nose, and sallow skin.\n\nIt happened very suddenly. The hook-nosed teacher looked past Quirrell\'s\nturban straight into Harry\'s eyes -- and a sharp, hot pain shot across\nthe scar on Harry\'s forehead.\n\n"Ouch!" Harry clapped a hand to his head.\n\n"What is it?" asked Percy.\n\n"N-nothing."\n\nThe pain had gone as quickly as it had come. Harder to shake off was the\nfeeling Harry had gotten from the teacher\'s look -- a feeling that he\ndidn\'t like Harry at all.\n\n"Who\'s that teacher talking to Professor Quirrell?" he asked Percy.\n\n"Oh, you know Quirrell already, do you? No wonder he\'s looking so\nnervous, that\'s Professor Snape. He teaches Potions, but he doesn\'t want\nto -- everyone knows he\'s after Quirrell\'s job. Knows an awful lot about\nthe Dark Arts, Snape."\n\nHarry watched Snape for a while, but Snape didn\'t look at him again.\n\nAt last, the desserts too disappeared, and Professor Dumbledore got to\nhis feet again. The hall fell silent.\n\n"Ahern -- just a few more words now that we are all fed and watered. I\nhave a few start-of-term notices to give you.\n\n"First years should note that the forest on the grounds is forbidden to\nall pupils. And a few of our older students would do well to remember\nthat as well."\n\nDumbledore\'s twinkling eyes flashed in the direction of the Weasley\ntwins.\n\n"I have also been asked by Mr. Filch, the caretaker, to remind you all\nthat no magic should be used between classes in the corridors.\n\n"Quidditch trials will be held in the second week of the term. Anyone\ninterested in playing for their house teams should contact Madam Hooch.\n\n"And finally, I must tell you that this year, the third-floor corridor\non the right-hand side is out of bounds to everyone who does not wish to\ndie a very painful death."\n\nHarry laughed, but he was one of the few who did.\n\n"He'}
21:37:48,915 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:48,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5319999999992433. input_tokens=1800, output_tokens=144
21:37:49,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:49,124 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:49,239 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:49,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.7189999999991414. input_tokens=34, output_tokens=389
21:37:49,433 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:49,433 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:49,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:49,487 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:49,586 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:49,588 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:49,866 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:49,868 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:50,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:50,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 5.29700000000048. input_tokens=34, output_tokens=571
21:37:50,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:50,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8279999999995198. input_tokens=34, output_tokens=177
21:37:51,18 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:51,19 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:51,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:51,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.0. input_tokens=1809, output_tokens=82
21:37:51,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:51,264 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:51,382 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:51,382 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:52,37 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:52,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 0.6409999999996217. input_tokens=1798, output_tokens=50
21:37:52,70 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:52,70 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:52,162 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:52,172 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:52,282 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:52,283 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:52,893 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:52,894 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:52,901 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:52,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.6560000000008586. input_tokens=34, output_tokens=185
21:37:53,566 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:53,569 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:53,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:53,631 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:53,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:53,741 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:53,742 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 197268, Requested 7608. Please try again in 1.462s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:37:53,745 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'That\'s another\npoint you\'ve lost for Gryffindor."\n\nThis was so unfair that Harry opened his mouth to argue, but Ron kicked\nhim behind their cauldron.\n\n"Doi* push it," he muttered, "I\'ve heard Snape can turn very nasty."\n\nAs they climbed the steps out of the dungeon an hour later, Harry\'s mind\nwas racing and his spirits were low. He\'d lost two points for Gryffindor\nin his very first week -- why did Snape hate him so much? "Cheer up,"\nsaid Ron, "Snape\'s always taking points off Fred and George. Can I come\nand meet Hagrid with you?"\n\nAt five to three they left the castle and made their way across the\ngrounds. Hagrid lived in a small wooden house on the edge of the\nforbidden forest. A crossbow and a pair of galoshes were outside the\nfront door.\n\nWhen Harry knocked they heard a frantic scrabbling from inside and\nseveral booming barks. Then Hagrid\'s voice rang out, saying, "Back, Fang\n-- back."\n\nHagrid\'s big, hairy face appeared in the crack as he pulled the door\nopen.\n\n"Hang on," he said. "Back, Fang."\n\nHe let them in, struggling to keep a hold on the collar of an enormous\nblack boarhound.\n\nThere was only one room inside. Hams and pheasants were hanging from the\nceiling, a copper kettle was boiling on the open fire, and in the corner\nstood a massive bed with a patchwork quilt over it.\n\n"Make yerselves at home," said Hagrid, letting go of Fang, who bounded\nstraight at Ron and started licking his ears. Like Hagrid, Fang was\nclearly not as fierce as he looked.\n\n"This is Ron," Harry told Hagrid, who was pouring boiling water into a\nlarge teapot and putting rock cakes onto a plate.\n\n"Another Weasley, eh?" said Hagrid, glancing at Ron\'s freckles. I spent\nhalf me life chasin\' yer twin brothers away from the forest."\n\nThe rock cakes were shapeless lumps with raisins that almost broke their\nteeth, but Harry and Ron pretended to be enjoying them as they told\nHagrid all about their first -lessons. Fang rested his head on Harry\'s\nknee and drooled all over his robes.\n\nHarry and Ron were delighted to hear Hagrid call Fitch "that old git."\n\n"An\' as fer that cat, Mrs. Norris, I\'d like ter introduce her to Fang\nsometime. D\'yeh know, every time I go up ter the school, she follows me\neverywhere? Can\'t get rid of her -- Fitch puts her up to it."\n\nHarry told Hagrid about Snape\'s lesson. Hagrid, like Ron, told Harry not\nto worry about it, that Snape liked hardly any of the students.\n\n"But he seemed to really hate me."\n\n"Rubbish!" said Hagrid. "Why should he?"\n\nYet Harry couldn\'t help thinking that Hagrid didn\'t quite meet his eyes\nwhen he said that.\n\n"How\'s yer brother Charlie?" Hagrid asked Ron. "I liked him a lot --\ngreat with animals."\n\nHarry wondered if Hagrid had changed the subject on purpose. While Ron\ntold Hagrid all about Charlie\'s work with dragons, Harry picked up a\npiece of paper that was lying on the table under the tea cozy. It was a\ncutting from the Daily Prophet:\n\nGRINGOTTS BREAK-IN LATEST\n\nInvestigations continue into the break-in at Gringotts on 31 July,\nwidely believed to be the work of Dark wizards or witches unknown.\n\nGringotts goblins today insisted that nothing had been taken. The vault\nthat was searched had in fact been emptied the same day.\n\n"But we\'re not telling you what was in there, so keep your noses out if\nyou know what\'s good for you," said a Gringotts spokesgoblin this\nafternoon.\n\nHarry remembered Ron telling him on the train that someone had tried to\nrob Gringotts, but Ron hadn\'t mentioned the date.\n\n"Hagrid!" said Harry, "that Gringotts break-in happened on my birthday!\nIt might\'ve been happening while we were there!"\n\nThere was no doubt about it, Hagrid definitely didn\'t meet Harry\'s eyes\nthis time. He grunted and offered him another rock cake. Harry read the\nstory again. The vault that was searched had in fact been emptied\nearlier that same day. Hagrid had emptied vault seven hundred and\nthirteen, if you could call it emptying, taking out that grubby little\npackage. Had that been what the thieves were looking for?\n\nAs Harry and Ron walked back to the castle for dinner, their pockets\nweighed down with rock cakes they\'d been too polite to refuse, Harry\nthought that none of the lessons he\'d had so far had given him as much\nto think about as tea with Hagrid. Had Hagrid collected that package\njust in time? Where was it now? And did Hagrid know something about\nSnape that he didn\'t want to tell Harry?\n\n\nCHAPTER NINE\n\nTHE MIDNIGHT DUEL\n\nHarry had never believed he would meet a boy he hated more than Dudley,\nbut that was before he met Draco Malfoy.\tStill, first-year\nGryffindors only had Potions with the Slytherins, so they didn\'t have to\nput up with Malfoy much. Or at least, they didn\'t until they spotted a\nnotice pinned up in the Gryffindor common room that made them all groan.\nFlying lessons would be starting on Thursday -- and Gryffindor and\nSlytherin\twould be'}
21:37:53,764 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:53,765 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:53,808 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:53,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.061999999999898. input_tokens=34, output_tokens=308
21:37:54,53 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:54,54 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:54,468 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:54,469 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:54,818 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:54,819 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:55,148 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:55,149 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:55,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:55,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.4060000000008586. input_tokens=34, output_tokens=258
21:37:55,609 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:55,611 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:56,1 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:56,10 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:56,118 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:56,120 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:56,239 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:56,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.484999999998763. input_tokens=34, output_tokens=185
21:37:56,373 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:56,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0. input_tokens=34, output_tokens=83
21:37:56,626 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:56,628 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:56,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:56,835 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:56,912 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:56,913 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:56,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:56,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 10.51600000000144. input_tokens=2936, output_tokens=858
21:37:57,69 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:57,71 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:57,94 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:57,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 5.093999999999141. input_tokens=1862, output_tokens=499
21:37:58,6 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:58,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 1.8430000000007567. input_tokens=1802, output_tokens=146
21:37:58,846 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:58,848 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:59,94 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:59,95 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:59,716 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:59,717 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:59,770 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:37:59,772 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:37:59,983 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:37:59,984 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.6719999999986612. input_tokens=34, output_tokens=403
21:38:00,184 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:00,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2809999999990396. input_tokens=1809, output_tokens=231
21:38:00,302 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:00,302 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:00,304 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:00,304 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:00,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:00,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.686999999999898. input_tokens=34, output_tokens=294
21:38:01,33 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:01,34 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:01,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:01,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 2.57799999999952. input_tokens=1800, output_tokens=160
21:38:01,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:01,500 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:01,695 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:01,696 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:01,700 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:01,700 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:02,256 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:02,256 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:02,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:02,417 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:02,756 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:02,756 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:03,124 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:03,125 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:03,415 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:03,417 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:03,875 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:03,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 2.890000000001237. input_tokens=34, output_tokens=256
21:38:03,993 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:03,994 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:04,62 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:04,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 4.188000000000102. input_tokens=34, output_tokens=375
21:38:04,456 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:04,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.7180000000007567. input_tokens=1804, output_tokens=145
21:38:04,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:04,495 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:04,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:04,578 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:04,935 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:04,936 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:05,217 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:05,217 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:05,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:05,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 1.4690000000009604. input_tokens=1808, output_tokens=135
21:38:05,737 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:05,738 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:05,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:05,794 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:05,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:05,807 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:06,25 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:06,25 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:06,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:06,141 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:06,142 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 197223, Requested 7699. Please try again in 1.476s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:38:06,143 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'of their\nfirst class he took the roll call, and when he reached Harry\'s name he\ngave an excited squeak and toppled out of sight.\n\nProfessor McGonagall was again different. Harry had been quite right to\nthink she wasn\'t a teacher to cross. Strict and clever, she gave them a\ntalking-to the moment they sat down in her first class.\n\n"Transfiguration is some of the most complex and dangerous magic you\nwill learn at Hogwarts," she said. "Anyone messing around in my class\nwill leave and not come back. You have been warned."\n\nThen she changed her desk into a pig and back again. They were all very\nimpressed and couldn\'t wait to get started, but soon realized they\nweren\'t going to be changing the furniture into animals for a long time.\nAfter taking a lot of complicated notes, they were each given a match\nand started trying to turn it into a needle. By the end of the lesson,\nonly Hermione Granger had made any difference to her match; Professor\nMcGonagall showed the class how it had gone all silver and pointy and\ngave Hermione a rare smile.\n\nThe class everyone had really been looking forward to was Defense\nAgainst the Dark Arts, but Quirrell\'s lessons turned out to be a bit of\na joke. His classroom smelled strongly of garlic, which everyone said\nwas to ward off a vampire he\'d met in Romania and was afraid would be\ncoming back to get him one of these days. His turban, he told them, had\nbeen given to him by an African prince as a thank-you for getting rid of\na troublesome zombie, but they weren\'t sure they believed this story.\nFor one thing, when Seamus Finnigan asked eagerly to hear how Quirrell\nhad fought off the zombie, Quirrell went pink and started talking about\nthe weather; for another, they had noticed that a funny smell hung\naround the turban, and the Weasley twins insisted that it was stuffed\nfull of garlic as well, so that Quirrell was protected wherever he went.\n\nHarry was very relieved to find out that he wasn\'t miles behind everyone\nelse. Lots of people had come from Muggle families and, like him, hadn\'t\nhad any idea that they were witches and wizards. There was so much to\nlearn that even people like Ron didn\'t have much of a head start.\n\nFriday was an important day for Harry and Ron. They finally managed to\nfind their way down to the Great Hall for breakfast without getting lost\nonce.\n\n"What have we got today?" Harry asked Ron as he poured sugar on his\nporridge.\n\n"Double Potions with the Slytherins," said Ron. "Snape\'s Head of\nSlytherin House. They say he always favors them -- we\'ll be able to see\nif it\'s true."\n\n"Wish McGonagall favored us, " said Harry. Professor McGonagall was head\nof Gryffindor House, but it hadn\'t stopped her from giving them a huge\npile of homework the day before.\n\nJust then, the mail arrived. Harry had gotten used to this by now, but\nit had given him a bit of a shock on the first morning, when about a\nhundred owls had suddenly streamed into the Great Hall during breakfast,\ncircling the tables until they saw their owners, and dropping letters\nand packages onto their laps.\n\nHedwig hadn\'t brought Harry anything so far. She sometimes flew in to\nnibble his ear and have a bit of toast before going off to sleep in the\nowlery with the other school owls. This morning, however, she fluttered\ndown between the marmalade and the sugar bowl and dropped a note onto\nHarry\'s plate. Harry tore it open at once. It said, in a very untidy\nscrawl:\n\n\nDear Harry,\n\nI know you get Friday afternoons off, so would you like to come and have\na cup of tea with me around three?\n\nI want to hear all about your first week. Send us an answer back with\nHedwig.\n\nHagrid\n\n\nHarry borrowed Ron\'s quill, scribbled Yes, please, see you later on the\nback of the note, and sent Hedwig off again.\n\nIt was lucky that Harry had tea with Hagrid to look forward to, because\nthe Potions lesson turned out to be the worst thing that had happened to\nhim so far.\n\nAt the start-of-term banquet, Harry had gotten the idea that Professor\nSnape disliked him. By the end of the first Potions lesson, he knew he\'d\nbeen wrong. Snape didn\'t dislike Harry -- he hated him.\n\nPotions lessons took place down in one of the dungeons. It was colder\nhere than up in the main castle, and would have been quite creepy enough\nwithout the pickled animals floating in glass jars all around the walls.\n\nSnape, like Flitwick, started the class by taking the roll call, and\nlike Flitwick, he paused at Harry\'s name.\n\n"Ah, Yes," he said softly, "Harry Potter. Our new -- celebrity."\n\nDraco Malfoy and his friends Crabbe and Goyle sniggered behind their\nhands. Snape finished calling the names and looked up at the class. His\neyes were black like Hagrid\'s, but they had none of Hagrid\'s warmth.\nThey were cold and empty and made you think of dark tunnels.\n\n"You are here to learn the subtle science and exact art of\npotionmaking," he began. He spoke in barely more than a whisper, but\nthey caught every word -- like Professor McGonagall, Snape had y caught\nevery word -- like Professor McGonagall, Snape had'}
21:38:06,240 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:06,241 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:06,241 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 196895, Requested 7313. Please try again in 1.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:38:06,241 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'way along the rows of books. The lamp looked as if it was floating along\nin midair, and even though Harry could feel his arm supporting it, the\nsight gave him the creeps.\n\nThe Restricted Section was right at the back of the library. Step ping\ncarefully over the rope that separated these books from the rest of the\nlibrary, he held up his lamp to read the titles.\n\nThey didn\'t tell him much. Their peeling, faded gold letters spelled\nwords in languages Harry couldn\'t understand. Some had no title at all.\nOne book had a dark stain on it that looked horribly like blood. The\nhairs on the back of Harry\'s neck prickled. Maybe he was imagining it,\nmaybe not, but he thought a faint whispering was coming from the books,\nas though they knew someone was there who shouldn\'t be.\n\nHe had to start somewhere. Setting the lamp down carefully on the floor,\nhe looked along the bottom shelf for an interestinglooking book. A large\nblack and silver volume caught his eye. He pulled it out with\ndifficulty, because it was very heavy, and, balancing it on his knee,\nlet it fall open.\n\nA piercing, bloodcurdling shriek split the silence -- the book was\nscreaming! Harry snapped it shut, but the shriek went on and on, one\nhigh, unbroken, earsplitting note. He stumbled backward and knocked over\nhis lamp, which went out at once. Panicking, he heard footsteps coming\ndown the corridor outside -- stuffing the shrieking book back on the\nshelf, he ran for it. He passed Filch in the doorway; Filch\'s pale, wild\neyes looked straight through him, and Harry slipped under Filch\'s\noutstretched arm and streaked off up the corridor, the book\'s shrieks\nstill ringing in his ears.\n\nHe came to a sudden halt in front of a tall suit of armor. He had been\nso busy getting away from the library, he hadn\'t paid attention to where\nhe was going. Perhaps because it was dark, he didn\'t recognize where he\nwas at all. There was a suit of armor near the kitchens, he knew, but he\nmust be five floors above there.\n\n"You asked me to come directly to you, Professor, if anyone was\nwandering around at night, and somebody\'s been in the library Restricted\nSection."\n\nHarry felt the blood drain out of his face. Wherever he was, Filch must\nknow a shortcut, because his soft, greasy voice was getting nearer, and\nto his horror, it was Snape who replied, "The Restricted Section? Well,\nthey can\'t be far, we\'ll catch them."\n\nHarry stood rooted to the spot as Filch and Snape came around the corner\nahead. They couldn\'t see him, of course, but it was a narrow corridor\nand if they came much nearer they\'d knock right into him -- the cloak\ndidn\'t stop him from being solid.\n\nHe backed away as quietly as he could. A door stood ajar to his left. It\nwas his only hope. He squeezed through it, holding his breath, trying\nnot to move it, and to his relief he managed to get inside the room\nwithout their noticing anything. They walked straight past, and Harry\nleaned against the wall, breathing deeply, listening to their footsteps\ndying away. That had been close, very close. It was a few seconds before\nhe noticed anything about the room he had hidden in.\n\nIt looked like an unused classroom. The dark shapes of desks and chairs\nwere piled against the walls, and there was an upturned wastepaper\nbasket -- but propped against the wall facing him was something that\ndidn\'t look as if it belonged there, something that looked as if someone\nhad just put it there to keep it out of the way.\n\nIt was a magnificent mirror, as high as the ceiling, with an ornate gold\nframe, standing on two clawed feet. There was an inscription carved\naround the top: Erised stra ehru oyt ube cafru oyt on wohsi. His panic\nfading now that there was no sound of Filch and Snape, Harry moved\nnearer to the mirror, wanting to look at himself but see no reflection\nagain. He stepped in front of it.\n\nHe had to clap his hands to his mouth to stop himself from screaming. He\nwhirled around. His heart was pounding far more furiously than when the\nbook had screamed -- for he had seen not only himself in the mirror, but\na whole crowd of people standing right behind him.\n\nBut the room was empty. Breathing very fast, he turned slowly back to\nthe mirror.\n\nThere he was, reflected in it, white and scared-looking, and there,\nreflected behind him, were at least ten others. Harry looked over his\nshoulder -- but still, no one was there. Or were they all invisible,\ntoo? Was he in fact in a room full of invisible people and this mirror\'s\ntrick was that it reflected them, invisible or not?\n\nHe looked in the mirror again. A woman standing right behind his\nreflection was smiling at him and waving. He reached out a hand and felt\nthe air behind him. If she was really there, he\'d touch her, their\nreflections were so close together, but he felt only air -- she and the\nothers existed only in the mirror.\n\nShe was a very pretty woman. She had dark red hair and her eyes -- her\neyes are just like mine, Harry thought, edging a little closer to the\nglass. Bright green -- exactly the same shape, but then he noticed that\nshe was crying; smiling, but crying at the same'}
21:38:06,262 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:06,262 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:06,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:06,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 1.875. input_tokens=1798, output_tokens=163
21:38:06,695 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:06,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4220000000004802. input_tokens=34, output_tokens=106
21:38:06,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:06,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.92200000000048. input_tokens=1805, output_tokens=480
21:38:07,45 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:07,46 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:07,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:07,369 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:08,297 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:08,299 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:08,464 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:08,465 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:08,731 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:08,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.65599999999904. input_tokens=34, output_tokens=349
21:38:08,843 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:08,844 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:09,58 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:09,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.639999999999418. input_tokens=34, output_tokens=273
21:38:09,87 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:09,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 1.264999999999418. input_tokens=1809, output_tokens=96
21:38:09,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:09,200 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:09,212 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:09,212 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:09,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:09,597 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:09,845 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:09,846 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:10,82 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:10,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0310000000008586. input_tokens=34, output_tokens=67
21:38:10,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:10,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 4.031999999999243. input_tokens=34, output_tokens=405
21:38:11,222 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:11,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 2.6090000000003783. input_tokens=34, output_tokens=223
21:38:11,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:11,310 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:11,624 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:11,625 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:11,902 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:11,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 0.9380000000001019. input_tokens=34, output_tokens=70
21:38:12,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:12,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.4069999999992433. input_tokens=1812, output_tokens=220
21:38:12,557 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:12,558 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:12,869 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:12,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 2.375. input_tokens=1806, output_tokens=236
21:38:12,873 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:12,874 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:13,256 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:13,256 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:13,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:13,332 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:14,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:14,106 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:14,407 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:14,409 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.375. input_tokens=1806, output_tokens=119
21:38:14,547 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:14,547 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:14,810 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:14,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.04700000000048. input_tokens=1805, output_tokens=285
21:38:15,21 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:15,22 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:15,147 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:15,148 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:15,472 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:15,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 1.8279999999995198. input_tokens=1812, output_tokens=151
21:38:15,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:15,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 3.188000000000102. input_tokens=1806, output_tokens=270
21:38:15,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:15,698 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:15,765 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:15,765 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:16,282 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:16,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 1.8909999999996217. input_tokens=1796, output_tokens=166
21:38:16,399 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:16,399 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:17,187 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:17,189 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:17,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:17,424 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:17,722 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:17,723 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:17,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:17,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.1089999999985594. input_tokens=34, output_tokens=316
21:38:18,143 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:18,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.67200000000048. input_tokens=34, output_tokens=222
21:38:18,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:18,252 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:18,279 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:18,280 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:18,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:18,995 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:19,54 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:19,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2969999999986612. input_tokens=1808, output_tokens=229
21:38:19,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:19,216 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 1.375. input_tokens=1810, output_tokens=132
21:38:19,997 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:19,998 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:20,109 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:20,222 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:20,224 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:20,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.95299999999952. input_tokens=1810, output_tokens=421
21:38:20,419 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:20,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.3289999999997235. input_tokens=34, output_tokens=143
21:38:20,992 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:20,992 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:21,532 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:21,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 1.4689999999991414. input_tokens=1805, output_tokens=122
21:38:21,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:21,699 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:21,751 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:21,752 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:21,996 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:21,998 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:22,41 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:22,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.625. input_tokens=34, output_tokens=159
21:38:22,273 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:22,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 2.79700000000048. input_tokens=1813, output_tokens=275
21:38:22,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:22,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 3.04700000000048. input_tokens=34, output_tokens=308
21:38:22,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:22,665 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:22,665 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 195322, Requested 7504. Please try again in 847ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:38:22,666 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'a very good player yet and they kept\nshouting different bits of advice at him, which was confusing. "Don\'t\nsend me there, can\'t you see his knight? Send him, we can afford to lose\nhim." On Christmas Eve, Harry went to bed looking forward to the next\nday for the food and the fun, but not expecting any presents at all.\nWhen he woke early in the morning, however, the first thing he saw was a\nsmall pile of packages at the foot of his bed.\n\n"Merry Christmas," said Ron sleepily as Harry scrambled out of bed and\npulled on his bathrobe.\n\n"You, too," said Harry. "Will you look at this? I\'ve got some presents!"\n\n"What did you expect, turnips?" said Ron, turning to his own pile, which\nwas a lot bigger than Harry\'s.\n\nHarry picked up the top parcel. It was wrapped in thick brown paper and\nscrawled across it was To Harry, from Hagrid. Inside was a roughly cut\nwooden flute. Hagrid had obviously whittled it himself. Harry blew it --\nit sounded a bit like an owl.\n\nA second, very small parcel contained a note.\n\nWe received your message and enclose your Christmas present. From Uncle\nVernon and Aunt Petunia. Taped to the note was a fifty-pence piece.\n\n"That\'s friendly," said Harry.\n\nRon was fascinated by the fifty pence.\n\n"Weird!" he said, \'NMat a shape! This is money?"\n\n"You can keep it," said Harry, laughing at how pleased Ron was. "Hagrid\nand my aunt and uncle -- so who sent these?"\n\n"I think I know who that one\'s from," said Ron, turning a bit pink and\npointing to a very lumpy parcel. "My mom. I told her you didn\'t expect\nany presents and -- oh, no," he groaned, "she\'s made you a Weasley\nsweater."\n\nHarry had torn open the parcel to find a thick, hand-knitted sweater in\nemerald green and a large box of homemade fudge.\n\n"Every year she makes us a sweater," said Ron, unwrapping his own, "and\nmine\'s always maroon."\n\n"That\'s really nice of her," said Harry, trying the fudge, which was\nvery tasty.\n\nHis next present also contained candy -- a large box of Chocolate Frogs\nfrom Hermione.\n\nThis only left one parcel. Harry picked it up and felt it. It was very\nlight. He unwrapped it.\n\nSomething fluid and silvery gray went slithering to the floor where it\nlay in gleaming folds. Ron gasped.\n\n"I\'ve heard of those," he said in a hushed voice, dropping the box of\nEvery Flavor Beans he\'d gotten from Hermione. "If that\'s what I think it\nis -- they\'re really rare, and really valuable."\n\n"What is it?"\n\nHarry picked the shining, silvery cloth off the floor. It was strange to\nthe touch, like water woven into material.\n\n"It\'s an invisibility cloak," said Ron, a look of awe on his face. "I\'m\nsure it is -- try it on."\n\nHarry threw the cloak around his shoulders and Ron gave a yell.\n\n"It is! Look down!"\n\nHarry looked down at his feet, but they were gone. He dashed to the\nmirror. Sure enough, his reflection looked back at him, just his head\nsuspended in midair, his body completely invisible. He pulled the cloak\nover his head and his reflection vanished completely.\n\n"There\'s a note!" said Ron suddenly. "A note fell out of it!"\n\nHarry pulled off the cloak and seized the letter. Written in narrow,\nloopy writing he had never seen before were the following words: Your\nfather left this in my possession before he died. It is time it was\nreturned to you. Use it well.\n\nA Very Merry Christmas to you.\n\n\nThere was no signature. Harry stared at the note. Ron was admiring the\ncloak.\n\n"I\'d give anything for one of these," he said. "Anything. What\'s the\nmatter?"\n\n"Nothing," said Harry. He felt very strange. Who had sent the cloak? Had\nit really once belonged to his father?\n\nBefore he could say or think anything else, the dormitory door was flung\nopen and Fred and George Weasley bounded in. Harry stuffed the cloak\nquickly out of sight. He didn\'t feel like sharing it with anyone else\nyet.\n\n"Merry Christmas!"\n\n"Hey, look -- Harry\'s got a Weasley sweater, too!"\n\nFred and George were wearing blue sweaters, one with a large yellow F on\nit, the other a G.\n\n"Harry\'s is better than ours, though," said Fred, holding up Harry\'s\nsweater. "She obviously makes more of an effort if you\'re not family."\n\n"Why aren\'t you wearing yours, Ron?" George demanded. "Come on, get it\non, they\'re lovely and warm."\n\n"I hate maroon," Ron moaned halfheartedly as he pulled it over his head.\n\n"You haven\'t got a letter on yours," George observed. "I suppose she\nthinks you don\'t forget your name. But we\'re not stupid -- we know we\'re\ncalled Gred and Forge."\n\n"What\'s all th is noise.\n\nPercy Weasley stuck his head through the door, looking disapproving. He\nhad clearly gotten halfway through unwrapping his presents as he, too,\ncarried a lumpy sweater over his arm, which\n\nFred seized.\n\n"P for prefect! Get it on, Percy, come on, we\'re all wearing ours, even\nHarry got one."\n\n"I -- don\'t -- want said Percy thickly, as the twins'}
21:38:22,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:22,785 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:22,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:22,974 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:22,980 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:22,981 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:23,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:23,321 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:23,388 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:23,388 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:23,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:23,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.4060000000008586. input_tokens=1800, output_tokens=101
21:38:23,468 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:23,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 2.04700000000048. input_tokens=1810, output_tokens=148
21:38:24,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:24,105 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:24,216 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:24,220 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:24,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:24,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.0. input_tokens=34, output_tokens=150
21:38:24,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:24,383 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:24,644 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:24,645 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:25,143 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:25,145 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:25,146 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 193572, Requested 7938. Please try again in 453ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:38:25,147 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '.... D\'you want the card, you\ncollect them, don\'t you?"\n\nAs Neville walked away, Harry looked at the Famous Wizard card.\n\n"Dumbledore again," he said, "He was the first one I ever-"\n\nHe gasped. He stared at the back of the card. Then he looked up at Ron\nand Hermione.\n\n"I\'ve found him!" he whispered. "I\'ve found Flamel! I told you I\'d read\nthe name somewhere before, I read it on the train coming here -- listen\nto this: \'Dumbledore is particularly famous for his defeat of the dark\nwizard Grindelwald in 1945, for the discovery of the twelve uses of\ndragon\'s blood, and his work on alchemy with his partner, Nicolas\nFlamel\'!"\n\nHermione jumped to her feet. She hadn\'t looked so excited since they\'d\ngotten back the marks for their very first piece of homework.\n\n"Stay there!" she said, and she sprinted up the stairs to the girls\'\ndormitories. Harry and Ron barely had time to exchange mystified looks\nbefore she was dashing back, an enormous old book in her arms.\n\n"I never thought to look in here!" she whispered excitedly. "I got this\nout of the library weeks ago for a bit of light reading."\n\n"Light?" said Ron, but Hermione told him to be quiet until she\'d looked\nsomething up, and started flicking frantically through the pages,\nmuttering to herself.\n\nAt last she found what she was looking for.\n\n"I knew it! I knew it!"\n\n"Are we allowed to speak yet?" said Ron grumpily. Hermione ignored him.\n\n"Nicolas Flamel," she whispered dramatically, "is the only known maker\nof the Sorcerer\'s Stone!"\n\nThis didn\'t have quite the effect she\'d expected.\n\n"The what?" said Harry and Ron.\n\n"Oh, honestly, don\'t you two read? Look -- read that, there."\n\nShe pushed the book toward them, and Harry and Ron read: The ancient\nstudy of alchemy is concerned with making the Sorcerer\'s Stone, a\nlegendary substance with astonishing powers. The stone will transform\nany metal into pure gold. It also produces the Elixir of Life, which\nwill make the drinker immortal.\n\nThere have been many reports of the Sorcerer\'s Stone over the centuries,\nbut the only Stone currently in existence belongs to Mr. Nicolas Flamel,\nthe noted alchemist and opera lover. Mr. Flamel, who celebrated his six\nhundred and sixty-fifth birthday last year, enjoys a quiet life in Devon\nwith his wife, Perenelle (six hundred and fifty-eight).\n\n"See?" said Hermione, when Harry and Ron had finished. "The dog must be\nguarding Flamel\'s Sorcerer\'s Stone! I bet he asked Dumbledore to keep it\nsafe for him, because they\'re friends and he knew someone was after it,\nthat\'s why he wanted the Stone moved out of Gringotts!"\n\n"A stone that makes gold and stops you from ever dying!" said Harry. "No\nwonder Snape\'s after it! Anyone would want it."\n\n"And no wonder we couldn\'t find Flamel in that Study of Recent\nDevelopments in Wizardry," said Ron. "He\'s not exactly recent if he\'s\nsix hundred and sixty-five, is he?"\n\nThe next morning in Defense Against the Dark Arts, while copying down\ndifferent ways of treating werewolf bites, Harry and Ron were still\ndiscussing what they\'d do with a Sorcerer\'s Stone if they had one. It\nwasn\'t until Ron said he\'d buy his own Quidditch team that Harry\nremembered about Snape and the coming match.\n\n"I\'m going to play," he told Ron and Hermione. "If I don\'t, all the\nSlytherins will think I\'m just too scared to face Snape. I\'ll show\nthem... it\'ll really wipe the smiles off their faces if we win."\n\n"Just as long as we\'re not wiping you off the field," said Hermione.\n\nAs the match drew nearer, however, Harry became more and more nervous,\nwhatever he told Ron and Hermione. The rest of the team wasn\'t too calm,\neither. The idea of overtaking Slytherin in the house championship was\nwonderful, no one had done it for seven years, but would they be allowed\nto, with such a biased referee?\n\nHarry didn\'t know whether he was imagining it or not, but he seemed to\nkeep running into Snape wherever he went. At times, he even wondered\nwhether Snape was following him, trying to catch him on his own. Potions\nlessons were turning into a sort of weekly torture, Snape was so\nhorrible to Harry. Could Snape possibly know they\'d found out about the\nSorcerer\'s Stone? Harry didn\'t see how he could -- yet he sometimes had\nthe horrible feeling that Snape could read minds.\n\nHarry knew, when they wished him good luck outside the locker rooms the\nnext afternoon, that Ron and Hermione were wondering whether they\'d ever\nsee him alive again. This wasn\'t what you\'d call comforting. Harry\nhardly heard a word of Wood\'s pep talk as he pulled on his Quidditch\nrobes and picked up his Nimbus Two Thousand.\n\nRon and Hermione, meanwhile, had found a place in the stands next to\nNeville, who couldn\'t understand why they looked so grim and worried, or\nwhy they had both brought their wands to the match. Little did Harry\nknow that Ron and Hermione had been secretly practicing the Leg-Locker\nCurse. They\'d gotten the idea from Malfoy using it on Neville, and were\nready to use it on Snape if he showed any sign of wanting to hurt Harry.\n\n"Now, don'}
21:38:25,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:25,655 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:25,808 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:25,808 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:26,457 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:26,457 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:27,276 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:27,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.1560000000008586. input_tokens=1807, output_tokens=136
21:38:27,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:27,417 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:27,417 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 193669, Requested 7689. Please try again in 407ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:38:27,418 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'later,\nthey were surprised to see that all the curtains were closed. Hagrid\ncalled "Who is it?" before he let them in, and then shut the door\nquickly behind them.\n\nIt was stifling hot inside. Even though it was such a warm day, there\nwas a blazing fire in the grate. Hagrid made them tea and offered them\nstoat sandwiches, which they refused.\n\n"So -- yeh wanted to ask me somethin\'?"\n\n"Yes," said Harry. There was no point beating around the bush. "We were\nwondering if you could tell us what\'s guarding the Sorcerer\'s Stone\napart from Fluffy."\n\nHagrid frowned at him.\n\n"0\' course I cant, he said. "Number one, I don\' know meself. Number two,\nyeh know too much already, so I wouldn\' tell yeh if I could. That\nStone\'s here fer a good reason. It Was almost stolen outta Gringotts - I\ns\'ppose yeh\'ve worked that out an\' all? Beats me how yeh even know abou\'\nFluffy."\n\n"Oh, come on, Hagrid, you might not want to tell us, but you do know,\nyou know everything that goes on round here," said Hermione in a warm,\nflattering voice. Hagrid\'s beard twitched and they could tell he was\nsmiling. "We only wondered who had done the guarding, really." Hermione\nwent on. "We wondered who Dumbledore had trusted enough to help him,\napart from you."\n\nHagrid\'s chest swelled at these last words. Harry and Ron beamed at\nHermione.\n\n"Well, I don\' s\'pose it could hurt ter tell yeh that... let\'s see... he\nborrowed Fluffy from me... then some o\' the teachers did enchantments...\nProfessor Sprout -- Professor Flitwick -- Professor McGonagall --" he\nticked them off on his fingers, "Professor Quirrell -- an\' Dumbledore\nhimself did somethin\', o\' course. Hang on, I\'ve forgotten someone. Oh\nyeah, Professor Snape."\n\n"Snape?"\n\n"Yeah -- yer not still on abou\' that, are yeh? Look, Snape helped\nprotect the Stone, he\'s not about ter steal it."\n\nHarry knew Ron and Hermione were thinking the same as he was. If Snape\nhad been in on protecting the Stone, it must have been easy to find out\nhow the other teachers had guarded it. He probably knew everything --\nexcept, it seemed, Quirrell\'s spell and how to get past Fluffy.\n\n"You\'re the only one who knows how to get past Fluffy. aren\'t you,\nHagrid?" said Harry anxiously. "And you wouldn\'t tell anyone, would you?\nNot even one of the teachers?"\n\n"Not a soul knows except me an\' Dumbledore," said Hagrid proudly.\n\n"Well, that\'s something," Harry muttered to the others. "Hagrid, can we\nhave a window open? I\'m boiling."\n\n"Can\'t, Harry, sorry," said Hagrid. Harry noticed him glance at the\nfire. Harry looked at it, too.\n\n"Hagrid -- what\'s that?"\n\nBut he already knew what it was. In the very heart of the fire,\nunderneath the kettle, was a huge, black egg.\n\n"Ah," said Hagrid, fiddling nervously with his beard, "That\'s er..."\n\n"Where did you get it, Hagrid?" said Ron, crouching over the fire to get\na closer look at the egg. "It must\'ve cost you a fortune."\n\n"Won it," said Hagrid. "Las\' night. I was down in the village havin\' a\nfew drinks an\' got into a game o\' cards with a stranger. Think he was\nquite glad ter get rid of it, ter be honest."\n\n"But what are you going to do with it when it\'s hatched?" said Hermione.\n\n"Well, I\'ve bin doin\' some readin\' , said Hagrid, pulling a large book\nfrom under his pillow. "Got this outta the library -- Dragon Breeding\nfor Pleasure and Profit -- it\'s a bit outta date, o\' course, but it\'s\nall in here. Keep the egg in the fire, \'cause their mothers breathe on I\nem, see, an\' when it hatches, feed it on a bucket o\' brandy mixed with\nchicken blood every half hour. An\' see here -- how ter recognize\ndiff\'rent eggs -- what I got there\'s a Norwegian Ridgeback. They\'re\nrare, them."\n\nHe looked very pleased with himself, but Hermione didn\'t.\n\n"Hagrid, you live in a wooden house," she said.\n\nBut Hagrid wasn\'t listening. He was humming merrily as he stoked the\nfire.\n\nSo now they had something else to worry about: what might happen to\nHagrid if anyone found out he was hiding an illegal dragon in his hut.\n"Wonder what it\'s like to have a peaceful life," Ron sighed, as evening\nafter evening they struggled through all the extra homework they were\ngetting. Hermione had now started making study schedules for Harry and\nRon, too. It was driving them nuts.\n\nThen, one breakfast time, Hedwig brought Harry another note from Hagrid.\nHe had written only two words: It\'s hatching.\n\nRon wanted to skip Herbology and go straight down to the hut. Hermione\nwouldn\'t hear of it.\n\n"Hermione, how many times in our lives are we going to see a dragon\nhatching?"\n\n"We\'ve got lessons, we\'ll get into trouble, and that\'s nothing to what\nHagrid\'s going to be in when'}
21:38:27,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:27,854 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:28,100 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:28,101 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:28,228 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:28,228 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:28,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:28,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.860000000000582. input_tokens=1810, output_tokens=332
21:38:28,401 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:28,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 4.54700000000048. input_tokens=34, output_tokens=373
21:38:28,731 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:28,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.515999999999622. input_tokens=34, output_tokens=435
21:38:28,855 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:28,856 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:29,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:29,701 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 2.79700000000048. input_tokens=1810, output_tokens=231
21:38:29,813 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:29,814 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:30,121 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:30,121 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:30,132 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:30,132 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:30,359 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:30,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.9689999999991414. input_tokens=34, output_tokens=169
21:38:30,480 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:30,481 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:30,885 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:30,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.4840000000003783. input_tokens=1802, output_tokens=106
21:38:30,996 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:30,997 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:31,74 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:31,74 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:31,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:31,503 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 2.811999999999898. input_tokens=1806, output_tokens=241
21:38:31,594 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:31,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.17200000000048. input_tokens=1800, output_tokens=274
21:38:31,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:31,708 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:31,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:31,827 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:31,910 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:31,911 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:31,952 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:31,953 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:32,200 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:32,201 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:32,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:32,269 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:32,408 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:32,408 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:32,495 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:32,496 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:32,727 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:32,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.79700000000048. input_tokens=34, output_tokens=280
21:38:32,742 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:32,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 1.9529999999995198. input_tokens=1801, output_tokens=140
21:38:33,87 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:33,88 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:33,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:33,439 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:33,498 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:33,502 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:33,551 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:33,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2339999999985594. input_tokens=1802, output_tokens=187
21:38:33,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:33,656 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:33,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:33,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.2819999999992433. input_tokens=34, output_tokens=199
21:38:34,154 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:34,154 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:34,221 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:34,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 1.2970000000004802. input_tokens=34, output_tokens=104
21:38:34,335 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:34,335 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:34,582 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:34,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8590000000003783. input_tokens=1807, output_tokens=135
21:38:34,787 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:34,789 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:34,991 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:34,992 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:35,250 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:35,255 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:35,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:35,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1409999999996217. input_tokens=34, output_tokens=101
21:38:35,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:35,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 1.9220000000004802. input_tokens=34, output_tokens=188
21:38:35,493 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:35,494 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:35,839 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:35,840 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:36,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:36,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.2340000000003783. input_tokens=34, output_tokens=191
21:38:36,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:36,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.7659999999996217. input_tokens=1810, output_tokens=168
21:38:36,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:36,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 1.1719999999986612. input_tokens=34, output_tokens=90
21:38:37,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:37,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1090000000003783. input_tokens=34, output_tokens=97
21:38:37,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:37,506 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:37,729 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:37,729 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:37,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:37,973 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:38,273 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:38,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 1.8590000000003783. input_tokens=1794, output_tokens=143
21:38:38,296 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:38,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1719999999986612. input_tokens=34, output_tokens=78
21:38:38,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:38,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6090000000003783. input_tokens=1801, output_tokens=87
21:38:39,180 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:39,181 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:39,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:39,651 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:39,798 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:39,799 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:40,240 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:40,241 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:40,241 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 199585, Requested 7790. Please try again in 2.212s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:38:40,243 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'hall and along the dark corridors. UP another\nstaircase, then another -- even one of Harry\'s shortcuts didn\'t make the\nwork much easier.\n\n"Nearly there!" Harry panted as they reached the corridor beneath the\ntallest tower.\n\nThen a sudden movement ahead of them made them almost drop the crate.\nForgetting that they were already invisible, they shrank into the\nshadows, staring at the dark outlines of two people grappling with each\nother ten feet away. A lamp flared.\n\nProfessor McGonagall, in a tartan bathrobe and a hair net, had Malfoy by\nthe ear.\n\n"Detention!" she shouted. "And twenty points from Slytherin! Wandering\naround in the middle of the night, how dare you --"\n\n"You don\'t understand, Professor. Harry Potter\'s coming -- he\'s got a\ndragon!"\n\n"What utter rubbish! How dare you tell such lies! Come on -- I shall see\nProfessor Snape about you, Malfoy!"\n\nThe steep spiral staircase up to the top of the tower seemed the easiest\nthing in the world after that. Not until they\'d stepped out into the\ncold night air did they throw off the cloak, glad to be able to breathe\nproperly again. Hermione did a sort of jig.\n\n"Malfoy\'s got detention! I could sing!"\n\n"Don\'t," Harry advised her.\n\nChuckling about Malfoy, they waited, Norbert thrashing about in his\ncrate. About ten minutes later, four broomsticks came swooping down out\nof the darkness.\n\nCharlie\'s friends were a cheery lot. They showed Harry and Hermione the\nharness they\'d rigged up, so they could suspend Norbert between them.\nThey all helped buckle Norbert safely into it and then Harry and\nHermione shook hands with the others and thanked them very much.\n\nAt last, Norbert was going... going... gone.\n\nThey slipped back down the spiral staircase, their hearts as light as\ntheir hands, now that Norbert was off them. No more dragon -- Malfoy in\ndetention -- what could spoil their happiness?\n\nThe answer to that was waiting at the foot of the stairs. As they\nstepped into the corridor, Filch\'s face loomed suddenly out of the\ndarkness.\n\n"Well, well, well," he whispered, "we are in trouble."\n\nThey\'d left the invisibility cloak on top of the tower.\n\n\nCHAPTER FIFTEEN\n\nTHE FORIBIDDEN FOREST\n\nThings couldn\'t have been worse.\n\nFilch took them down to Professor McGonagall\'s study on the first floor,\nwhere they sat and waited without saying a word to each other. Hermione\nwas trembling. Excuses, alibis, and wild cover- up stories chased each\nother around Harry\'s brain, each more feeble than the last. He couldn\'t\nsee how they were going to get out of trouble this time. They were\ncornered. How could they have been so stupid as to forget the cloak?\nThere was no reason on earth that Professor McGonagall would accept for\ntheir being out of bed and creeping around the school in the dead of\nnight, let alone being up the tallest astronomy tower, which was\nout-of-bounds except for classes. Add Norbert and the invisibility\ncloak, and they might as well be packing their bags already.\n\nHad Harry thought that things couldn\'t have been worse? He was wrong.\nWhen Professor McGonagall appeared, she was leading Neville.\n\n"Harry!" Neville burst Out, the moment he saw the other two. "I was\ntrying to find you to warn you, I heard Malfoy saying he was going to\ncatch you, he said you had a drag --"\n\nHarry shook his head violently to shut Neville up, but Professor\nMcGonagall had seen. She looked more likely to breathe fire than Norbert\nas she towered over the three of them.\n\n"I would never have believed it of any of you. Mr. Filch says you were\nup in the astronomy tower. It\'s one o\'clock in the morning. Explain\nyourselves."\n\nIt was the first time Hermione had ever failed to answer a teacher\'s\nquestion. She was staring at her slippers, as still as a statue.\n\n"I think I\'ve got a good idea of what\'s been going on," said Professor\nMcGonagall. "It doesn\'t take a genius to work it out. You fed Draco\nMalfoy some cock-and-bull story about a dragon, trying to get him out of\nbed and into trouble. I\'ve already caught him. I suppose you think it\'s\nfunny that Longbottom here heard the story and believed it, too?"\n\nHarry caught Neville\'s eye and tried to tell him without words that this\nwasn\'t true, because Neville was looking stunned and hurt. Poor,\nblundering Neville -- Harry knew what it must have cost him to try and\nfind them in the dark, to warn them.\n\n"I\'m disgusted," said Professor McGonagall. "Four students out of bed in\none night! I\'ve never heard of such a thing before! You, Miss Granger, I\nthought you had more sense. As for you, Mr. Potter, I thought Gryffindor\nmeant more to you than this. All three of you will receive detentions --\nyes, you too, Mr. Longbottom, nothing gives you the right to walk around\nschool at night, especially these days, it\'s very dangerous -- and fifty\npoints will be taken from Gryffindor."\n\n"Fifty?" Harry gasped -- they would lose the lead, the lead he\'d won in\nthe last Quidditch match.\n\n"Fifty points each," said Professor McGonagall, breathing heavily\nthrough her'}
21:38:40,361 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:40,362 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:40,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:40,573 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:40,767 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:40,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.5. input_tokens=34, output_tokens=257
21:38:41,18 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:41,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 2.125. input_tokens=1806, output_tokens=175
21:38:41,130 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:41,131 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:41,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:41,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.9690000000009604. input_tokens=1806, output_tokens=297
21:38:41,348 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:41,349 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:41,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:41,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 1.6869999999998981. input_tokens=1806, output_tokens=140
21:38:41,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:41,944 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:42,16 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:42,17 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:42,114 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:42,115 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:42,229 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:42,230 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:42,231 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 195537, Requested 7549. Please try again in 925ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:38:42,232 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'of candles.\n\n"How many days you got left until yer holidays?" Hagrid asked.\n\n"Just one," said Hermione. "And that reminds me -Harry, Ron, we\'ve got\nhalf an hour before lunch, we should be in the library."\n\n"Oh yeah, you\'re right," said Ron, tearing his eyes away from Professor\nFlitwick, who had golden bubbles blossoming out of his wand and was\ntrailing them over the branches of the new tree.\n\n"The library?" said Hagrid, following them out of the hall. "Just before\nthe holidays? Bit keen, aren\'t yeh?"\n\n"Oh, we\'re not working," Harry told him brightly. "Ever since you\nmentioned Nicolas Flamel we\'ve been trying to find out who he is."\n\n"You what?" Hagrid looked shocked. "Listen here -- I\'ve told yeh -- drop\nit. It\'s nothin\' to you what that dog\'s guardin\'."\n\n"We just want to know who Nicolas Flamel is, that\'s all," said Hermione.\n\n"Unless you\'d like to tell us and save us the trouble?" Harry added. "We\nmust\'ve been through hundreds of books already and we can\'t find him\nanywhere -- just give us a hint -- I know I\'ve read his name somewhere."\n\n"I\'m sayin\' nothin, said Hagrid flatly.\n\n"Just have to find out for ourselves, then," said Ron, and they left\nHagrid looking disgruntled and hurried off to the library.\n\nThey had indeed been searching books for Flamel\'s name ever since Hagrid\nhad let it slip, because how else were they going to find out what Snape\nwas trying to steal? The trouble was, it was very hard to know where to\nbegin, not knowing what Flamel might have done to get himself into a\nbook. He wasn\'t in Great Wizards of the Twentieth Century, or Notable\nMagical Names of Our Time; he was missing, too, from Important Modern\nMagical Discoveries, and A Study of Recent Developments in Wizardry. And\nthen, of course, there was the sheer size of the library; tens of\nthousands of books; thousands of shelves; hundreds of narrow rows.\n\nHermione took out a list of subjects and titles she had decided to\nsearch while Ron strode off down a row of books and started pulling them\noff the shelves at random. Harry wandered over to the Restricted\nSection. He had been wondering for a while if Flamel wasn\'t somewhere in\nthere. Unfortunately, you needed a specially signed note from one of the\nteachers to look in any of the restricted books, and he knew he\'d never\nget one. These were the books containing powerful Dark Magic never\ntaught at Hogwarts, and only read by older students studying advanced\nDefense Against the Dark Arts.\n\n"What are you looking for, boy?"\n\n"Nothing," said Harry.\n\nMadam Pince the librarian brandished a feather duster at him.\n\n"You\'d better get out, then. Go on -- out!"\n\nWishing he\'d been a bit quicker at thinking up some story, Harry left\nthe library. He, Ron, and Hermione had already agreed they\'d better not\nask Madam Pince where they could find Flamel. They were sure she\'d be\nable to tell them, but they couldn\'t risk Snape hearing what they were\nup to.\n\nHarry waited outside in the corridor to see if the other two had found\nanything, but he wasn\'t very hopeful. They had been looking for two\nweeks, after A, but as they only had odd moments between lessons it\nwasn\'t surprising they\'d found nothing. What they really needed was a\nnice long search without Madam Pince breathing down their necks.\n\nFive minutes later, Ron and Hermione joined him, shaking their heads.\nThey went off to lunch.\n\n"You will keep looking while I\'m away, won\'t you?" said Hermione. "And\nsend me an owl if you find anything."\n\n"And you could ask your parents if they know who Flamel is," said Ron.\n"It\'d be safe to ask them."\n\n"Very safe, as they\'re both dentists," said Hermione.\n\nOnce the holidays had started, Ron and Harry were having too good a time\nto think much about Flamel. They had the dormitory to themselves and the\ncommon room was far emptier than usual, so they were able to get the\ngood armchairs by the fire. They sat by the hour eating anything they\ncould spear on a toasting fork -- bread, English muffins, marshmallows\n-- and plotting ways of getting Malfoy expelled, which were fun to talk\nabout even if they wouldn\'t work.\n\nRon also started teaching Harry wizard chess. This was exactly like\nMuggle chess except that the figures were alive, which made it a lot\nlike directing troops in battle. Ron\'s set was very old and battered.\nLike everything else he owned, it had once belonged to someone else in\nhis family -- in this case, his grandfather. However, old chessmen\nweren\'t a drawback at all. Ron knew them so well he never had trouble\ngetting them to do what he wanted.\n\nHarry played with chessmen Seamus Finnigan had lent him, and they didn\'t\ntrust him at all. He wasn\'t a very good player yet and they kept\nshouting different bits of advice at him, which was confusing. "Don\'t\nsend me there, can\'t you see his knight? Send him, we can afford to lose\nhim." On Christmas Eve, Harry went to bed looking forward to the next\nday for the food and the fun, but not expecting any presents at all.\nWhen he woke early in the morning, however, the first thing he saw was a\nsmall pile of packages'}
21:38:42,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:42,353 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:42,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:42,504 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:42,594 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:42,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 1.9059999999990396. input_tokens=1805, output_tokens=141
21:38:43,61 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:43,63 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:43,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:43,190 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:43,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:43,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 2.3280000000013388. input_tokens=34, output_tokens=200
21:38:43,633 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:43,633 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:43,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:43,656 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:43,737 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:43,738 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:44,314 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:44,319 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:44,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:44,428 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:44,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:44,864 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:45,81 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:45,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.139999999999418. input_tokens=34, output_tokens=132
21:38:45,484 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:45,484 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:45,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:45,487 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:45,763 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:45,764 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:46,91 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:46,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0. input_tokens=34, output_tokens=89
21:38:46,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:46,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.515999999999622. input_tokens=1804, output_tokens=481
21:38:46,315 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:46,316 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:46,345 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:46,346 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:46,455 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:46,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.7180000000007567. input_tokens=1811, output_tokens=165
21:38:46,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:46,937 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:47,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:47,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1560000000008586. input_tokens=34, output_tokens=92
21:38:47,606 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:47,606 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:48,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:48,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5789999999997235. input_tokens=34, output_tokens=100
21:38:48,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:48,91 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:48,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:48,98 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:48,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:48,326 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:48,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:48,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 8 retries took 4.95299999999952. input_tokens=34, output_tokens=445
21:38:48,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:48,708 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:49,332 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:49,333 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:49,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:49,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 1.4060000000008586. input_tokens=34, output_tokens=95
21:38:49,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:49,487 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:50,69 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:50,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 1.7030000000013388. input_tokens=34, output_tokens=109
21:38:50,622 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:50,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.375. input_tokens=34, output_tokens=206
21:38:50,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:50,842 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:51,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:51,28 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:51,288 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:51,290 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 2.0939999999991414. input_tokens=34, output_tokens=203
21:38:51,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:51,643 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:51,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:51,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 1.4840000000003783. input_tokens=1800, output_tokens=137
21:38:52,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:52,76 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:52,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:52,249 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:52,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:52,321 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:52,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:52,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 1.515000000001237. input_tokens=1798, output_tokens=100
21:38:52,901 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:52,902 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:53,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:53,322 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:53,322 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 193132, Requested 7390. Please try again in 156ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:38:53,323 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'unny rabbit. When it bit me he told me off for frightening it. And when\nI left, he was singing it a lullaby."\n\nThere was a tap on the dark window.\n\n"It\'s Hedwig!" said Harry, hurrying to let her in. "She\'ll have\nCharlie\'s answer!"\n\nThe three of them put their heads together to read the note.\n\nDear Ron,\n\nHow are you? Thanks for the letter -- I\'d be glad to take the Norwegian\nRidgeback, but it won\'t be easy getting him here. I think the best thing\nwill be to send him over with some friends of mine who are coming to\nvisit me next week. Trouble is, they mustn\'t be seen carrying an illegal\ndragon.\n\nCould you get the Ridgeback up the tallest tower at midnight on\nSaturday? They can meet you there and take him away while it\'s still\ndark.\n\nSend me an answer as soon as possible.\n\nLove,\n\nCharlie\n\nThey looked at one another.\n\n"We\'ve got the invisibility cloak," said Harry. "It shouldn\'t be too\ndifficult -- I think the cloaks big enough to cover two of us and\nNorbert."\n\nIt was a mark of how bad the last week had been that the other two\nagreed with him. Anything to get rid of Norbert -- and Malfoy.\n\nThere was a hitch. By the next morning, Ron\'s bitten hand had swollen to\ntwice its usual size. He didn\'t know whether it was safe to go to Madam\nPomfrey -- would she recognize a dragon bite? By the afternoon, though,\nhe had no choice. The cut had turned a nasty shade of green. It looked\nas if Norbert\'s fangs were poisonous.\n\nHarry and Hermione rushed up to the hospital wing at the end of the day\nto find Ron in a terrible state in bed.\n\n"It\'s not just my hand," he whispered, "although that feels like it\'s\nabout to fall off. Malfoy told Madam Pomfrey he wanted to borrow one of\nmy books so he could come and have a good laugh at me. He kept\nthreatening to tell her what really bit me -- I\'ve told her it was a\ndog, but I don\'t think she believes me -I shouldn\'t have hit him at the\nQuidditch match, that\'s why he\'s doing this."\n\nHarry and Hermione tried to calm Ron down.\n\n"It\'ll all be over at midnight on Saturday," said Hermione, but this\ndidn\'t soothe Ron at all. On the contrary, he sat bolt upright and broke\ninto a sweat.\n\n"Midnight on Saturday!" he said in a hoarse voice. "Oh no oh no -- I\'ve\njust remembered -- Charlie\'s letter was in that book Malfoy took, he\'s\ngoing to know we\'re getting rid of Norbert."\n\nHarry and Hermione didn\'t get a chance to answer. Madam Pomfrey came\nover at that moment and made them leave, saying Ron needed sleep.\n\n"It\'s too late to change the plan now," Harry told Hermione. "We haven\'t\ngot time to send Charlie another owl, and this could be our only chance\nto get rid of Norbert. We\'ll have to risk it. And we have got the\ninvisibility cloak, Malfoy doesn\'t know about that."\n\nThey found Fang, the boarhound, sitting outside with a bandaged tail\nwhen they went to tell Hagrid, who opened a window to talk to them.\n\n"I won\'t let you in," he puffed. "Norbert\'s at a tricky stage -- nothin\'\nI can\'t handle."\n\nWhen they told him about Charlie\'s letter, his eyes filled with tears,\nalthough that might have been because Norbert had just bitten him on the\nleg.\n\n"Aargh! It\'s all right, he only got my boot -- jus\' playin\' -- he\'s only\na baby, after all."\n\nThe baby banged its tail on the wall, making the windows rattle. Harry\nand Hermione walked back to the castle feeling Saturday couldn\'t come\nquickly enough.\n\nThey would have felt sorry for Hagrid when the time came for him to say\ngood-bye to Norbert if they hadn\'t been so worried about what they had\nto do. It was a very dark, cloudy night, and they were a bit late\narriving at Hagrid\'s hut because they\'d had to wait for Peeves to get\nout of their way in the entrance hall, where he\'d been playing tennis\nagainst the wall. Hagrid had Norbert packed and ready in a large crate.\n\n"He\'s got lots o\' rats an\' some brandy fer the journey," said Hagrid in\na muffled voice. "An\' I\'ve packed his teddy bear in case he gets\nlonely."\n\nFrom inside the crate came ripping noises that sounded to Harry as\nthough the teddy was having his head torn off.\n\n"Bye-bye, Norbert!" Hagrid sobbed, as Harry and Hermione covered the\ncrate with the invisibility cloak and stepped underneath it themselves.\n"Mommy will never forget you!"\n\nHow they managed to get the crate back up to the castle, they never\nknew. Midnight ticked nearer as they heaved Norbert up the marble\nstaircase in the entrance hall and along the dark corridors. UP another\nstaircase, then another -- even one of Harry\'s shortcuts didn\'t make the\nwork much easier.\n\n"Nearly there!" Harry panted as they reached the corridor beneath the\ntallest tower.\n\nThen a sudden movement ahead of them made them almost drop the crate.\nForgetting that they were already invisible, they shrank into the\nshadows, staring at the dark outlines of two people grappling with each\nother ten feet away. A'}
21:38:53,334 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:53,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 3.4210000000002765. input_tokens=1813, output_tokens=241
21:38:53,438 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:53,439 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:53,449 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:53,449 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:53,497 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:53,497 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:53,993 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:53,994 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:54,437 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:54,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.9380000000001019. input_tokens=34, output_tokens=221
21:38:54,554 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:54,555 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:54,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:54,556 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:54,713 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:54,714 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:55,121 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:55,124 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:55,232 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:55,232 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:55,250 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:55,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.3590000000003783. input_tokens=34, output_tokens=231
21:38:55,382 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:55,383 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:55,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:55,918 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:56,436 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:56,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 3.2030000000013388. input_tokens=34, output_tokens=217
21:38:56,493 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:56,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 2.9069999999992433. input_tokens=34, output_tokens=193
21:38:56,611 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:56,612 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:56,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:56,614 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:56,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:56,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 2.5. input_tokens=34, output_tokens=198
21:38:57,212 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:57,213 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:57,293 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:57,293 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:57,790 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:57,794 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:57,922 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:57,922 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:58,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:58,136 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:58,201 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:58,201 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:58,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:58,215 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:58,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:58,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.7190000000009604. input_tokens=1889, output_tokens=353
21:38:58,992 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:58,996 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:59,56 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:59,56 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:59,107 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:59,107 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:59,145 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:59,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.29700000000048. input_tokens=34, output_tokens=197
21:38:59,262 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:38:59,262 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:38:59,778 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:38:59,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.32799999999952. input_tokens=34, output_tokens=312
21:39:00,14 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:00,15 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:00,29 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:00,29 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:00,663 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:00,663 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:00,964 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:00,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 4.609000000000378. input_tokens=1802, output_tokens=224
21:39:01,18 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:01,19 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:01,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:01,47 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:01,222 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:01,223 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:01,915 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:01,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 2.75. input_tokens=1808, output_tokens=139
21:39:02,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:02,25 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:02,206 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:02,206 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:02,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:02,377 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:02,456 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:02,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.686999999999898. input_tokens=34, output_tokens=132
21:39:02,468 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:02,469 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:02,566 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:02,567 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:02,665 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:02,667 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:02,720 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:02,721 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:02,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:02,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 2.17200000000048. input_tokens=34, output_tokens=86
21:39:03,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:03,311 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:03,726 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:03,727 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:03,955 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:03,956 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 5.389999999999418. input_tokens=1810, output_tokens=315
21:39:03,979 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:03,980 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:04,34 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:04,34 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:04,39 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:04,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 1.735000000000582. input_tokens=34, output_tokens=95
21:39:04,338 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:04,339 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 1.3430000000007567. input_tokens=1813, output_tokens=143
21:39:04,604 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:04,604 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:04,675 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:04,675 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:04,675 root ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 161, in _process_document
    response = await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\resources\chat\completions.py", line 1633, in create
    return await self._post(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1532, in request
    return await self._request(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\lib\site-packages\openai\_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 196847, Requested 7701. Please try again in 1.364s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
21:39:04,677 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'nearly raised the bewitched ceiling; the stars\noverhead seemed to quiver. Percy could be heard telling the other\nprefects, "My brother, you know! My youngest brother! Got past\nMcGonagall\'s giant chess set!"\n\nAt last there was silence again.\n\n"Second -- to Miss Hermione Granger... for the use of cool logic in the\nface of fire, I award Gryffindor house fifty points."\n\nHermione buried her face in her arms; Harry strongly suspected she had\nburst into tears. Gryffindors up and down the table were beside\nthemselves -- they were a hundred points up. "Third -- to Mr. Harry\nPotter..." said Dumbledore. The room went deadly quiet for pure nerve\nand outstanding courage, I award Gryffindor house sixty points."\n\nThe din was deafening. Those who could add up while yelling themselves\nhoarse knew that Gryffindor now had four hundred and seventy-two points\n-- exactly the same as Slytherin. They had tied for the house cup -- if\nonly Dumbledore had given Harry just one more point.\n\nDumbledore raised his hand. The room gradually fell silent.\n\n"There are all kinds of courage," said Dumbledore, smiling. "It takes a\ngreat deal of bravery to stand up to our enemies, but just as much to\nstand up to our friends. I therefore award ten points to Mr. Neville\nLongbottom."\n\nSomeone standing outside the Great Hall might well have thought some\nsort of explosion had taken place, so loud was the noise that erupted\nfrom the Gryffindor table. Harry, Ron, and Hermione stood up to yell and\ncheer as Neville, white with shock, disappeared under a pile of people\nhugging him. He had never won so much as a point for Gryffindor before.\nHarry, still cheering, nudged Ron in the ribs and pointed at Malfoy, who\ncouldn\'t have looked more stunned and horrified if he\'d just had the\nBody-Bind Curse put on him.\n\n"Which means, Dumbledore called over the storm of applause, for even\nRavenclaw and Hufflepuff were celebrating the downfall of Slytherin, "we\nneed a little change of decoration."\n\nHe clapped his hands. In an instant, the green hangings became scarlet\nand the silver became gold; the huge Slytherin serpent vanished and a\ntowering Gryffindor lion took its place. Snape was shaking Professor\nMcGonagall\'s hand, with a horrible, forced smile. He caught Harry\'s eye\nand Harry knew at once that Snape\'s feelings toward him hadn\'t changed\none jot. This didn\'t worry Harry. It seemed as though life would be back\nto normal next year, or as normal as it ever was at Hogwarts.\n\nIt was the best evening of Harry\'s life, better than winning at\nQuidditch, or Christmas, or knocking out mountain trolls... he would\nnever, ever forget tonight.\n\nHarry had almost forgotten that the exam results were still to come, but\ncome they did. To their great surprise, both he and Ron passed with good\nmarks; Hermione, of course, had the best grades of the first years. Even\nNeville scraped through, his good Herbology mark making up for his\nabysmal Potions one. They had hoped that Goyle, who was almost as stupid\nas he was mean, might be thrown out, but he had passed, too. It was a\nshame, but as Ron said, you couldn\'t have everything in life.\n\nAnd suddenly, their wardrobes were empty, their trunks were packed,\nNeville\'s toad was found lurking in a corner of the toilets; notes were\nhanded out to all students, warning them not to use magic over the\nholidays ("I always hope they\'ll forget to give us these," said Fred\nWeasley sadly); Hagrid was there to take them down to the fleet of boats\nthat sailed across the lake; they were boarding the Hogwarts Express;\ntalking and laughing as the countryside became greener and tidier;\neating Bettie Bott\'s Every Flavor Beans as they sped past Muggle towns;\npulling off their wizard robes and putting on jackets and coats; pulling\ninto platform nine and three-quarters at King\'s Cross Station.\n\nIt took quite a while for them all to get off the platform. A wizened\nold guard was up by the ticket barrier, letting them go through the gate\nin twos and threes so they didn\'t attract attention by all bursting out\nof a solid wall at once and alarming the Muggles.\n\n"You must come and stay this summer," said Ron, "both of you -- I\'ll\nsend you an owl."\n\n"Thanks," said Harry, "I\'ll need something to look forward to." People\njostled them as they moved forward toward the gateway back to the Muggle\nworld. Some of them called:\n\n"Bye, Harry!"\n\n"See you, Potter!"\n\n"Still famous," said Ron, grinning at him.\n\n"Not where I\'m going, I promise you," said Harry.\n\nHe, Ron, and Hermione passed through the gateway together. "There he is,\nMom, there he is, look!"\n\nIt was Ginny Weasley, Ron\'s younger sister, but she wasn\'t pointing at\nRon.\n\n"Harry Potter!" she squealed. "Look, Mom! I can see\n\n"Be quiet, Ginny, and it\'s rude to point."\n\nMrs. Weasley smiled down at them.\n\n"Busy year?" she said.\n\n"Very," said Harry. "Thanks for the fudge and the sweater, Mrs.\nWeasley."\n\n"Oh, it was nothing, dear."\n\n"Ready, are you?"\n\nIt was Uncle Vernon,'}
21:39:05,21 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:05,24 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:05,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:05,28 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:05,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:05,132 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:05,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:05,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 2.061999999999898. input_tokens=34, output_tokens=212
21:39:05,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:05,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1409999999996217. input_tokens=34, output_tokens=93
21:39:06,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:06,521 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:07,201 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:07,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.235000000000582. input_tokens=34, output_tokens=204
21:39:07,448 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:07,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 1.4380000000001019. input_tokens=34, output_tokens=83
21:39:08,336 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:08,341 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:08,344 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:08,345 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:08,671 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:08,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 6.921999999998661. input_tokens=2937, output_tokens=787
21:39:08,791 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:08,791 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:09,228 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:09,229 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:09,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:09,939 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:10,138 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:10,138 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:10,426 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:10,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.0309999999990396. input_tokens=34, output_tokens=166
21:39:10,876 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:10,877 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:11,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:11,248 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:11,739 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:11,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 3.2189999999991414. input_tokens=34, output_tokens=173
21:39:12,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:12,521 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:12,875 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:12,875 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:13,271 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:13,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 7.25. input_tokens=2936, output_tokens=762
21:39:13,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:13,382 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:13,786 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:13,788 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:14,533 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:14,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 2.313000000000102. input_tokens=34, output_tokens=239
21:39:14,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:14,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 1.9220000000004802. input_tokens=34, output_tokens=90
21:39:16,18 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:16,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 4.890999999999622. input_tokens=1808, output_tokens=356
21:39:16,497 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:16,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 6.671999999998661. input_tokens=2936, output_tokens=540
21:39:16,643 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:16,643 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:16,764 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:16,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 9.563000000000102. input_tokens=2936, output_tokens=899
21:39:16,946 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:16,947 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:17,273 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:17,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 3.2659999999996217. input_tokens=34, output_tokens=314
21:39:18,109 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:18,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.0939999999991414. input_tokens=34, output_tokens=177
21:39:18,111 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:18,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 3.0939999999991414. input_tokens=34, output_tokens=328
21:39:18,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:18,919 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:19,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:19,357 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:19,566 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:19,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.063000000000102. input_tokens=34, output_tokens=288
21:39:20,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:20,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 2.0310000000008586. input_tokens=34, output_tokens=228
21:39:21,232 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:21,233 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 2.5939999999991414. input_tokens=34, output_tokens=283
21:39:21,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:21,444 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.8280000000013388. input_tokens=34, output_tokens=272
21:39:22,191 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:22,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 7 retries took 2.04700000000048. input_tokens=1816, output_tokens=122
21:39:23,779 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:23,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 2.54700000000048. input_tokens=34, output_tokens=229
21:39:24,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:24,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8589999999985594. input_tokens=34, output_tokens=110
21:39:29,296 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:29,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 6.764999999999418. input_tokens=34, output_tokens=416
21:39:31,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:31,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 10.0. input_tokens=2936, output_tokens=772
21:39:31,117 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:31,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 2.20299999999952. input_tokens=34, output_tokens=145
21:39:31,222 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:31,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 6 retries took 4.561999999999898. input_tokens=1804, output_tokens=262
21:39:32,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:32,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 3.4840000000003783. input_tokens=34, output_tokens=267
21:39:33,694 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:33,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.4690000000009604. input_tokens=34, output_tokens=194
21:39:36,354 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:36,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.25. input_tokens=34, output_tokens=480
21:39:36,375 datashaper.workflow.workflow INFO executing verb merge_graphs
21:39:36,466 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
21:39:36,700 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
21:39:36,702 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
21:39:36,724 datashaper.workflow.workflow INFO executing verb summarize_descriptions
21:39:37,488 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:37,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6569999999992433. input_tokens=154, output_tokens=23
21:39:37,614 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:37,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7969999999986612. input_tokens=146, output_tokens=44
21:39:37,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:37,808 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9839999999985594. input_tokens=172, output_tokens=50
21:39:37,835 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:37,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0460000000002765. input_tokens=167, output_tokens=68
21:39:37,839 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:37,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9840000000003783. input_tokens=174, output_tokens=77
21:39:37,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:37,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0619999999998981. input_tokens=171, output_tokens=72
21:39:37,928 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:37,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43699999999989814. input_tokens=144, output_tokens=20
21:39:37,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:37,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0940000000009604. input_tokens=181, output_tokens=99
21:39:38,68 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.235000000000582. input_tokens=243, output_tokens=115
21:39:38,124 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2969999999986612. input_tokens=218, output_tokens=103
21:39:38,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3289999999997235. input_tokens=243, output_tokens=97
21:39:38,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3289999999997235. input_tokens=233, output_tokens=121
21:39:38,231 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4220000000004802. input_tokens=205, output_tokens=107
21:39:38,286 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4689999999991414. input_tokens=279, output_tokens=147
21:39:38,325 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4690000000009604. input_tokens=299, output_tokens=118
21:39:38,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5460000000002765. input_tokens=290, output_tokens=150
21:39:38,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5619999999998981. input_tokens=408, output_tokens=148
21:39:38,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5630000000001019. input_tokens=224, output_tokens=127
21:39:38,461 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6710000000002765. input_tokens=221, output_tokens=138
21:39:38,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6880000000001019. input_tokens=154, output_tokens=34
21:39:38,572 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7660000000014406. input_tokens=183, output_tokens=68
21:39:38,590 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7810000000008586. input_tokens=603, output_tokens=125
21:39:38,625 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.40600000000085856. input_tokens=138, output_tokens=15
21:39:38,665 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0470000000004802. input_tokens=290, output_tokens=101
21:39:38,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,762 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9369999999998981. input_tokens=196, output_tokens=103
21:39:38,860 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0319999999992433. input_tokens=488, output_tokens=228
21:39:38,888 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7340000000003783. input_tokens=170, output_tokens=65
21:39:38,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=182, output_tokens=102
21:39:38,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7960000000002765. input_tokens=164, output_tokens=62
21:39:38,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1409999999996217. input_tokens=595, output_tokens=214
21:39:38,983 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:38,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.109999999998763. input_tokens=238, output_tokens=113
21:39:39,23 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9529999999995198. input_tokens=208, output_tokens=101
21:39:39,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46899999999914144. input_tokens=144, output_tokens=20
21:39:39,238 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9219999999986612. input_tokens=166, output_tokens=80
21:39:39,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3440000000009604. input_tokens=441, output_tokens=146
21:39:39,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8289999999997235. input_tokens=172, output_tokens=70
21:39:39,441 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5780000000013388. input_tokens=155, output_tokens=36
21:39:39,489 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5939999999991414. input_tokens=148, output_tokens=25
21:39:39,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8599999999987631. input_tokens=197, output_tokens=68
21:39:39,590 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.014999999999418. input_tokens=223, output_tokens=104
21:39:39,603 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.625. input_tokens=141, output_tokens=37
21:39:39,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9060000000008586. input_tokens=1488, output_tokens=308
21:39:39,744 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7819999999992433. input_tokens=154, output_tokens=48
21:39:39,756 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9220000000004802. input_tokens=478, output_tokens=188
21:39:39,772 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7190000000009604. input_tokens=168, output_tokens=53
21:39:39,861 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.734999999998763. input_tokens=325, output_tokens=166
21:39:39,877 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8590000000003783. input_tokens=183, output_tokens=75
21:39:39,916 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3909999999996217. input_tokens=264, output_tokens=130
21:39:39,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7030000000013388. input_tokens=169, output_tokens=43
21:39:39,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:39,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4680000000007567. input_tokens=147, output_tokens=26
21:39:40,35 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2819999999992433. input_tokens=355, output_tokens=134
21:39:40,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.75. input_tokens=145, output_tokens=43
21:39:40,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,97 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6090000000003783. input_tokens=162, output_tokens=41
21:39:40,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.29700000000048. input_tokens=1176, output_tokens=399
21:39:40,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,142 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2029999999995198. input_tokens=203, output_tokens=106
21:39:40,168 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8909999999996217. input_tokens=163, output_tokens=69
21:39:40,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5470000000004802. input_tokens=159, output_tokens=38
21:39:40,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8899999999994179. input_tokens=187, output_tokens=86
21:39:40,419 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8289999999997235. input_tokens=177, output_tokens=74
21:39:40,527 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.75. input_tokens=194, output_tokens=69
21:39:40,570 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5780000000013388. input_tokens=179, output_tokens=75
21:39:40,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6560000000008586. input_tokens=168, output_tokens=51
21:39:40,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7649999999994179. input_tokens=196, output_tokens=68
21:39:40,754 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,755 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7970000000004802. input_tokens=191, output_tokens=71
21:39:40,770 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1720000000004802. input_tokens=268, output_tokens=119
21:39:40,833 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1710000000002765. input_tokens=484, output_tokens=268
21:39:40,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,919 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.625. input_tokens=427, output_tokens=211
21:39:40,963 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:40,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8590000000003783. input_tokens=194, output_tokens=80
21:39:41,15 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,16 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9840000000003783. input_tokens=232, output_tokens=107
21:39:41,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.688000000000102. input_tokens=581, output_tokens=256
21:39:41,68 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2030000000013388. input_tokens=294, output_tokens=134
21:39:41,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,80 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6710000000002765. input_tokens=169, output_tokens=56
21:39:41,111 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,112 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0779999999995198. input_tokens=254, output_tokens=119
21:39:41,161 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4539999999997235. input_tokens=405, output_tokens=136
21:39:41,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8899999999994179. input_tokens=241, output_tokens=87
21:39:41,179 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0309999999990396. input_tokens=231, output_tokens=113
21:39:41,206 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.875. input_tokens=172, output_tokens=64
21:39:41,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1090000000003783. input_tokens=195, output_tokens=94
21:39:41,225 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,227 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6559999999990396. input_tokens=164, output_tokens=54
21:39:41,245 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.359999999998763. input_tokens=250, output_tokens=135
21:39:41,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5630000000001019. input_tokens=169, output_tokens=87
21:39:41,338 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,339 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.625. input_tokens=176, output_tokens=56
21:39:41,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5319999999992433. input_tokens=152, output_tokens=33
21:39:41,378 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2180000000007567. input_tokens=569, output_tokens=117
21:39:41,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,445 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5310000000008586. input_tokens=151, output_tokens=31
21:39:41,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.75. input_tokens=169, output_tokens=65
21:39:41,781 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2659999999996217. input_tokens=260, output_tokens=127
21:39:41,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5470000000004802. input_tokens=163, output_tokens=38
21:39:41,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0309999999990396. input_tokens=171, output_tokens=86
21:39:41,809 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2339999999985594. input_tokens=225, output_tokens=114
21:39:41,881 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8590000000003783. input_tokens=179, output_tokens=81
21:39:41,934 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8119999999998981. input_tokens=169, output_tokens=63
21:39:41,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:41,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8909999999996217. input_tokens=208, output_tokens=96
21:39:42,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9840000000003783. input_tokens=279, output_tokens=100
21:39:42,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7029999999995198. input_tokens=174, output_tokens=73
21:39:42,110 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0469999999986612. input_tokens=250, output_tokens=92
21:39:42,115 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9069999999992433. input_tokens=167, output_tokens=46
21:39:42,144 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8130000000001019. input_tokens=172, output_tokens=63
21:39:42,235 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.875. input_tokens=1087, output_tokens=251
21:39:42,259 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9369999999998981. input_tokens=224, output_tokens=97
21:39:42,269 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1090000000003783. input_tokens=181, output_tokens=63
21:39:42,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.5. input_tokens=1496, output_tokens=354
21:39:42,447 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2660000000014406. input_tokens=209, output_tokens=112
21:39:42,460 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2810000000008586. input_tokens=169, output_tokens=48
21:39:42,467 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1090000000003783. input_tokens=284, output_tokens=134
21:39:42,491 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0469999999986612. input_tokens=171, output_tokens=81
21:39:42,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46899999999914144. input_tokens=142, output_tokens=26
21:39:42,735 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9380000000001019. input_tokens=193, output_tokens=78
21:39:42,741 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.390999999999622. input_tokens=695, output_tokens=289
21:39:42,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0939999999991414. input_tokens=934, output_tokens=242
21:39:42,964 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:42,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1560000000008586. input_tokens=211, output_tokens=118
21:39:43,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3430000000007567. input_tokens=207, output_tokens=120
21:39:43,144 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1409999999996217. input_tokens=168, output_tokens=46
21:39:43,165 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2819999999992433. input_tokens=211, output_tokens=68
21:39:43,200 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2810000000008586. input_tokens=164, output_tokens=54
21:39:43,243 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9689999999991414. input_tokens=146, output_tokens=34
21:39:43,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7970000000004802. input_tokens=173, output_tokens=73
21:39:43,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5470000000004802. input_tokens=152, output_tokens=31
21:39:43,334 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3590000000003783. input_tokens=170, output_tokens=62
21:39:43,360 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5779999999995198. input_tokens=225, output_tokens=97
21:39:43,466 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.014999999999418. input_tokens=176, output_tokens=65
21:39:43,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.390000000001237. input_tokens=170, output_tokens=60
21:39:43,538 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,539 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7970000000004802. input_tokens=142, output_tokens=34
21:39:43,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8289999999997235. input_tokens=342, output_tokens=156
21:39:43,560 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8279999999995198. input_tokens=168, output_tokens=51
21:39:43,572 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4220000000004802. input_tokens=173, output_tokens=85
21:39:43,674 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.45299999999952. input_tokens=268, output_tokens=153
21:39:43,699 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,700 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.610000000000582. input_tokens=229, output_tokens=86
21:39:43,703 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4680000000007567. input_tokens=155, output_tokens=31
21:39:43,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8590000000003783. input_tokens=172, output_tokens=88
21:39:43,831 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3590000000003783. input_tokens=216, output_tokens=85
21:39:43,845 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8909999999996217. input_tokens=211, output_tokens=81
21:39:43,905 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6720000000004802. input_tokens=280, output_tokens=111
21:39:43,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8280000000013388. input_tokens=198, output_tokens=106
21:39:43,998 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:43,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6400000000012369. input_tokens=149, output_tokens=52
21:39:44,31 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9069999999992433. input_tokens=210, output_tokens=74
21:39:44,56 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7649999999994179. input_tokens=197, output_tokens=73
21:39:44,119 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5630000000001019. input_tokens=168, output_tokens=38
21:39:44,181 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6089999999985594. input_tokens=148, output_tokens=22
21:39:44,223 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0779999999995198. input_tokens=230, output_tokens=117
21:39:44,267 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0159999999996217. input_tokens=177, output_tokens=58
21:39:44,303 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8439999999991414. input_tokens=164, output_tokens=70
21:39:44,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.188000000000102. input_tokens=432, output_tokens=273
21:39:44,725 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6720000000004802. input_tokens=150, output_tokens=49
21:39:44,759 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4220000000004802. input_tokens=169, output_tokens=95
21:39:44,798 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5939999999991414. input_tokens=459, output_tokens=226
21:39:44,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.264999999999418. input_tokens=174, output_tokens=85
21:39:44,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9850000000005821. input_tokens=178, output_tokens=98
21:39:44,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.610000000000582. input_tokens=208, output_tokens=122
21:39:44,949 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4530000000013388. input_tokens=318, output_tokens=162
21:39:44,964 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9210000000002765. input_tokens=161, output_tokens=96
21:39:45,38 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8130000000001019. input_tokens=202, output_tokens=80
21:39:45,59 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,60 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.75. input_tokens=179, output_tokens=51
21:39:45,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4060000000008586. input_tokens=228, output_tokens=85
21:39:45,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1710000000002765. input_tokens=174, output_tokens=77
21:39:45,107 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9529999999995198. input_tokens=714, output_tokens=254
21:39:45,137 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9530000000013388. input_tokens=170, output_tokens=49
21:39:45,181 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4689999999991414. input_tokens=283, output_tokens=120
21:39:45,222 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.375. input_tokens=205, output_tokens=98
21:39:45,234 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.75. input_tokens=183, output_tokens=67
21:39:45,238 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5469999999986612. input_tokens=219, output_tokens=120
21:39:45,335 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0619999999998981. input_tokens=195, output_tokens=84
21:39:45,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,415 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6880000000001019. input_tokens=147, output_tokens=61
21:39:45,525 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:45,526 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:45,574 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7660000000014406. input_tokens=171, output_tokens=58
21:39:45,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.07799999999952. input_tokens=320, output_tokens=192
21:39:45,582 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7649999999994179. input_tokens=176, output_tokens=63
21:39:45,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.07799999999952. input_tokens=241, output_tokens=201
21:39:45,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6720000000004802. input_tokens=159, output_tokens=51
21:39:45,637 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8119999999998981. input_tokens=170, output_tokens=84
21:39:45,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5930000000007567. input_tokens=175, output_tokens=97
21:39:45,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:45,820 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:45,841 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0470000000004802. input_tokens=219, output_tokens=86
21:39:45,965 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:45,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8430000000007567. input_tokens=160, output_tokens=65
21:39:46,8 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0619999999998981. input_tokens=185, output_tokens=76
21:39:46,27 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8909999999996217. input_tokens=182, output_tokens=81
21:39:46,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8910000000014406. input_tokens=193, output_tokens=85
21:39:46,148 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1409999999996217. input_tokens=203, output_tokens=109
21:39:46,154 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,155 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2189999999991414. input_tokens=205, output_tokens=118
21:39:46,162 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,163 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5789999999997235. input_tokens=181, output_tokens=45
21:39:46,315 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.235000000000582. input_tokens=170, output_tokens=56
21:39:46,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0930000000007567. input_tokens=172, output_tokens=71
21:39:46,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,378 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.67200000000048. input_tokens=299, output_tokens=164
21:39:46,397 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,398 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6409999999996217. input_tokens=189, output_tokens=106
21:39:46,404 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0789999999997235. input_tokens=231, output_tokens=99
21:39:46,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5630000000001019. input_tokens=172, output_tokens=38
21:39:46,456 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8279999999995198. input_tokens=169, output_tokens=53
21:39:46,470 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8909999999996217. input_tokens=171, output_tokens=87
21:39:46,524 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,525 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4380000000001019. input_tokens=170, output_tokens=80
21:39:46,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:46,528 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:46,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.485000000000582. input_tokens=287, output_tokens=126
21:39:46,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9849999999987631. input_tokens=189, output_tokens=59
21:39:46,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:46,651 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:46,652 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:46,652 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:46,733 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:46,734 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:46,738 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7819999999992433. input_tokens=164, output_tokens=65
21:39:46,743 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7029999999995198. input_tokens=186, output_tokens=104
21:39:46,781 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1720000000004802. input_tokens=164, output_tokens=75
21:39:46,950 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:46,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6100000000005821. input_tokens=165, output_tokens=43
21:39:47,31 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8909999999996217. input_tokens=178, output_tokens=84
21:39:47,42 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0159999999996217. input_tokens=163, output_tokens=91
21:39:47,71 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0630000000001019. input_tokens=169, output_tokens=65
21:39:47,97 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7189999999991414. input_tokens=162, output_tokens=49
21:39:47,212 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:47,214 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:47,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,255 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0310000000008586. input_tokens=366, output_tokens=124
21:39:47,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.889999999999418. input_tokens=237, output_tokens=114
21:39:47,468 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=210, output_tokens=83
21:39:47,544 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.389999999999418. input_tokens=178, output_tokens=56
21:39:47,628 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5619999999998981. input_tokens=210, output_tokens=77
21:39:47,680 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:47,681 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:47,686 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9060000000008586. input_tokens=180, output_tokens=63
21:39:47,798 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:47,799 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:47,818 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.75. input_tokens=167, output_tokens=49
21:39:47,866 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.625. input_tokens=359, output_tokens=175
21:39:47,910 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8439999999991414. input_tokens=181, output_tokens=61
21:39:47,915 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:47,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5939999999991414. input_tokens=211, output_tokens=94
21:39:48,42 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.639999999999418. input_tokens=251, output_tokens=144
21:39:48,66 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3280000000013388. input_tokens=198, output_tokens=74
21:39:48,100 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9369999999998981. input_tokens=282, output_tokens=141
21:39:48,184 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,186 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.139999999999418. input_tokens=170, output_tokens=55
21:39:48,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6720000000004802. input_tokens=231, output_tokens=152
21:39:48,318 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:48,319 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:48,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:48,349 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:48,355 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:48,356 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:48,421 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,425 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0309999999990396. input_tokens=257, output_tokens=132
21:39:48,536 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9850000000005821. input_tokens=198, output_tokens=77
21:39:48,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.625. input_tokens=173, output_tokens=94
21:39:48,604 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8590000000003783. input_tokens=257, output_tokens=146
21:39:48,632 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7180000000007567. input_tokens=162, output_tokens=54
21:39:48,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8590000000003783. input_tokens=168, output_tokens=73
21:39:48,738 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:48,739 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:48,749 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:48,749 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:48,787 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.4220000000004802. input_tokens=249, output_tokens=93
21:39:48,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:48,910 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:48,974 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:48,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3439999999991414. input_tokens=196, output_tokens=96
21:39:49,31 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2189999999991414. input_tokens=187, output_tokens=101
21:39:49,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9839999999985594. input_tokens=169, output_tokens=54
21:39:49,86 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,87 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0310000000008586. input_tokens=184, output_tokens=92
21:39:49,118 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0159999999996217. input_tokens=378, output_tokens=143
21:39:49,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.7350000000005821. input_tokens=181, output_tokens=56
21:39:49,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7189999999991414. input_tokens=206, output_tokens=134
21:39:49,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=196, output_tokens=72
21:39:49,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6569999999992433. input_tokens=184, output_tokens=51
21:39:49,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:49,284 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:49,314 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8910000000014406. input_tokens=187, output_tokens=66
21:39:49,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,378 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2810000000008586. input_tokens=218, output_tokens=83
21:39:49,419 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:49,419 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:49,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:49,556 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:49,563 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:49,564 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:49,677 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5619999999998981. input_tokens=163, output_tokens=39
21:39:49,681 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.139999999999418. input_tokens=293, output_tokens=111
21:39:49,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6089999999985594. input_tokens=166, output_tokens=43
21:39:49,809 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.8899999999994179. input_tokens=171, output_tokens=66
21:39:49,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0940000000009604. input_tokens=375, output_tokens=114
21:39:49,869 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:49,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9529999999995198. input_tokens=167, output_tokens=68
21:39:49,990 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:49,991 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:50,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:50,76 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:50,94 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:50,95 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:50,142 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.625. input_tokens=336, output_tokens=174
21:39:50,163 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,163 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.125. input_tokens=184, output_tokens=84
21:39:50,170 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9369999999998981. input_tokens=170, output_tokens=76
21:39:50,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6880000000001019. input_tokens=161, output_tokens=47
21:39:50,498 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0310000000008586. input_tokens=273, output_tokens=104
21:39:50,564 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5940000000009604. input_tokens=175, output_tokens=75
21:39:50,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:50,601 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:50,603 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5470000000004802. input_tokens=248, output_tokens=116
21:39:50,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5939999999991414. input_tokens=230, output_tokens=118
21:39:50,686 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:50,686 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:50,719 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:50,720 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:50,786 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:50,787 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:50,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:50,800 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:50,820 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0160000000014406. input_tokens=203, output_tokens=79
21:39:50,914 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=299, output_tokens=151
21:39:50,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:50,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5630000000001019. input_tokens=169, output_tokens=90
21:39:51,89 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9369999999998981. input_tokens=166, output_tokens=70
21:39:51,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:51,212 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:51,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.7029999999995198. input_tokens=196, output_tokens=58
21:39:51,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,313 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1560000000008586. input_tokens=209, output_tokens=93
21:39:51,537 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,538 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.0630000000001019. input_tokens=186, output_tokens=71
21:39:51,659 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:51,661 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:51,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3590000000003783. input_tokens=197, output_tokens=91
21:39:51,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8279999999995198. input_tokens=159, output_tokens=63
21:39:51,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9530000000013388. input_tokens=676, output_tokens=190
21:39:51,802 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8589999999985594. input_tokens=165, output_tokens=60
21:39:51,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6410000000014406. input_tokens=193, output_tokens=107
21:39:51,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 2 retries took 1.4369999999998981. input_tokens=245, output_tokens=131
21:39:51,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:51,952 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:51,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:51,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1559999999990396. input_tokens=270, output_tokens=149
21:39:52,14 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:52,16 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3440000000009604. input_tokens=238, output_tokens=157
21:39:52,218 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:52,219 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:52,284 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:52,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9689999999991414. input_tokens=211, output_tokens=76
21:39:52,379 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:52,380 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:52,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:52,415 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:52,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:52,416 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:52,451 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:52,451 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:52,541 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:52,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5630000000001019. input_tokens=165, output_tokens=39
21:39:52,545 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:52,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8119999999998981. input_tokens=185, output_tokens=51
21:39:52,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:52,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 2 retries took 2.1090000000003783. input_tokens=206, output_tokens=124
21:39:52,627 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:52,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3590000000003783. input_tokens=189, output_tokens=58
21:39:52,689 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:52,690 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:52,750 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:52,751 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:52,751 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:52,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9530000000013388. input_tokens=204, output_tokens=92
21:39:52,792 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:52,794 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:52,865 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:52,866 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:52,996 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:52,997 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1719999999986612. input_tokens=166, output_tokens=81
21:39:53,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.9069999999992433. input_tokens=175, output_tokens=75
21:39:53,234 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.7189999999991414. input_tokens=233, output_tokens=137
21:39:53,247 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6880000000001019. input_tokens=165, output_tokens=54
21:39:53,293 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 2.2039999999997235. input_tokens=397, output_tokens=200
21:39:53,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.875. input_tokens=196, output_tokens=84
21:39:53,421 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 2 retries took 1.2029999999995198. input_tokens=191, output_tokens=105
21:39:53,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.734999999998763. input_tokens=286, output_tokens=143
21:39:53,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:53,608 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:53,634 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:53,635 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:53,677 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.0. input_tokens=220, output_tokens=90
21:39:53,693 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,694 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9690000000009604. input_tokens=322, output_tokens=207
21:39:53,745 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.75. input_tokens=168, output_tokens=50
21:39:53,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,846 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3119999999998981. input_tokens=161, output_tokens=95
21:39:53,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,875 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.625. input_tokens=165, output_tokens=44
21:39:53,937 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9220000000004802. input_tokens=211, output_tokens=104
21:39:53,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:53,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6710000000002765. input_tokens=161, output_tokens=51
21:39:54,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:54,76 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:54,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:54,86 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:54,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.640000000001237. input_tokens=277, output_tokens=168
21:39:54,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7030000000013388. input_tokens=174, output_tokens=42
21:39:54,181 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9369999999998981. input_tokens=173, output_tokens=55
21:39:54,197 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8280000000013388. input_tokens=171, output_tokens=47
21:39:54,299 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:54,300 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:54,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:54,310 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:54,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2030000000013388. input_tokens=225, output_tokens=113
21:39:54,336 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6399999999994179. input_tokens=169, output_tokens=45
21:39:54,467 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:54,468 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:54,483 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.75. input_tokens=164, output_tokens=52
21:39:54,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.1090000000003783. input_tokens=161, output_tokens=72
21:39:54,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.8119999999998981. input_tokens=187, output_tokens=45
21:39:54,931 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7969999999986612. input_tokens=183, output_tokens=63
21:39:54,944 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.9220000000004802. input_tokens=228, output_tokens=83
21:39:54,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.014999999999418. input_tokens=206, output_tokens=101
21:39:54,987 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:54,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1409999999996217. input_tokens=195, output_tokens=97
21:39:55,6 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3280000000013388. input_tokens=218, output_tokens=84
21:39:55,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:39:55,124 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:39:55,128 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=197, output_tokens=110
21:39:55,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8439999999991414. input_tokens=167, output_tokens=56
21:39:55,256 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,404 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,488 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,771 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.5. input_tokens=163, output_tokens=19
21:39:55,782 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0619999999998981. input_tokens=171, output_tokens=79
21:39:55,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:55,923 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,8 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2810000000008586. input_tokens=187, output_tokens=60
21:39:56,82 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,108 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1719999999986612. input_tokens=168, output_tokens=68
21:39:56,187 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,255 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7970000000004802. input_tokens=184, output_tokens=63
21:39:56,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1409999999996217. input_tokens=197, output_tokens=111
21:39:56,432 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9220000000004802. input_tokens=196, output_tokens=77
21:39:56,781 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:56,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3119999999998981. input_tokens=188, output_tokens=100
21:39:57,69 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:57,72 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.485000000000582. input_tokens=224, output_tokens=170
21:39:57,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.671999999998661. input_tokens=454, output_tokens=206
21:39:57,690 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:57,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:57,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7189999999991414. input_tokens=167, output_tokens=59
21:39:58,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:58,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 3 retries took 1.4539999999997235. input_tokens=265, output_tokens=104
21:39:58,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 2 retries took 1.0470000000004802. input_tokens=206, output_tokens=92
21:39:58,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:58,388 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:58,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8440000000009604. input_tokens=163, output_tokens=68
21:39:58,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:58,648 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:58,649 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7970000000004802. input_tokens=205, output_tokens=123
21:39:58,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:58,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8130000000001019. input_tokens=193, output_tokens=82
21:39:59,177 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:59,466 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:59,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7029999999995198. input_tokens=167, output_tokens=45
21:39:59,661 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:59,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9850000000005821. input_tokens=181, output_tokens=76
21:39:59,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:39:59,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9689999999991414. input_tokens=169, output_tokens=53
21:40:00,49 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:00,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9210000000002765. input_tokens=192, output_tokens=61
21:40:00,315 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:00,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9530000000013388. input_tokens=167, output_tokens=62
21:40:00,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:00,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8589999999985594. input_tokens=171, output_tokens=59
21:40:00,632 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:00,771 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:00,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2969999999986612. input_tokens=177, output_tokens=115
21:40:01,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2809999999990396. input_tokens=193, output_tokens=124
21:40:01,379 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:01,400 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:01,401 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 2 retries took 0.7190000000009604. input_tokens=167, output_tokens=41
21:40:01,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.0. input_tokens=169, output_tokens=71
21:40:01,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1720000000004802. input_tokens=215, output_tokens=96
21:40:01,719 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:01,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 0.6560000000008586. input_tokens=166, output_tokens=48
21:40:01,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:01,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2810000000008586. input_tokens=202, output_tokens=103
21:40:02,26 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:02,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9529999999995198. input_tokens=165, output_tokens=39
21:40:02,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1880000000001019. input_tokens=183, output_tokens=108
21:40:02,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1880000000001019. input_tokens=192, output_tokens=92
21:40:02,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:02,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2809999999990396. input_tokens=175, output_tokens=102
21:40:02,989 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:02,990 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8599999999987631. input_tokens=177, output_tokens=48
21:40:03,17 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:03,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.485000000000582. input_tokens=161, output_tokens=93
21:40:03,195 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:03,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 3 retries took 0.6720000000004802. input_tokens=167, output_tokens=49
21:40:03,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.2659999999996217. input_tokens=206, output_tokens=115
21:40:03,905 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:03,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7189999999991414. input_tokens=164, output_tokens=45
21:40:04,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.375. input_tokens=170, output_tokens=76
21:40:04,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8899999999994179. input_tokens=161, output_tokens=52
21:40:04,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8119999999998981. input_tokens=168, output_tokens=73
21:40:04,701 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 2 retries took 0.7659999999996217. input_tokens=175, output_tokens=53
21:40:04,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:04,882 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:04,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8280000000013388. input_tokens=176, output_tokens=79
21:40:04,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:05,49 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:05,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:05,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=289, output_tokens=150
21:40:05,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 3 retries took 0.7339999999985594. input_tokens=163, output_tokens=54
21:40:05,343 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:05,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6559999999990396. input_tokens=207, output_tokens=152
21:40:05,474 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 3 retries took 1.2809999999990396. input_tokens=168, output_tokens=68
21:40:05,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0310000000008586. input_tokens=164, output_tokens=77
21:40:05,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4689999999991414. input_tokens=412, output_tokens=187
21:40:05,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0779999999995198. input_tokens=183, output_tokens=79
21:40:06,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.125. input_tokens=188, output_tokens=115
21:40:06,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:06,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1409999999996217. input_tokens=184, output_tokens=79
21:40:07,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:07,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 3 retries took 2.2659999999996217. input_tokens=261, output_tokens=120
21:40:07,178 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
21:40:07,487 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
21:40:07,488 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
21:40:07,519 datashaper.workflow.workflow INFO executing verb cluster_graph
21:40:08,196 datashaper.workflow.workflow INFO executing verb select
21:40:08,201 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:40:08,555 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:40:08,555 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:40:08,599 datashaper.workflow.workflow INFO executing verb unpack_graph
21:40:08,843 datashaper.workflow.workflow INFO executing verb rename
21:40:08,853 datashaper.workflow.workflow INFO executing verb select
21:40:08,866 datashaper.workflow.workflow INFO executing verb dedupe
21:40:08,877 datashaper.workflow.workflow INFO executing verb rename
21:40:08,885 datashaper.workflow.workflow INFO executing verb filter
21:40:08,915 datashaper.workflow.workflow INFO executing verb text_split
21:40:08,939 datashaper.workflow.workflow INFO executing verb drop
21:40:08,966 datashaper.workflow.workflow INFO executing verb merge
21:40:09,88 datashaper.workflow.workflow INFO executing verb text_embed
21:40:09,92 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
21:40:09,306 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
21:40:09,306 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
21:40:09,347 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 525 inputs via 525 snippets using 33 batches. max_batch_size=16, max_tokens=8191
21:40:09,870 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:09,900 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:09,919 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:09,935 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:09,937 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:09,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:09,984 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:09,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6560000000008586. input_tokens=816, output_tokens=0
21:40:10,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,43 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,43 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6719999999986612. input_tokens=1003, output_tokens=0
21:40:10,69 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,81 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,81 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7180000000007567. input_tokens=905, output_tokens=0
21:40:10,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.75. input_tokens=1610, output_tokens=0
21:40:10,127 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,151 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7810000000008586. input_tokens=1187, output_tokens=0
21:40:10,178 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,179 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8130000000001019. input_tokens=455, output_tokens=0
21:40:10,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8590000000003783. input_tokens=1703, output_tokens=0
21:40:10,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8599999999987631. input_tokens=857, output_tokens=0
21:40:10,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.875. input_tokens=1098, output_tokens=0
21:40:10,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,284 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9069999999992433. input_tokens=704, output_tokens=0
21:40:10,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9530000000013388. input_tokens=646, output_tokens=0
21:40:10,345 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,346 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0. input_tokens=1232, output_tokens=0
21:40:10,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.015000000001237. input_tokens=663, output_tokens=0
21:40:10,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0159999999996217. input_tokens=937, output_tokens=0
21:40:10,425 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0469999999986612. input_tokens=938, output_tokens=0
21:40:10,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0779999999995198. input_tokens=1204, output_tokens=0
21:40:10,477 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.125. input_tokens=576, output_tokens=0
21:40:10,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1560000000008586. input_tokens=944, output_tokens=0
21:40:10,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1880000000001019. input_tokens=895, output_tokens=0
21:40:10,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1880000000001019. input_tokens=414, output_tokens=0
21:40:10,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2189999999991414. input_tokens=692, output_tokens=0
21:40:10,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2659999999996217. input_tokens=1820, output_tokens=0
21:40:10,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2810000000008586. input_tokens=726, output_tokens=0
21:40:10,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3119999999998981. input_tokens=964, output_tokens=0
21:40:10,703 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6710000000002765. input_tokens=498, output_tokens=0
21:40:10,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3590000000003783. input_tokens=574, output_tokens=0
21:40:10,742 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7969999999986612. input_tokens=488, output_tokens=0
21:40:10,891 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,892 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:10,984 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8130000000001019. input_tokens=453, output_tokens=0
21:40:11,5 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:11,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8909999999996217. input_tokens=377, output_tokens=0
21:40:11,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9369999999998981. input_tokens=476, output_tokens=0
21:40:11,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:11,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8119999999998981. input_tokens=329, output_tokens=0
21:40:11,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9060000000008586. input_tokens=528, output_tokens=0
21:40:11,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:40:11,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2340000000003783. input_tokens=482, output_tokens=0
21:40:11,519 datashaper.workflow.workflow INFO executing verb drop
21:40:11,531 datashaper.workflow.workflow INFO executing verb filter
21:40:11,556 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:40:12,10 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:40:12,12 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:40:12,86 datashaper.workflow.workflow INFO executing verb layout_graph
21:40:13,299 datashaper.workflow.workflow INFO executing verb unpack_graph
21:40:13,921 datashaper.workflow.workflow INFO executing verb unpack_graph
21:40:14,220 datashaper.workflow.workflow INFO executing verb filter
21:40:14,276 datashaper.workflow.workflow INFO executing verb drop
21:40:14,289 datashaper.workflow.workflow INFO executing verb select
21:40:14,302 datashaper.workflow.workflow INFO executing verb rename
21:40:14,311 datashaper.workflow.workflow INFO executing verb join
21:40:14,347 datashaper.workflow.workflow INFO executing verb convert
21:40:14,411 datashaper.workflow.workflow INFO executing verb rename
21:40:14,413 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:40:14,768 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:40:14,769 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:40:14,826 datashaper.workflow.workflow INFO executing verb create_final_communities
21:40:15,724 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:40:16,83 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:40:16,83 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:40:16,101 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:40:16,170 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
21:40:16,477 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
21:40:16,487 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:40:16,843 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
21:40:16,843 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:40:16,849 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:40:16,886 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:40:16,946 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
21:40:17,3 datashaper.workflow.workflow INFO executing verb select
21:40:17,8 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:40:17,348 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:40:17,367 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:40:17,379 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:40:17,433 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
21:40:17,519 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
21:40:17,566 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
21:40:17,606 datashaper.workflow.workflow INFO executing verb prepare_community_reports
21:40:17,607 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=4 => 525
21:40:17,642 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 525
21:40:17,741 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 525
21:40:17,929 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 525
21:40:18,188 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 525
21:40:18,382 datashaper.workflow.workflow INFO executing verb create_community_reports
21:40:24,517 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:24,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.125. input_tokens=2181, output_tokens=617
21:40:27,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:27,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.921000000000276. input_tokens=6914, output_tokens=754
21:40:35,960 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:35,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.514999999999418. input_tokens=2309, output_tokens=692
21:40:36,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:36,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.406999999999243. input_tokens=9840, output_tokens=879
21:40:37,286 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:37,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.860000000000582. input_tokens=7123, output_tokens=839
21:40:37,931 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:37,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.48399999999856. input_tokens=2095, output_tokens=614
21:40:38,791 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:38,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.328999999999724. input_tokens=9876, output_tokens=818
21:40:39,645 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:39,647 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.21900000000096. input_tokens=2633, output_tokens=751
21:40:39,747 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:39,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.311999999999898. input_tokens=7328, output_tokens=946
21:40:39,841 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:39,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.406000000000859. input_tokens=2461, output_tokens=791
21:40:40,300 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:40,301 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:40,303 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:40,303 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:40,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:40,305 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:40,317 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:40,317 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:40,321 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:40,321 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:40,344 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:40,345 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:40,354 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:40,354 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:40,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:40,378 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:40,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:40,584 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:41,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:41,702 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:42,227 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:42,227 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:42,287 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:42,288 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:43,330 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:43,331 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:43,458 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:43,459 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:45,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:40:45,328 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:40:45,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:45,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.688000000000102. input_tokens=2150, output_tokens=594
21:40:45,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:45,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.015999999999622. input_tokens=2284, output_tokens=685
21:40:46,3 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:46,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.061999999999898. input_tokens=2416, output_tokens=680
21:40:46,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:46,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.093999999999141. input_tokens=2179, output_tokens=641
21:40:46,122 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:46,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.156999999999243. input_tokens=2198, output_tokens=587
21:40:46,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:46,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.57799999999952. input_tokens=2267, output_tokens=602
21:40:47,507 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:47,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.578000000001339. input_tokens=2822, output_tokens=799
21:40:47,937 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:47,939 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.938000000000102. input_tokens=2327, output_tokens=754
21:40:47,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:47,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.014999999999418. input_tokens=3447, output_tokens=871
21:40:48,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:48,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.234000000000378. input_tokens=3600, output_tokens=780
21:40:49,579 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:49,581 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 7.5460000000002765. input_tokens=3565, output_tokens=760
21:40:49,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:49,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.609000000000378. input_tokens=7605, output_tokens=980
21:40:50,926 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:50,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.968999999999141. input_tokens=4683, output_tokens=848
21:40:51,213 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:51,214 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:51,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.186999999999898. input_tokens=2790, output_tokens=836
21:40:51,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:51,451 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:51,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.5. input_tokens=3666, output_tokens=845
21:40:51,716 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.234000000000378. input_tokens=7127, output_tokens=879
21:40:52,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 7.04700000000048. input_tokens=2695, output_tokens=722
21:40:55,488 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:55,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 7.984999999998763. input_tokens=3330, output_tokens=785
21:40:57,965 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:40:57,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 8.671000000000276. input_tokens=3439, output_tokens=840
21:41:01,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:01,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 10.843000000000757. input_tokens=3204, output_tokens=901
21:41:02,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:02,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 8.015999999999622. input_tokens=3168, output_tokens=776
21:41:04,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:04,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 10.765999999999622. input_tokens=3133, output_tokens=794
21:41:04,734 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:04,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 8.859999999998763. input_tokens=4516, output_tokens=846
21:41:09,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:09,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 11.54700000000048. input_tokens=7656, output_tokens=1009
21:41:15,333 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:15,334 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.70299999999952. input_tokens=2065, output_tokens=557
21:41:15,883 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:15,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.578000000001339. input_tokens=2136, output_tokens=553
21:41:16,536 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:16,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.5789999999997235. input_tokens=2116, output_tokens=551
21:41:17,685 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:17,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.061999999999898. input_tokens=2173, output_tokens=577
21:41:17,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:17,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.15599999999904. input_tokens=2834, output_tokens=798
21:41:17,802 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:17,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.15599999999904. input_tokens=3283, output_tokens=769
21:41:17,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:18,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.218999999999141. input_tokens=3240, output_tokens=761
21:41:18,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:18,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.625. input_tokens=2102, output_tokens=469
21:41:18,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:18,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.938000000000102. input_tokens=3064, output_tokens=823
21:41:18,830 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:18,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.186999999999898. input_tokens=4918, output_tokens=888
21:41:19,265 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:19,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.640999999999622. input_tokens=3315, output_tokens=882
21:41:19,913 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:19,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.593999999999141. input_tokens=2217, output_tokens=725
21:41:20,767 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:20,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.17200000000048. input_tokens=2328, output_tokens=597
21:41:21,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:21,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.735000000000582. input_tokens=2807, output_tokens=847
21:41:22,37 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:22,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.218999999999141. input_tokens=2551, output_tokens=784
21:41:23,469 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:23,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.186999999999898. input_tokens=2414, output_tokens=683
21:41:23,711 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:23,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:23,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:23,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.609999999998763. input_tokens=2373, output_tokens=689
21:41:24,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.156000000000859. input_tokens=2412, output_tokens=775
21:41:24,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.20299999999952. input_tokens=9683, output_tokens=1047
21:41:27,569 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:27,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.92200000000048. input_tokens=2251, output_tokens=690
21:41:28,319 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:28,320 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.656000000000859. input_tokens=2446, output_tokens=682
21:41:28,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:28,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.139999999999418. input_tokens=2122, output_tokens=642
21:41:30,945 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:30,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.21900000000096. input_tokens=2692, output_tokens=749
21:41:31,920 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:31,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.264999999999418. input_tokens=2342, output_tokens=709
21:41:32,382 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:32,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.265000000001237. input_tokens=2125, output_tokens=623
21:41:33,163 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:33,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.375. input_tokens=2470, output_tokens=699
21:41:34,790 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:34,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.281999999999243. input_tokens=2212, output_tokens=725
21:41:36,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:36,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.328000000001339. input_tokens=2796, output_tokens=894
21:41:36,632 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:36,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.109000000000378. input_tokens=2355, output_tokens=715
21:41:37,111 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:37,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.921999999998661. input_tokens=2834, output_tokens=686
21:41:38,119 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:38,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.718999999999141. input_tokens=2566, output_tokens=857
21:41:39,418 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:39,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.890999999999622. input_tokens=2644, output_tokens=721
21:41:41,315 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:41,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.96900000000096. input_tokens=2415, output_tokens=730
21:41:41,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:41,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.063000000000102. input_tokens=3061, output_tokens=931
21:41:41,564 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:41,566 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.938000000000102. input_tokens=2927, output_tokens=819
21:41:42,47 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:42,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.70299999999952. input_tokens=2580, output_tokens=692
21:41:44,229 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:44,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.95299999999952. input_tokens=3143, output_tokens=853
21:41:46,261 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:46,300 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:46,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.859000000000378. input_tokens=3282, output_tokens=745
21:41:46,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.23399999999856. input_tokens=3387, output_tokens=908
21:41:47,25 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:47,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.688000000000102. input_tokens=2892, output_tokens=737
21:41:51,551 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:51,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.671999999998661. input_tokens=3130, output_tokens=857
21:41:51,601 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:51,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.25. input_tokens=3859, output_tokens=854
21:41:53,931 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:53,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.093999999999141. input_tokens=4149, output_tokens=828
21:41:54,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:54,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.186999999999898. input_tokens=4105, output_tokens=888
21:41:56,744 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:56,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.046999999998661. input_tokens=3808, output_tokens=888
21:41:58,99 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:58,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.29700000000048. input_tokens=4987, output_tokens=784
21:41:59,990 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:41:59,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.671999999998661. input_tokens=6255, output_tokens=839
21:42:03,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:03,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.311999999999898. input_tokens=6501, output_tokens=896
21:42:05,716 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:05,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.609000000000378. input_tokens=8317, output_tokens=865
21:42:12,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:12,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.563000000000102. input_tokens=9782, output_tokens=867
21:42:17,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:17,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.234000000000378. input_tokens=2074, output_tokens=443
21:42:17,972 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:17,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.343999999999141. input_tokens=2060, output_tokens=457
21:42:18,769 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:18,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.156000000000859. input_tokens=2084, output_tokens=561
21:42:20,126 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:20,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.531000000000859. input_tokens=2635, output_tokens=740
21:42:20,436 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:20,438 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.813000000000102. input_tokens=3173, output_tokens=742
21:42:20,796 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:20,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.186999999999898. input_tokens=3314, output_tokens=713
21:42:22,213 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:22,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.593000000000757. input_tokens=4519, output_tokens=834
21:42:22,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:22,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.656000000000859. input_tokens=7572, output_tokens=941
21:42:22,601 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:22,610 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.0. input_tokens=6271, output_tokens=881
21:42:23,474 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:23,476 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.843999999999141. input_tokens=3247, output_tokens=881
21:42:23,809 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:23,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.45299999999952. input_tokens=5637, output_tokens=864
21:42:25,469 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:25,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.40599999999904. input_tokens=7359, output_tokens=892
21:42:28,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:28,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.82799999999952. input_tokens=7380, output_tokens=870
21:42:32,87 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:32,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.656000000000859. input_tokens=8934, output_tokens=831
21:42:36,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:42:36,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.703000000001339. input_tokens=9731, output_tokens=928
21:42:36,860 datashaper.workflow.workflow INFO executing verb window
21:42:36,863 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:42:37,130 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
21:42:37,131 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:42:37,175 datashaper.workflow.workflow INFO executing verb unroll
21:42:37,190 datashaper.workflow.workflow INFO executing verb select
21:42:37,205 datashaper.workflow.workflow INFO executing verb rename
21:42:37,224 datashaper.workflow.workflow INFO executing verb join
21:42:37,242 datashaper.workflow.workflow INFO executing verb aggregate_override
21:42:37,261 datashaper.workflow.workflow INFO executing verb join
21:42:37,279 datashaper.workflow.workflow INFO executing verb rename
21:42:37,297 datashaper.workflow.workflow INFO executing verb convert
21:42:37,331 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
21:42:37,556 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
21:42:37,557 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
21:42:37,605 datashaper.workflow.workflow INFO executing verb rename
21:42:37,608 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:42:37,683 graphrag.index.cli INFO All workflows completed successfully.
21:14:44,18 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:14:44,26 graphrag.index.cli INFO Starting pipeline run for: 20241201-211443, dryrun=False
21:14:44,27 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:14:44,52 graphrag.index.create_pipeline_config INFO skipping workflows 
21:14:44,53 graphrag.index.run.run INFO Running pipeline
21:14:44,53 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:14:44,54 graphrag.index.input.load_input INFO loading input from root_dir=input
21:14:44,54 graphrag.index.input.load_input INFO using file storage for input
21:14:44,56 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:14:44,57 graphrag.index.input.text INFO found text files from input, found [('1 - Copy (2).txt', {}), ('1 - Copy (3).txt', {}), ('1 - Copy (4).txt', {}), ('1 - Copy.txt', {}), ('1.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:14:44,90 graphrag.index.input.text INFO Found 6 files, loading 6
21:14:44,91 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:14:44,91 graphrag.index.run.run INFO Final # of rows loaded: 6
21:14:44,238 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:14:44,245 datashaper.workflow.workflow INFO executing verb orderby
21:14:44,250 datashaper.workflow.workflow INFO executing verb zip
21:14:44,255 datashaper.workflow.workflow INFO executing verb aggregate_override
21:14:44,262 datashaper.workflow.workflow INFO executing verb chunk
21:14:44,678 datashaper.workflow.workflow INFO executing verb select
21:14:44,685 datashaper.workflow.workflow INFO executing verb unroll
21:14:44,693 datashaper.workflow.workflow INFO executing verb rename
21:14:44,701 datashaper.workflow.workflow INFO executing verb genid
21:14:44,718 datashaper.workflow.workflow INFO executing verb unzip
21:14:44,726 datashaper.workflow.workflow INFO executing verb copy
21:14:44,734 datashaper.workflow.workflow INFO executing verb filter
21:14:44,753 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:14:44,967 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
21:14:44,967 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:14:45,7 datashaper.workflow.workflow INFO executing verb entity_extract
21:14:45,35 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
21:14:45,489 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
21:14:45,489 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
21:14:49,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:49,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.125. input_tokens=34, output_tokens=192
21:14:49,870 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:49,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.8280000000086147. input_tokens=34, output_tokens=267
21:14:50,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:50,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.452999999979511. input_tokens=34, output_tokens=306
21:14:50,282 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:50,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.0310000000172295. input_tokens=2101, output_tokens=274
21:14:50,430 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:50,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.342999999993481. input_tokens=34, output_tokens=316
21:14:50,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:50,598 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.485000000015134. input_tokens=34, output_tokens=299
21:14:51,270 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:51,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.5789999999979045. input_tokens=34, output_tokens=374
21:14:51,403 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:51,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.312000000005355. input_tokens=34, output_tokens=425
21:14:51,797 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:51,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.452999999979511. input_tokens=34, output_tokens=367
21:14:52,335 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:52,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.26600000000326. input_tokens=34, output_tokens=447
21:14:52,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:52,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.328000000008615. input_tokens=34, output_tokens=506
21:14:52,662 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:52,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.952999999979511. input_tokens=34, output_tokens=552
21:14:53,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:53,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.64100000000326. input_tokens=34, output_tokens=551
21:14:54,318 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:54,320 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.046999999991385. input_tokens=34, output_tokens=364
21:14:58,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:14:58,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.875. input_tokens=2937, output_tokens=993
21:15:05,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:05,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.0. input_tokens=34, output_tokens=474
21:15:05,437 datashaper.workflow.workflow INFO executing verb merge_graphs
21:15:05,593 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
21:15:05,773 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
21:15:05,774 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
21:15:05,811 datashaper.workflow.workflow INFO executing verb summarize_descriptions
21:15:07,88 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.125. input_tokens=155, output_tokens=32
21:15:07,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3280000000086147. input_tokens=192, output_tokens=71
21:15:07,360 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2969999999913853. input_tokens=190, output_tokens=78
21:15:07,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.375. input_tokens=177, output_tokens=86
21:15:07,563 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5150000000139698. input_tokens=181, output_tokens=87
21:15:07,623 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5780000000086147. input_tokens=237, output_tokens=82
21:15:07,778 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=195, output_tokens=101
21:15:07,821 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.875. input_tokens=236, output_tokens=131
21:15:07,833 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.875. input_tokens=236, output_tokens=108
21:15:07,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,849 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8280000000086147. input_tokens=189, output_tokens=103
21:15:07,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:07,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9690000000118744. input_tokens=304, output_tokens=135
21:15:08,59 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9059999999881256. input_tokens=299, output_tokens=132
21:15:08,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0940000000118744. input_tokens=259, output_tokens=140
21:15:08,181 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.187000000005355. input_tokens=192, output_tokens=83
21:15:08,282 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2190000000118744. input_tokens=325, output_tokens=170
21:15:08,348 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=214, output_tokens=89
21:15:08,475 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.452999999979511. input_tokens=376, output_tokens=190
21:15:08,552 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9370000000053551. input_tokens=179, output_tokens=59
21:15:08,591 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.625. input_tokens=456, output_tokens=170
21:15:08,611 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0469999999913853. input_tokens=167, output_tokens=70
21:15:08,683 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7030000000086147. input_tokens=344, output_tokens=189
21:15:08,689 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8589999999967404. input_tokens=176, output_tokens=46
21:15:08,738 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.187000000005355. input_tokens=225, output_tokens=89
21:15:08,963 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:08,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.687999999994645. input_tokens=278, output_tokens=129
21:15:09,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0160000000032596. input_tokens=606, output_tokens=246
21:15:09,182 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=170, output_tokens=57
21:15:09,244 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8910000000032596. input_tokens=557, output_tokens=143
21:15:09,290 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.452999999979511. input_tokens=347, output_tokens=117
21:15:09,317 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.312000000005355. input_tokens=1150, output_tokens=234
21:15:09,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1559999999881256. input_tokens=221, output_tokens=78
21:15:09,451 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4530000000086147. input_tokens=702, output_tokens=286
21:15:09,458 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.985000000015134. input_tokens=162, output_tokens=51
21:15:09,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1570000000065193. input_tokens=201, output_tokens=81
21:15:09,512 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,514 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5469999999913853. input_tokens=941, output_tokens=291
21:15:09,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4690000000118744. input_tokens=262, output_tokens=111
21:15:09,744 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7339999999967404. input_tokens=1180, output_tokens=283
21:15:09,764 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.73499999998603. input_tokens=507, output_tokens=288
21:15:09,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.827999999979511. input_tokens=295, output_tokens=160
21:15:09,895 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=327, output_tokens=129
21:15:09,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:09,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=183, output_tokens=44
21:15:10,99 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.264999999984866. input_tokens=523, output_tokens=206
21:15:10,183 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.203000000008615. input_tokens=1433, output_tokens=380
21:15:10,206 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5160000000032596. input_tokens=488, output_tokens=113
21:15:10,393 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,394 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5940000000118744. input_tokens=160, output_tokens=20
21:15:10,467 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.860000000015134. input_tokens=320, output_tokens=135
21:15:10,526 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7810000000172295. input_tokens=286, output_tokens=104
21:15:10,546 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0939999999827705. input_tokens=169, output_tokens=60
21:15:10,570 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6089999999967404. input_tokens=249, output_tokens=90
21:15:10,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1400000000139698. input_tokens=172, output_tokens=72
21:15:10,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,703 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2339999999967404. input_tokens=236, output_tokens=83
21:15:10,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.437000000005355. input_tokens=170, output_tokens=62
21:15:10,730 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,732 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2179999999934807. input_tokens=196, output_tokens=83
21:15:10,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0310000000172295. input_tokens=167, output_tokens=72
21:15:10,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.562000000005355. input_tokens=174, output_tokens=76
21:15:10,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3439999999827705. input_tokens=257, output_tokens=98
21:15:10,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:10,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=219, output_tokens=91
21:15:11,2 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9839999999967404. input_tokens=386, output_tokens=137
21:15:11,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4690000000118744. input_tokens=490, output_tokens=230
21:15:11,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4679999999934807. input_tokens=679, output_tokens=219
21:15:11,77 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9690000000118744. input_tokens=171, output_tokens=61
21:15:11,243 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.312999999994645. input_tokens=219, output_tokens=92
21:15:11,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5460000000020955. input_tokens=274, output_tokens=126
21:15:11,457 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.875. input_tokens=167, output_tokens=52
21:15:11,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1719999999913853. input_tokens=215, output_tokens=126
21:15:11,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3439999999827705. input_tokens=208, output_tokens=107
21:15:11,588 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9379999999946449. input_tokens=165, output_tokens=55
21:15:11,623 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.437999999994645. input_tokens=246, output_tokens=118
21:15:11,633 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1099999999860302. input_tokens=204, output_tokens=80
21:15:11,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.312000000005355. input_tokens=238, output_tokens=105
21:15:11,816 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3910000000032596. input_tokens=379, output_tokens=230
21:15:11,841 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.062999999994645. input_tokens=1051, output_tokens=333
21:15:11,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.297000000020489. input_tokens=187, output_tokens=93
21:15:11,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.125. input_tokens=141, output_tokens=17
21:15:11,873 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,875 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7969999999913853. input_tokens=187, output_tokens=47
21:15:11,945 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1410000000032596. input_tokens=282, output_tokens=90
21:15:11,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:11,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9219999999913853. input_tokens=163, output_tokens=44
21:15:12,1 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.062999999994645. input_tokens=173, output_tokens=79
21:15:12,9 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2349999999860302. input_tokens=186, output_tokens=65
21:15:12,21 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1560000000172295. input_tokens=166, output_tokens=89
21:15:12,298 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5939999999827705. input_tokens=360, output_tokens=131
21:15:12,401 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9060000000172295. input_tokens=181, output_tokens=55
21:15:12,408 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,409 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.672000000020489. input_tokens=290, output_tokens=152
21:15:12,456 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3910000000032596. input_tokens=253, output_tokens=103
21:15:12,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2969999999913853. input_tokens=184, output_tokens=83
21:15:12,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7339999999967404. input_tokens=302, output_tokens=129
21:15:12,755 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.875. input_tokens=459, output_tokens=274
21:15:12,775 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9370000000053551. input_tokens=180, output_tokens=48
21:15:12,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4219999999913853. input_tokens=206, output_tokens=109
21:15:12,891 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9379999999946449. input_tokens=166, output_tokens=61
21:15:12,920 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6099999999860302. input_tokens=261, output_tokens=125
21:15:12,946 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:12,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4060000000172295. input_tokens=211, output_tokens=119
21:15:13,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3439999999827705. input_tokens=210, output_tokens=100
21:15:13,176 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3280000000086147. input_tokens=254, output_tokens=84
21:15:13,193 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6089999999967404. input_tokens=276, output_tokens=126
21:15:13,239 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3910000000032596. input_tokens=198, output_tokens=94
21:15:13,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,281 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4530000000086147. input_tokens=220, output_tokens=101
21:15:13,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4059999999881256. input_tokens=196, output_tokens=96
21:15:13,456 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=187, output_tokens=69
21:15:13,673 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2659999999741558. input_tokens=205, output_tokens=99
21:15:13,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8120000000053551. input_tokens=164, output_tokens=40
21:15:13,773 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8280000000086147. input_tokens=164, output_tokens=38
21:15:13,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7969999999913853. input_tokens=244, output_tokens=99
21:15:13,796 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=168, output_tokens=89
21:15:13,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.389999999984866. input_tokens=785, output_tokens=269
21:15:13,916 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:13,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9689999999827705. input_tokens=359, output_tokens=137
21:15:14,4 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3589999999967404. input_tokens=369, output_tokens=222
21:15:14,145 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2660000000032596. input_tokens=479, output_tokens=185
21:15:14,170 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,173 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4059999999881256. input_tokens=246, output_tokens=120
21:15:14,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.812999999994645. input_tokens=236, output_tokens=108
21:15:14,359 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3429999999934807. input_tokens=237, output_tokens=131
21:15:14,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7189999999827705. input_tokens=362, output_tokens=156
21:15:14,541 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.25. input_tokens=170, output_tokens=120
21:15:14,667 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4219999999913853. input_tokens=375, output_tokens=114
21:15:14,685 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4059999999881256. input_tokens=193, output_tokens=116
21:15:14,791 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1719999999913853. input_tokens=280, output_tokens=204
21:15:14,870 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0939999999827705. input_tokens=183, output_tokens=50
21:15:14,911 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,913 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=220, output_tokens=96
21:15:14,946 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:14,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=212, output_tokens=106
21:15:15,12 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2190000000118744. input_tokens=223, output_tokens=101
21:15:15,117 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3280000000086147. input_tokens=183, output_tokens=64
21:15:15,178 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.14000000001397. input_tokens=229, output_tokens=135
21:15:15,186 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0150000000139698. input_tokens=229, output_tokens=71
21:15:15,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2030000000086147. input_tokens=165, output_tokens=55
21:15:15,253 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3440000000118744. input_tokens=725, output_tokens=238
21:15:15,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.812000000005355. input_tokens=177, output_tokens=106
21:15:15,464 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.610000000015134. input_tokens=180, output_tokens=84
21:15:15,584 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.860000000015134. input_tokens=266, output_tokens=179
21:15:15,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2039999999979045. input_tokens=470, output_tokens=234
21:15:15,679 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4679999999934807. input_tokens=228, output_tokens=116
21:15:15,709 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2190000000118744. input_tokens=189, output_tokens=70
21:15:15,752 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8280000000086147. input_tokens=158, output_tokens=39
21:15:15,758 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8910000000032596. input_tokens=172, output_tokens=60
21:15:15,795 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1099999999860302. input_tokens=187, output_tokens=57
21:15:15,892 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=209, output_tokens=104
21:15:15,986 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9679999999934807. input_tokens=163, output_tokens=45
21:15:16,3 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.812999999994645. input_tokens=208, output_tokens=124
21:15:16,9 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1410000000032596. input_tokens=449, output_tokens=199
21:15:16,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.735000000015134. input_tokens=197, output_tokens=92
21:15:16,91 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.547000000020489. input_tokens=181, output_tokens=81
21:15:16,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4689999999827705. input_tokens=248, output_tokens=117
21:15:16,432 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,434 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1559999999881256. input_tokens=219, output_tokens=78
21:15:16,464 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.547000000020489. input_tokens=269, output_tokens=139
21:15:16,484 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,485 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.812000000005355. input_tokens=219, output_tokens=108
21:15:16,618 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6089999999967404. input_tokens=164, output_tokens=22
21:15:16,652 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9370000000053551. input_tokens=172, output_tokens=51
21:15:16,670 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4059999999881256. input_tokens=189, output_tokens=75
21:15:16,743 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.562999999994645. input_tokens=214, output_tokens=88
21:15:16,816 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8120000000053551. input_tokens=156, output_tokens=35
21:15:16,878 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,879 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=229, output_tokens=93
21:15:16,893 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2969999999913853. input_tokens=186, output_tokens=65
21:15:16,920 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7349999999860302. input_tokens=210, output_tokens=102
21:15:16,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:16,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0. input_tokens=190, output_tokens=49
21:15:17,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.812999999994645. input_tokens=223, output_tokens=156
21:15:17,26 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2810000000172295. input_tokens=293, output_tokens=104
21:15:17,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0469999999913853. input_tokens=248, output_tokens=73
21:15:17,239 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.764999999984866. input_tokens=243, output_tokens=152
21:15:17,304 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8280000000086147. input_tokens=187, output_tokens=53
21:15:17,386 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,388 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=218, output_tokens=91
21:15:17,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8440000000118744. input_tokens=313, output_tokens=160
21:15:17,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=465, output_tokens=153
21:15:17,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9530000000086147. input_tokens=168, output_tokens=47
21:15:17,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=180, output_tokens=102
21:15:17,691 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2650000000139698. input_tokens=190, output_tokens=61
21:15:17,786 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0469999999913853. input_tokens=166, output_tokens=76
21:15:17,815 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7179999999934807. input_tokens=191, output_tokens=104
21:15:17,835 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.187999999994645. input_tokens=196, output_tokens=70
21:15:17,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,888 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4219999999913853. input_tokens=195, output_tokens=91
21:15:17,901 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,905 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2340000000258442. input_tokens=170, output_tokens=75
21:15:17,926 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,927 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9370000000053551. input_tokens=189, output_tokens=64
21:15:17,931 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.125. input_tokens=171, output_tokens=72
21:15:17,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.14000000001397. input_tokens=363, output_tokens=191
21:15:17,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5160000000032596. input_tokens=176, output_tokens=60
21:15:17,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.187000000005355. input_tokens=225, output_tokens=126
21:15:17,953 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:17,956 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.25. input_tokens=337, output_tokens=243
21:15:18,3 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:18,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0780000000086147. input_tokens=169, output_tokens=51
21:15:18,92 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:18,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0780000000086147. input_tokens=179, output_tokens=72
21:15:18,122 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:18,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2339999999967404. input_tokens=194, output_tokens=96
21:15:18,449 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:18,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5780000000086147. input_tokens=172, output_tokens=111
21:15:18,967 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:18,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.937999999994645. input_tokens=215, output_tokens=116
21:15:19,40 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
21:15:19,244 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
21:15:19,244 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
21:15:19,277 datashaper.workflow.workflow INFO executing verb cluster_graph
21:15:19,862 datashaper.workflow.workflow INFO executing verb select
21:15:19,867 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:15:20,89 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:15:20,89 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:15:20,134 datashaper.workflow.workflow INFO executing verb unpack_graph
21:15:20,404 datashaper.workflow.workflow INFO executing verb rename
21:15:20,420 datashaper.workflow.workflow INFO executing verb select
21:15:20,434 datashaper.workflow.workflow INFO executing verb dedupe
21:15:20,448 datashaper.workflow.workflow INFO executing verb rename
21:15:20,463 datashaper.workflow.workflow INFO executing verb filter
21:15:20,503 datashaper.workflow.workflow INFO executing verb text_split
21:15:20,527 datashaper.workflow.workflow INFO executing verb drop
21:15:20,541 datashaper.workflow.workflow INFO executing verb merge
21:15:20,683 datashaper.workflow.workflow INFO executing verb text_embed
21:15:20,685 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
21:15:21,202 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
21:15:21,202 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
21:15:21,259 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
21:15:21,657 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.375. input_tokens=417, output_tokens=0
21:15:21,723 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5. input_tokens=1926, output_tokens=0
21:15:21,811 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,813 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,823 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,832 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5629999999946449. input_tokens=877, output_tokens=0
21:15:21,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5780000000086147. input_tokens=851, output_tokens=0
21:15:21,902 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,904 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,937 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6719999999913853. input_tokens=617, output_tokens=0
21:15:21,981 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:21,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6719999999913853. input_tokens=801, output_tokens=0
21:15:22,23 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,25 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,27 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.75. input_tokens=958, output_tokens=0
21:15:22,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8279999999795109. input_tokens=603, output_tokens=0
21:15:22,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8440000000118744. input_tokens=667, output_tokens=0
21:15:22,171 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,172 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9060000000172295. input_tokens=1081, output_tokens=0
21:15:22,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9219999999913853. input_tokens=717, output_tokens=0
21:15:22,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9849999999860302. input_tokens=493, output_tokens=0
21:15:22,295 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,296 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0309999999881256. input_tokens=697, output_tokens=0
21:15:22,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0310000000172295. input_tokens=817, output_tokens=0
21:15:22,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.077999999979511. input_tokens=1018, output_tokens=0
21:15:22,400 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,401 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:15:22,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1099999999860302. input_tokens=626, output_tokens=0
21:15:22,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1560000000172295. input_tokens=1283, output_tokens=0
21:15:22,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7809999999881256. input_tokens=134, output_tokens=0
21:15:22,539 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2349999999860302. input_tokens=870, output_tokens=0
21:15:22,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.312000000005355. input_tokens=1907, output_tokens=0
21:15:22,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2969999999913853. input_tokens=596, output_tokens=0
21:15:22,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3590000000258442. input_tokens=616, output_tokens=0
21:15:22,684 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3900000000139698. input_tokens=983, output_tokens=0
21:15:22,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.422000000020489. input_tokens=1378, output_tokens=0
21:15:22,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.437999999994645. input_tokens=702, output_tokens=0
21:15:22,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5159999999741558. input_tokens=1444, output_tokens=0
21:15:22,894 datashaper.workflow.workflow INFO executing verb drop
21:15:22,910 datashaper.workflow.workflow INFO executing verb filter
21:15:22,938 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:15:23,222 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:15:23,222 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:15:23,264 datashaper.workflow.workflow INFO executing verb layout_graph
21:15:24,316 datashaper.workflow.workflow INFO executing verb unpack_graph
21:15:24,628 datashaper.workflow.workflow INFO executing verb unpack_graph
21:15:24,957 datashaper.workflow.workflow INFO executing verb filter
21:15:25,17 datashaper.workflow.workflow INFO executing verb drop
21:15:25,35 datashaper.workflow.workflow INFO executing verb select
21:15:25,53 datashaper.workflow.workflow INFO executing verb rename
21:15:25,70 datashaper.workflow.workflow INFO executing verb convert
21:15:25,129 datashaper.workflow.workflow INFO executing verb join
21:15:25,158 datashaper.workflow.workflow INFO executing verb rename
21:15:25,161 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:15:25,372 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:15:25,373 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:15:25,421 datashaper.workflow.workflow INFO executing verb create_final_communities
21:15:26,37 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:15:26,265 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:15:26,266 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:15:26,279 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:15:26,344 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
21:15:26,649 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
21:15:26,662 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:15:26,894 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
21:15:26,894 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:15:26,902 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:15:26,918 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:15:27,27 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
21:15:27,75 datashaper.workflow.workflow INFO executing verb select
21:15:27,78 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:15:27,291 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:15:27,291 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:15:27,298 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:15:27,355 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
21:15:27,402 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
21:15:27,446 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
21:15:27,482 datashaper.workflow.workflow INFO executing verb prepare_community_reports
21:15:27,483 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 405
21:15:27,521 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
21:15:27,707 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
21:15:27,978 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
21:15:28,254 datashaper.workflow.workflow INFO executing verb create_community_reports
21:15:41,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:41,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.030999999988126. input_tokens=3806, output_tokens=874
21:15:42,832 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:42,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.578000000008615. input_tokens=7231, output_tokens=861
21:15:51,310 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:51,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.078000000008615. input_tokens=2116, output_tokens=530
21:15:52,287 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:52,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.0. input_tokens=2150, output_tokens=605
21:15:54,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:54,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.077999999979511. input_tokens=2368, output_tokens=654
21:15:54,388 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:54,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.10999999998603. input_tokens=2412, output_tokens=764
21:15:54,424 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:54,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.25. input_tokens=2470, output_tokens=771
21:15:54,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:54,680 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.437000000005355. input_tokens=2652, output_tokens=800
21:15:54,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:54,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.5. input_tokens=3011, output_tokens=779
21:15:54,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:54,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.671999999991385. input_tokens=2660, output_tokens=814
21:15:54,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:54,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.546999999991385. input_tokens=4976, output_tokens=813
21:15:55,325 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:55,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.094000000011874. input_tokens=7230, output_tokens=727
21:15:56,390 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:56,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.171999999991385. input_tokens=3923, output_tokens=797
21:15:56,425 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:56,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.25. input_tokens=2636, output_tokens=801
21:15:56,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:56,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.23399999999674. input_tokens=2254, output_tokens=644
21:15:56,967 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:56,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.64100000000326. input_tokens=2390, output_tokens=683
21:15:57,198 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:57,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.89100000000326. input_tokens=8811, output_tokens=904
21:15:57,259 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:57,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.921999999991385. input_tokens=4970, output_tokens=961
21:15:57,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:57,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.077999999979511. input_tokens=4368, output_tokens=857
21:15:57,769 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:57,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.562999999994645. input_tokens=4276, output_tokens=893
21:15:57,903 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:57,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.719000000011874. input_tokens=3692, output_tokens=891
21:15:58,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:15:58,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.735000000015134. input_tokens=9873, output_tokens=914
21:16:02,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:02,647 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.375. input_tokens=5908, output_tokens=1001
21:16:12,53 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:12,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.39000000001397. input_tokens=2079, output_tokens=594
21:16:14,185 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:14,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.5. input_tokens=3541, output_tokens=763
21:16:14,276 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:14,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.51500000001397. input_tokens=3617, output_tokens=751
21:16:14,440 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:14,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.719000000011874. input_tokens=3307, output_tokens=745
21:16:14,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:14,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.875. input_tokens=2148, output_tokens=664
21:16:14,747 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:14,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.078000000008615. input_tokens=2277, output_tokens=677
21:16:15,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:15,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.578000000008615. input_tokens=4890, output_tokens=795
21:16:15,207 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:15,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.469000000011874. input_tokens=3767, output_tokens=788
21:16:15,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:15,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.60899999999674. input_tokens=3950, output_tokens=814
21:16:15,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:15,378 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.687999999994645. input_tokens=3268, output_tokens=808
21:16:15,553 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:15,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.937000000005355. input_tokens=2448, output_tokens=803
21:16:16,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:16,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.046999999991385. input_tokens=2692, output_tokens=758
21:16:17,467 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:17,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.860000000015134. input_tokens=8052, output_tokens=964
21:16:17,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:17,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.046999999991385. input_tokens=6306, output_tokens=966
21:16:17,697 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:17,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.046999999991385. input_tokens=2217, output_tokens=840
21:16:17,841 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:17,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.125. input_tokens=3342, output_tokens=827
21:16:18,965 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:18,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.187999999994645. input_tokens=4622, output_tokens=870
21:16:20,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:20,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.562000000005355. input_tokens=3075, output_tokens=907
21:16:20,709 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:20,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.04700000002049. input_tokens=2102, output_tokens=531
21:16:21,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:21,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.46799999999348. input_tokens=2225, output_tokens=560
21:16:21,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:21,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.889999999984866. input_tokens=3908, output_tokens=760
21:16:25,137 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:25,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.48499999998603. input_tokens=5350, output_tokens=813
21:16:25,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:25,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.219000000011874. input_tokens=5736, output_tokens=914
21:16:25,550 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:25,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.85899999999674. input_tokens=4102, output_tokens=839
21:16:26,470 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:26,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.844000000011874. input_tokens=7643, output_tokens=874
21:16:26,873 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:26,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.687999999994645. input_tokens=5930, output_tokens=904
21:16:27,119 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:27,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.375. input_tokens=4779, output_tokens=895
21:16:27,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:27,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.562999999994645. input_tokens=9820, output_tokens=958
21:16:35,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:35,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.187000000005355. input_tokens=2060, output_tokens=460
21:16:36,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:36,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.64100000000326. input_tokens=2340, output_tokens=595
21:16:37,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:37,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.797000000020489. input_tokens=2854, output_tokens=777
21:16:39,63 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:39,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.828000000008615. input_tokens=6587, output_tokens=920
21:16:39,91 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:39,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.953000000008615. input_tokens=5094, output_tokens=779
21:16:39,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:39,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.03200000000652. input_tokens=3309, output_tokens=801
21:16:39,704 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:39,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.5. input_tokens=9788, output_tokens=820
21:16:40,171 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:40,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.046999999991385. input_tokens=2971, output_tokens=761
21:16:41,152 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:41,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.937000000005355. input_tokens=7267, output_tokens=944
21:16:42,236 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:42,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.077999999979511. input_tokens=9655, output_tokens=942
21:16:44,748 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:16:44,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.5. input_tokens=7452, output_tokens=976
21:16:44,802 datashaper.workflow.workflow INFO executing verb window
21:16:44,806 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:16:45,31 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
21:16:45,31 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:16:45,93 datashaper.workflow.workflow INFO executing verb unroll
21:16:45,119 datashaper.workflow.workflow INFO executing verb select
21:16:45,146 datashaper.workflow.workflow INFO executing verb rename
21:16:45,171 datashaper.workflow.workflow INFO executing verb join
21:16:45,203 datashaper.workflow.workflow INFO executing verb aggregate_override
21:16:45,232 datashaper.workflow.workflow INFO executing verb join
21:16:45,268 datashaper.workflow.workflow INFO executing verb rename
21:16:45,296 datashaper.workflow.workflow INFO executing verb convert
21:16:45,418 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
21:16:45,690 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
21:16:45,691 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
21:16:45,764 datashaper.workflow.workflow INFO executing verb rename
21:16:45,767 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:16:45,894 graphrag.index.cli INFO All workflows completed successfully.
21:20:12,615 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:20:12,620 graphrag.index.cli INFO Starting pipeline run for: 20241201-212012, dryrun=False
21:20:12,621 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:20:12,624 graphrag.index.create_pipeline_config INFO skipping workflows 
21:20:12,624 graphrag.index.run.run INFO Running pipeline
21:20:12,625 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:20:12,626 graphrag.index.input.load_input INFO loading input from root_dir=input
21:20:12,626 graphrag.index.input.load_input INFO using file storage for input
21:20:12,631 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:20:12,632 graphrag.index.input.text INFO found text files from input, found [('2 - Copy (2).txt', {}), ('2 - Copy (3).txt', {}), ('2 - Copy (4).txt', {}), ('2 - Copy.txt', {}), ('2.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:20:12,652 graphrag.index.input.text INFO Found 6 files, loading 6
21:20:12,654 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:20:12,654 graphrag.index.run.run INFO Final # of rows loaded: 6
21:20:12,813 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:20:12,821 datashaper.workflow.workflow INFO executing verb orderby
21:20:12,827 datashaper.workflow.workflow INFO executing verb zip
21:20:12,833 datashaper.workflow.workflow INFO executing verb aggregate_override
21:20:12,842 datashaper.workflow.workflow INFO executing verb chunk
21:20:13,202 datashaper.workflow.workflow INFO executing verb select
21:20:13,208 datashaper.workflow.workflow INFO executing verb unroll
21:20:13,215 datashaper.workflow.workflow INFO executing verb rename
21:20:13,221 datashaper.workflow.workflow INFO executing verb genid
21:20:13,235 datashaper.workflow.workflow INFO executing verb unzip
21:20:13,243 datashaper.workflow.workflow INFO executing verb copy
21:20:13,250 datashaper.workflow.workflow INFO executing verb filter
21:20:13,265 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:20:13,443 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
21:20:13,444 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:20:13,481 datashaper.workflow.workflow INFO executing verb entity_extract
21:20:13,500 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
21:20:13,993 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
21:20:13,993 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
21:20:17,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:17,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7820000000065193. input_tokens=2096, output_tokens=234
21:20:21,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:21,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.639999999984866. input_tokens=34, output_tokens=368
21:20:21,872 datashaper.workflow.workflow INFO executing verb merge_graphs
21:20:22,8 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
21:20:22,188 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
21:20:22,188 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
21:20:22,222 datashaper.workflow.workflow INFO executing verb summarize_descriptions
21:20:23,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:23,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1719999999913853. input_tokens=177, output_tokens=54
21:20:23,960 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:23,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.610000000015134. input_tokens=226, output_tokens=101
21:20:24,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:24,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7190000000118744. input_tokens=214, output_tokens=105
21:20:24,218 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:24,219 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8440000000118744. input_tokens=238, output_tokens=118
21:20:24,636 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:24,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2820000000065193. input_tokens=449, output_tokens=154
21:20:24,804 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:24,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0. input_tokens=369, output_tokens=170
21:20:24,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:24,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0160000000032596. input_tokens=479, output_tokens=163
21:20:26,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:26,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.062999999994645. input_tokens=725, output_tokens=244
21:20:27,544 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:27,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.390999999974156. input_tokens=243, output_tokens=149
21:20:27,602 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
21:20:27,775 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
21:20:27,775 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
21:20:27,818 datashaper.workflow.workflow INFO executing verb cluster_graph
21:20:28,246 datashaper.workflow.workflow INFO executing verb select
21:20:28,250 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:20:28,456 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:20:28,457 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:20:28,497 datashaper.workflow.workflow INFO executing verb unpack_graph
21:20:28,677 datashaper.workflow.workflow INFO executing verb rename
21:20:28,691 datashaper.workflow.workflow INFO executing verb select
21:20:28,704 datashaper.workflow.workflow INFO executing verb dedupe
21:20:28,723 datashaper.workflow.workflow INFO executing verb rename
21:20:28,735 datashaper.workflow.workflow INFO executing verb filter
21:20:28,771 datashaper.workflow.workflow INFO executing verb text_split
21:20:28,794 datashaper.workflow.workflow INFO executing verb drop
21:20:28,809 datashaper.workflow.workflow INFO executing verb merge
21:20:28,953 datashaper.workflow.workflow INFO executing verb text_embed
21:20:28,955 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
21:20:29,408 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
21:20:29,408 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
21:20:29,462 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
21:20:30,132 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:20:30,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6879999999946449. input_tokens=936, output_tokens=0
21:20:30,256 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:20:30,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7820000000065193. input_tokens=131, output_tokens=0
21:20:30,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:20:30,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:20:30,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.110000000015134. input_tokens=669, output_tokens=0
21:20:30,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.139999999984866. input_tokens=1888, output_tokens=0
21:20:31,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
21:20:31,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0309999999881256. input_tokens=1051, output_tokens=0
21:20:31,553 datashaper.workflow.workflow INFO executing verb drop
21:20:31,567 datashaper.workflow.workflow INFO executing verb filter
21:20:31,592 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:20:31,852 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:20:31,853 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:20:31,892 datashaper.workflow.workflow INFO executing verb layout_graph
21:20:32,462 datashaper.workflow.workflow INFO executing verb unpack_graph
21:20:32,650 datashaper.workflow.workflow INFO executing verb unpack_graph
21:20:33,4 datashaper.workflow.workflow INFO executing verb filter
21:20:33,68 datashaper.workflow.workflow INFO executing verb drop
21:20:33,84 datashaper.workflow.workflow INFO executing verb select
21:20:33,101 datashaper.workflow.workflow INFO executing verb rename
21:20:33,119 datashaper.workflow.workflow INFO executing verb join
21:20:33,145 datashaper.workflow.workflow INFO executing verb convert
21:20:33,201 datashaper.workflow.workflow INFO executing verb rename
21:20:33,204 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:20:33,405 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:20:33,406 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:20:33,451 datashaper.workflow.workflow INFO executing verb create_final_communities
21:20:33,879 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:20:34,157 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:20:34,157 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:20:34,168 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:20:34,229 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
21:20:34,448 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
21:20:34,459 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:20:34,724 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
21:20:34,725 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:20:34,776 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:20:34,784 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:20:34,851 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
21:20:34,917 datashaper.workflow.workflow INFO executing verb select
21:20:34,920 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:20:35,211 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
21:20:35,215 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:20:35,224 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:20:35,280 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
21:20:35,323 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
21:20:35,363 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
21:20:35,399 datashaper.workflow.workflow INFO executing verb prepare_community_reports
21:20:35,400 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
21:20:35,612 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
21:20:35,855 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
21:20:36,364 datashaper.workflow.workflow INFO executing verb create_community_reports
21:20:46,39 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:46,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.171999999991385. input_tokens=2368, output_tokens=669
21:20:46,106 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:46,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.202999999979511. input_tokens=2181, output_tokens=668
21:20:46,250 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:46,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.296999999991385. input_tokens=2390, output_tokens=714
21:20:47,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:47,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.21799999999348. input_tokens=2254, output_tokens=650
21:20:47,223 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:47,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.437999999994645. input_tokens=2660, output_tokens=706
21:20:47,282 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:47,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.359000000025844. input_tokens=6507, output_tokens=804
21:20:47,380 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:47,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.562999999994645. input_tokens=3258, output_tokens=771
21:20:47,677 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:47,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.780999999988126. input_tokens=2550, output_tokens=679
21:20:47,943 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:47,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.094000000011874. input_tokens=3275, output_tokens=755
21:20:48,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:48,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.359000000025844. input_tokens=4368, output_tokens=810
21:20:48,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:48,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.360000000015134. input_tokens=2636, output_tokens=800
21:20:48,770 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:48,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.937000000005355. input_tokens=3201, output_tokens=851
21:20:48,945 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:48,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.10899999999674. input_tokens=4357, output_tokens=924
21:20:49,559 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:49,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.578000000008615. input_tokens=9874, output_tokens=872
21:20:49,780 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:49,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.969000000011874. input_tokens=2448, output_tokens=728
21:20:49,808 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:49,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.89000000001397. input_tokens=2150, output_tokens=613
21:20:49,862 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:49,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.030999999988126. input_tokens=5516, output_tokens=915
21:20:49,929 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:49,931 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.0. input_tokens=5192, output_tokens=861
21:20:50,82 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:50,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.14100000000326. input_tokens=4970, output_tokens=880
21:20:50,171 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:50,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.202999999979511. input_tokens=3475, output_tokens=873
21:20:50,343 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:50,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.375. input_tokens=3623, output_tokens=945
21:20:51,448 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:51,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.64100000000326. input_tokens=3706, output_tokens=840
21:20:52,236 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:52,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.35899999999674. input_tokens=6526, output_tokens=898
21:20:53,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:53,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.203000000008615. input_tokens=7552, output_tokens=917
21:20:53,798 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:20:53,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.85999999998603. input_tokens=3918, output_tokens=941
21:20:55,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:20:55,357 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:20:55,372 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:20:55,373 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:20:55,384 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:20:55,384 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:20:55,488 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:20:55,489 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:20:57,30 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:20:57,30 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:20:57,34 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:20:57,35 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:20:57,475 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:20:57,476 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:21:00,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:21:00,572 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:21:01,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:01,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.35999999998603. input_tokens=2122, output_tokens=502
21:21:01,709 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:01,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.860000000015134. input_tokens=2102, output_tokens=448
21:21:02,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:02,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.10899999999674. input_tokens=2217, output_tokens=696
21:21:03,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:03,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.172000000020489. input_tokens=2277, output_tokens=694
21:21:03,169 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:03,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.296999999991385. input_tokens=2308, output_tokens=702
21:21:04,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:04,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.562999999994645. input_tokens=2225, output_tokens=644
21:21:05,364 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:05,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.437999999994645. input_tokens=7809, output_tokens=813
21:21:05,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:05,444 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.703000000008615. input_tokens=2692, output_tokens=799
21:21:05,922 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:05,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 9.25. input_tokens=3971, output_tokens=781
21:21:05,950 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:05,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.062000000005355. input_tokens=3307, output_tokens=778
21:21:06,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:06,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.594000000011874. input_tokens=3518, output_tokens=859
21:21:06,720 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:06,721 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:06,723 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.828000000008615. input_tokens=5036, output_tokens=826
21:21:06,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.96799999999348. input_tokens=6688, output_tokens=909
21:21:07,246 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:07,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.405999999988126. input_tokens=2971, output_tokens=837
21:21:07,623 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:07,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.84399999998277. input_tokens=6095, output_tokens=836
21:21:08,447 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:08,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.703000000008615. input_tokens=6416, output_tokens=891
21:21:08,624 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:08,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.703000000008615. input_tokens=3617, output_tokens=717
21:21:08,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:08,990 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.077999999979511. input_tokens=3289, output_tokens=891
21:21:09,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:09,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.59299999999348. input_tokens=5350, output_tokens=956
21:21:09,891 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:09,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 10.78200000000652. input_tokens=8148, output_tokens=873
21:21:10,7 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:10,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.078999999997905. input_tokens=3406, output_tokens=996
21:21:10,734 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:10,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.827999999979511. input_tokens=3414, output_tokens=844
21:21:19,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:19,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 14.437999999994645. input_tokens=8446, output_tokens=903
21:21:28,430 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:28,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 14.73399999999674. input_tokens=9850, output_tokens=1082
21:21:35,946 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:35,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.25. input_tokens=2060, output_tokens=498
21:21:36,723 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:36,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.062000000005355. input_tokens=2116, output_tokens=567
21:21:37,504 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:37,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.828999999997905. input_tokens=2834, output_tokens=701
21:21:38,512 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:38,514 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.78200000000652. input_tokens=3006, output_tokens=706
21:21:39,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:39,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.422000000020489. input_tokens=2148, output_tokens=754
21:21:39,703 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:39,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.0. input_tokens=3309, output_tokens=790
21:21:40,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:40,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.59399999998277. input_tokens=2340, output_tokens=765
21:21:40,883 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:40,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.23499999998603. input_tokens=3950, output_tokens=839
21:21:41,618 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:41,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.875. input_tokens=5049, output_tokens=912
21:21:41,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:41,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.14100000000326. input_tokens=9433, output_tokens=999
21:21:42,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:42,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.046999999991385. input_tokens=6448, output_tokens=954
21:21:46,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:46,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.562999999994645. input_tokens=9220, output_tokens=849
21:21:47,527 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:21:47,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.53100000001723. input_tokens=7441, output_tokens=1137
21:21:47,583 datashaper.workflow.workflow INFO executing verb window
21:21:47,586 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:21:47,809 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
21:21:47,810 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:21:47,876 datashaper.workflow.workflow INFO executing verb unroll
21:21:47,905 datashaper.workflow.workflow INFO executing verb select
21:21:47,933 datashaper.workflow.workflow INFO executing verb rename
21:21:47,960 datashaper.workflow.workflow INFO executing verb join
21:21:47,995 datashaper.workflow.workflow INFO executing verb aggregate_override
21:21:48,25 datashaper.workflow.workflow INFO executing verb join
21:21:48,60 datashaper.workflow.workflow INFO executing verb rename
21:21:48,90 datashaper.workflow.workflow INFO executing verb convert
21:21:48,159 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
21:21:48,384 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
21:21:48,385 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
21:21:48,458 datashaper.workflow.workflow INFO executing verb rename
21:21:48,461 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:21:48,607 graphrag.index.cli INFO All workflows completed successfully.
21:44:22,859 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:44:22,866 graphrag.index.cli INFO Starting pipeline run for: 20241201-214422, dryrun=False
21:44:22,867 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:44:22,870 graphrag.index.create_pipeline_config INFO skipping workflows 
21:44:22,870 graphrag.index.run.run INFO Running pipeline
21:44:22,870 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:44:22,871 graphrag.index.input.load_input INFO loading input from root_dir=input
21:44:22,871 graphrag.index.input.load_input INFO using file storage for input
21:44:22,872 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:44:22,873 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:44:22,889 graphrag.index.input.text INFO Found 6 files, loading 6
21:44:22,891 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:44:22,892 graphrag.index.run.run INFO Final # of rows loaded: 6
21:44:23,32 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:44:23,33 datashaper.workflow.workflow INFO executing verb orderby
21:44:23,35 datashaper.workflow.workflow INFO executing verb zip
21:44:23,36 datashaper.workflow.workflow INFO executing verb aggregate_override
21:44:23,41 datashaper.workflow.workflow INFO executing verb chunk
21:44:23,403 datashaper.workflow.workflow INFO executing verb select
21:44:23,403 datashaper.workflow.workflow INFO executing verb unroll
21:44:23,406 datashaper.workflow.workflow INFO executing verb rename
21:44:23,406 datashaper.workflow.workflow INFO executing verb genid
21:44:23,413 datashaper.workflow.workflow INFO executing verb unzip
21:44:23,415 datashaper.workflow.workflow INFO executing verb copy
21:44:23,415 datashaper.workflow.workflow INFO executing verb filter
21:44:23,423 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:48:29,493 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:48:29,498 graphrag.index.cli INFO Starting pipeline run for: 20241201-214829, dryrun=False
21:48:29,498 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:48:29,501 graphrag.index.create_pipeline_config INFO skipping workflows 
21:48:29,501 graphrag.index.run.run INFO Running pipeline
21:48:29,501 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:48:29,502 graphrag.index.input.load_input INFO loading input from root_dir=input
21:48:29,502 graphrag.index.input.load_input INFO using file storage for input
21:48:29,503 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:48:29,504 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:48:29,518 graphrag.index.input.text INFO Found 6 files, loading 6
21:48:29,520 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:48:29,520 graphrag.index.run.run INFO Final # of rows loaded: 6
21:48:29,653 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:48:29,653 datashaper.workflow.workflow INFO executing verb orderby
21:48:29,654 datashaper.workflow.workflow INFO executing verb zip
21:48:29,655 datashaper.workflow.workflow INFO executing verb aggregate_override
21:48:29,658 datashaper.workflow.workflow INFO executing verb chunk
21:48:29,987 datashaper.workflow.workflow INFO executing verb select
21:48:29,987 datashaper.workflow.workflow INFO executing verb unroll
21:48:29,990 datashaper.workflow.workflow INFO executing verb rename
21:48:29,990 datashaper.workflow.workflow INFO executing verb genid
21:48:29,998 datashaper.workflow.workflow INFO executing verb unzip
21:48:29,999 datashaper.workflow.workflow INFO executing verb copy
21:48:29,999 datashaper.workflow.workflow INFO executing verb filter
21:48:30,7 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:50:26,788 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:50:26,791 graphrag.index.cli INFO Starting pipeline run for: 20241201-215026, dryrun=False
21:50:26,792 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:50:26,795 graphrag.index.create_pipeline_config INFO skipping workflows 
21:50:26,795 graphrag.index.run.run INFO Running pipeline
21:50:26,795 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:50:26,796 graphrag.index.input.load_input INFO loading input from root_dir=input
21:50:26,796 graphrag.index.input.load_input INFO using file storage for input
21:50:26,798 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:50:26,799 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:50:26,811 graphrag.index.input.text INFO Found 6 files, loading 6
21:50:26,812 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:50:26,813 graphrag.index.run.run INFO Final # of rows loaded: 6
21:50:26,971 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:50:26,977 datashaper.workflow.workflow INFO executing verb orderby
21:50:26,982 datashaper.workflow.workflow INFO executing verb zip
21:50:26,986 datashaper.workflow.workflow INFO executing verb aggregate_override
21:50:26,993 datashaper.workflow.workflow INFO executing verb chunk
21:50:27,363 datashaper.workflow.workflow INFO executing verb select
21:50:27,372 datashaper.workflow.workflow INFO executing verb unroll
21:50:27,381 datashaper.workflow.workflow INFO executing verb rename
21:50:27,388 datashaper.workflow.workflow INFO executing verb genid
21:50:27,403 datashaper.workflow.workflow INFO executing verb unzip
21:50:27,413 datashaper.workflow.workflow INFO executing verb copy
21:50:27,421 datashaper.workflow.workflow INFO executing verb filter
21:50:27,439 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:50:27,675 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
21:50:27,675 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:50:27,712 datashaper.workflow.workflow INFO executing verb entity_extract
21:50:27,729 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
21:50:28,180 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
21:50:28,180 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
21:50:28,676 datashaper.workflow.workflow INFO executing verb merge_graphs
21:50:28,857 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
21:50:29,50 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
21:50:29,51 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
21:50:29,83 datashaper.workflow.workflow INFO executing verb summarize_descriptions
21:50:30,53 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
21:50:30,229 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
21:50:30,229 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
21:50:30,259 datashaper.workflow.workflow INFO executing verb cluster_graph
21:50:30,632 datashaper.workflow.workflow INFO executing verb select
21:50:30,636 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:50:30,827 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:50:30,828 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:50:30,863 datashaper.workflow.workflow INFO executing verb unpack_graph
21:50:31,48 datashaper.workflow.workflow INFO executing verb rename
21:50:31,60 datashaper.workflow.workflow INFO executing verb select
21:50:31,71 datashaper.workflow.workflow INFO executing verb dedupe
21:50:31,84 datashaper.workflow.workflow INFO executing verb rename
21:50:31,97 datashaper.workflow.workflow INFO executing verb filter
21:50:31,143 datashaper.workflow.workflow INFO executing verb text_split
21:50:31,163 datashaper.workflow.workflow INFO executing verb drop
21:50:31,177 datashaper.workflow.workflow INFO executing verb merge
21:50:31,309 datashaper.workflow.workflow INFO executing verb text_embed
21:50:31,310 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
21:50:31,793 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
21:50:31,793 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
21:50:31,849 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
21:50:32,331 datashaper.workflow.workflow INFO executing verb drop
21:50:32,348 datashaper.workflow.workflow INFO executing verb filter
21:50:32,374 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:50:32,658 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:50:32,659 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:50:32,693 datashaper.workflow.workflow INFO executing verb layout_graph
21:50:33,193 datashaper.workflow.workflow INFO executing verb unpack_graph
21:50:33,407 datashaper.workflow.workflow INFO executing verb unpack_graph
21:50:33,859 datashaper.workflow.workflow INFO executing verb filter
21:50:33,914 datashaper.workflow.workflow INFO executing verb drop
21:50:33,932 datashaper.workflow.workflow INFO executing verb select
21:50:33,949 datashaper.workflow.workflow INFO executing verb rename
21:50:33,966 datashaper.workflow.workflow INFO executing verb convert
21:50:34,30 datashaper.workflow.workflow INFO executing verb join
21:50:34,63 datashaper.workflow.workflow INFO executing verb rename
21:50:34,85 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:50:34,322 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:50:34,322 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:50:34,373 datashaper.workflow.workflow INFO executing verb create_final_communities
21:50:34,787 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:50:35,1 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:50:35,1 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:50:35,9 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:50:35,61 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
21:50:35,284 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
21:50:35,293 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:50:35,503 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
21:50:35,504 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:50:35,517 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:50:35,523 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:50:35,604 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
21:50:35,652 datashaper.workflow.workflow INFO executing verb select
21:50:35,655 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:50:35,880 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:50:35,880 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:50:35,888 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:50:35,937 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
21:50:35,977 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
21:50:36,13 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
21:50:36,47 datashaper.workflow.workflow INFO executing verb prepare_community_reports
21:50:36,48 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
21:50:36,259 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
21:50:36,479 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
21:50:36,681 datashaper.workflow.workflow INFO executing verb create_community_reports
21:50:49,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:50:49,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.469000000011874. input_tokens=7604, output_tokens=919
21:50:52,586 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:50:52,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.53200000000652. input_tokens=6507, output_tokens=776
21:50:52,612 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:50:52,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.59299999999348. input_tokens=4368, output_tokens=781
21:50:52,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:50:52,839 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.812999999994645. input_tokens=4379, output_tokens=779
21:50:53,947 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:50:53,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.875. input_tokens=9868, output_tokens=1118
21:51:05,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:05,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.437000000005355. input_tokens=7784, output_tokens=783
21:51:08,112 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:08,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.264999999984866. input_tokens=6117, output_tokens=811
21:51:08,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:08,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.422000000020489. input_tokens=8498, output_tokens=952
21:51:08,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:08,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.64100000000326. input_tokens=3954, output_tokens=839
21:51:10,78 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:10,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.25. input_tokens=6416, output_tokens=918
21:51:13,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:13,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.65600000001723. input_tokens=3603, output_tokens=1132
21:51:25,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:25,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.687999999994645. input_tokens=9416, output_tokens=946
21:51:26,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:26,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.655999999988126. input_tokens=9377, output_tokens=1019
21:51:31,65 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:31,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.34299999999348. input_tokens=5093, output_tokens=944
21:51:34,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
21:51:34,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.79700000002049. input_tokens=7411, output_tokens=1117
21:51:34,574 datashaper.workflow.workflow INFO executing verb window
21:51:34,576 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:51:34,776 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
21:51:34,777 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:51:34,829 datashaper.workflow.workflow INFO executing verb unroll
21:51:34,854 datashaper.workflow.workflow INFO executing verb select
21:51:34,878 datashaper.workflow.workflow INFO executing verb rename
21:51:34,900 datashaper.workflow.workflow INFO executing verb join
21:51:34,953 datashaper.workflow.workflow INFO executing verb aggregate_override
21:51:34,978 datashaper.workflow.workflow INFO executing verb join
21:51:35,6 datashaper.workflow.workflow INFO executing verb rename
21:51:35,30 datashaper.workflow.workflow INFO executing verb convert
21:51:35,82 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
21:51:35,279 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
21:51:35,279 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
21:51:35,336 datashaper.workflow.workflow INFO executing verb rename
21:51:35,338 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:51:35,429 graphrag.index.cli INFO All workflows completed successfully.
21:52:16,812 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:52:16,815 graphrag.index.cli INFO Starting pipeline run for: 20241201-215216, dryrun=False
21:52:16,816 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:52:16,819 graphrag.index.create_pipeline_config INFO skipping workflows 
21:52:16,819 graphrag.index.run.run INFO Running pipeline
21:52:16,819 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:52:16,820 graphrag.index.input.load_input INFO loading input from root_dir=input
21:52:16,820 graphrag.index.input.load_input INFO using file storage for input
21:52:16,821 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:52:16,821 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:52:16,834 graphrag.index.input.text INFO Found 6 files, loading 6
21:52:16,836 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:52:16,836 graphrag.index.run.run INFO Final # of rows loaded: 6
21:52:16,970 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:52:16,970 datashaper.workflow.workflow INFO executing verb orderby
21:52:16,971 datashaper.workflow.workflow INFO executing verb zip
21:52:16,972 datashaper.workflow.workflow INFO executing verb aggregate_override
21:52:16,975 datashaper.workflow.workflow INFO executing verb chunk
21:52:17,333 datashaper.workflow.workflow INFO executing verb select
21:52:17,334 datashaper.workflow.workflow INFO executing verb unroll
21:52:17,336 datashaper.workflow.workflow INFO executing verb rename
21:52:17,337 datashaper.workflow.workflow INFO executing verb genid
21:52:17,344 datashaper.workflow.workflow INFO executing verb unzip
21:52:17,346 datashaper.workflow.workflow INFO executing verb copy
21:52:17,346 datashaper.workflow.workflow INFO executing verb filter
21:52:17,354 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:53:47,151 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:53:47,155 graphrag.index.cli INFO Starting pipeline run for: 20241201-215347, dryrun=False
21:53:47,156 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:53:47,158 graphrag.index.create_pipeline_config INFO skipping workflows 
21:53:47,158 graphrag.index.run.run INFO Running pipeline
21:53:47,158 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:53:47,159 graphrag.index.input.load_input INFO loading input from root_dir=input
21:53:47,159 graphrag.index.input.load_input INFO using file storage for input
21:53:47,160 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:53:47,160 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:53:47,173 graphrag.index.input.text INFO Found 6 files, loading 6
21:53:47,174 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:53:47,175 graphrag.index.run.run INFO Final # of rows loaded: 6
21:53:47,302 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:53:47,303 datashaper.workflow.workflow INFO executing verb orderby
21:53:47,304 datashaper.workflow.workflow INFO executing verb zip
21:53:47,305 datashaper.workflow.workflow INFO executing verb aggregate_override
21:53:47,307 datashaper.workflow.workflow INFO executing verb chunk
21:53:47,629 datashaper.workflow.workflow INFO executing verb select
21:53:47,630 datashaper.workflow.workflow INFO executing verb unroll
21:53:47,632 datashaper.workflow.workflow INFO executing verb rename
21:53:47,632 datashaper.workflow.workflow INFO executing verb genid
21:53:47,640 datashaper.workflow.workflow INFO executing verb unzip
21:53:47,642 datashaper.workflow.workflow INFO executing verb copy
21:53:47,642 datashaper.workflow.workflow INFO executing verb filter
21:53:47,649 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:54:38,54 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:54:38,59 graphrag.index.cli INFO Starting pipeline run for: 20241201-215438, dryrun=False
21:54:38,59 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:54:38,62 graphrag.index.create_pipeline_config INFO skipping workflows 
21:54:38,62 graphrag.index.run.run INFO Running pipeline
21:54:38,62 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:54:38,63 graphrag.index.input.load_input INFO loading input from root_dir=input
21:54:38,63 graphrag.index.input.load_input INFO using file storage for input
21:54:38,64 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:54:38,64 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:54:38,78 graphrag.index.input.text INFO Found 6 files, loading 6
21:54:38,80 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:54:38,80 graphrag.index.run.run INFO Final # of rows loaded: 6
21:54:38,218 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:54:38,219 datashaper.workflow.workflow INFO executing verb orderby
21:54:38,220 datashaper.workflow.workflow INFO executing verb zip
21:54:38,221 datashaper.workflow.workflow INFO executing verb aggregate_override
21:54:38,224 datashaper.workflow.workflow INFO executing verb chunk
21:54:38,564 datashaper.workflow.workflow INFO executing verb select
21:54:38,565 datashaper.workflow.workflow INFO executing verb unroll
21:54:38,567 datashaper.workflow.workflow INFO executing verb rename
21:54:38,568 datashaper.workflow.workflow INFO executing verb genid
21:54:38,575 datashaper.workflow.workflow INFO executing verb unzip
21:54:38,576 datashaper.workflow.workflow INFO executing verb copy
21:54:38,577 datashaper.workflow.workflow INFO executing verb filter
21:54:38,584 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:57:38,134 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:57:38,139 graphrag.index.cli INFO Starting pipeline run for: 20241201-215738, dryrun=False
21:57:38,140 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:57:38,142 graphrag.index.create_pipeline_config INFO skipping workflows 
21:57:38,142 graphrag.index.run.run INFO Running pipeline
21:57:38,142 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:57:38,143 graphrag.index.input.load_input INFO loading input from root_dir=input
21:57:38,144 graphrag.index.input.load_input INFO using file storage for input
21:57:38,144 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:57:38,145 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:57:38,158 graphrag.index.input.text INFO Found 6 files, loading 6
21:57:38,160 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:57:38,160 graphrag.index.run.run INFO Final # of rows loaded: 6
21:57:38,324 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:57:38,324 datashaper.workflow.workflow INFO executing verb orderby
21:57:38,325 datashaper.workflow.workflow INFO executing verb zip
21:57:38,326 datashaper.workflow.workflow INFO executing verb aggregate_override
21:57:38,330 datashaper.workflow.workflow INFO executing verb chunk
21:57:38,670 datashaper.workflow.workflow INFO executing verb select
21:57:38,671 datashaper.workflow.workflow INFO executing verb unroll
21:57:38,673 datashaper.workflow.workflow INFO executing verb rename
21:57:38,673 datashaper.workflow.workflow INFO executing verb genid
21:57:38,682 datashaper.workflow.workflow INFO executing verb unzip
21:57:38,683 datashaper.workflow.workflow INFO executing verb copy
21:57:38,684 datashaper.workflow.workflow INFO executing verb filter
21:57:38,692 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:58:17,521 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
21:58:17,525 graphrag.index.cli INFO Starting pipeline run for: 20241201-215817, dryrun=False
21:58:17,526 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:58:17,528 graphrag.index.create_pipeline_config INFO skipping workflows 
21:58:17,529 graphrag.index.run.run INFO Running pipeline
21:58:17,529 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
21:58:17,530 graphrag.index.input.load_input INFO loading input from root_dir=input
21:58:17,530 graphrag.index.input.load_input INFO using file storage for input
21:58:17,530 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
21:58:17,531 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
21:58:17,545 graphrag.index.input.text INFO Found 6 files, loading 6
21:58:17,546 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
21:58:17,546 graphrag.index.run.run INFO Final # of rows loaded: 6
21:58:17,691 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:58:17,692 datashaper.workflow.workflow INFO executing verb orderby
21:58:17,693 datashaper.workflow.workflow INFO executing verb zip
21:58:17,694 datashaper.workflow.workflow INFO executing verb aggregate_override
21:58:17,697 datashaper.workflow.workflow INFO executing verb chunk
21:58:18,30 datashaper.workflow.workflow INFO executing verb select
21:58:18,31 datashaper.workflow.workflow INFO executing verb unroll
21:58:18,33 datashaper.workflow.workflow INFO executing verb rename
21:58:18,34 datashaper.workflow.workflow INFO executing verb genid
21:58:18,41 datashaper.workflow.workflow INFO executing verb unzip
21:58:18,42 datashaper.workflow.workflow INFO executing verb copy
21:58:18,43 datashaper.workflow.workflow INFO executing verb filter
21:58:18,50 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:00:29,831 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:00:29,834 graphrag.index.cli INFO Starting pipeline run for: 20241201-220029, dryrun=False
22:00:29,835 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:00:29,838 graphrag.index.create_pipeline_config INFO skipping workflows 
22:00:29,838 graphrag.index.run.run INFO Running pipeline
22:00:29,838 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:00:29,839 graphrag.index.input.load_input INFO loading input from root_dir=input
22:00:29,839 graphrag.index.input.load_input INFO using file storage for input
22:00:29,841 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:00:29,842 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:00:29,858 graphrag.index.input.text INFO Found 6 files, loading 6
22:00:29,861 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:00:29,861 graphrag.index.run.run INFO Final # of rows loaded: 6
22:00:30,3 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:00:30,9 datashaper.workflow.workflow INFO executing verb orderby
22:00:30,13 datashaper.workflow.workflow INFO executing verb zip
22:00:30,18 datashaper.workflow.workflow INFO executing verb aggregate_override
22:00:30,25 datashaper.workflow.workflow INFO executing verb chunk
22:00:30,372 datashaper.workflow.workflow INFO executing verb select
22:00:30,379 datashaper.workflow.workflow INFO executing verb unroll
22:00:30,387 datashaper.workflow.workflow INFO executing verb rename
22:00:30,394 datashaper.workflow.workflow INFO executing verb genid
22:00:30,407 datashaper.workflow.workflow INFO executing verb unzip
22:00:30,414 datashaper.workflow.workflow INFO executing verb copy
22:00:30,423 datashaper.workflow.workflow INFO executing verb filter
22:00:30,442 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:00:30,658 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:00:30,658 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:00:30,693 datashaper.workflow.workflow INFO executing verb entity_extract
22:00:30,710 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:00:31,155 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:00:31,155 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:00:31,577 datashaper.workflow.workflow INFO executing verb merge_graphs
22:00:31,738 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:00:31,918 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:00:31,919 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:00:31,952 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:00:35,103 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:00:35,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.625. input_tokens=276, output_tokens=178
22:00:35,157 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:00:35,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4530000000086147. input_tokens=313, output_tokens=187
22:00:35,204 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:00:35,361 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:00:35,362 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:00:35,389 datashaper.workflow.workflow INFO executing verb cluster_graph
22:00:35,778 datashaper.workflow.workflow INFO executing verb select
22:00:35,783 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:00:35,970 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:00:35,980 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:00:36,14 datashaper.workflow.workflow INFO executing verb unpack_graph
22:00:36,179 datashaper.workflow.workflow INFO executing verb rename
22:00:36,190 datashaper.workflow.workflow INFO executing verb select
22:00:36,202 datashaper.workflow.workflow INFO executing verb dedupe
22:00:36,214 datashaper.workflow.workflow INFO executing verb rename
22:00:36,227 datashaper.workflow.workflow INFO executing verb filter
22:00:36,266 datashaper.workflow.workflow INFO executing verb text_split
22:00:36,289 datashaper.workflow.workflow INFO executing verb drop
22:00:36,303 datashaper.workflow.workflow INFO executing verb merge
22:00:36,409 datashaper.workflow.workflow INFO executing verb text_embed
22:00:36,410 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:00:36,811 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:00:36,811 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:00:36,856 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:00:37,266 datashaper.workflow.workflow INFO executing verb drop
22:00:37,279 datashaper.workflow.workflow INFO executing verb filter
22:00:37,304 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:00:37,586 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:00:37,587 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:00:37,622 datashaper.workflow.workflow INFO executing verb layout_graph
22:00:38,240 datashaper.workflow.workflow INFO executing verb unpack_graph
22:00:38,435 datashaper.workflow.workflow INFO executing verb unpack_graph
22:00:38,846 datashaper.workflow.workflow INFO executing verb drop
22:00:38,864 datashaper.workflow.workflow INFO executing verb filter
22:00:38,928 datashaper.workflow.workflow INFO executing verb select
22:00:38,947 datashaper.workflow.workflow INFO executing verb rename
22:00:38,966 datashaper.workflow.workflow INFO executing verb join
22:00:38,999 datashaper.workflow.workflow INFO executing verb convert
22:00:39,62 datashaper.workflow.workflow INFO executing verb rename
22:00:39,66 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:00:39,292 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:00:39,301 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:00:39,347 datashaper.workflow.workflow INFO executing verb create_final_communities
22:00:39,767 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:00:39,971 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:00:39,971 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:00:39,988 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:00:40,35 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:00:40,236 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:00:40,245 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:00:40,474 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
22:00:40,474 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:00:40,489 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:00:40,495 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:00:40,578 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:00:40,629 datashaper.workflow.workflow INFO executing verb select
22:00:40,631 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:00:40,846 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:00:40,846 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:00:40,854 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:00:40,901 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:00:40,940 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:00:40,989 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:00:41,21 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:00:41,22 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
22:00:41,207 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
22:00:41,415 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
22:00:41,643 datashaper.workflow.workflow INFO executing verb create_community_reports
22:00:54,225 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:00:54,227 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.327999999979511. input_tokens=3945, output_tokens=815
22:00:54,928 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:00:54,930 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.046000000002095. input_tokens=4378, output_tokens=891
22:00:55,57 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:00:55,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.14000000001397. input_tokens=9857, output_tokens=874
22:00:56,820 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:00:56,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.937999999994645. input_tokens=6528, output_tokens=830
22:01:09,545 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:01:09,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.905999999988126. input_tokens=7862, output_tokens=838
22:01:10,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:01:10,394 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.76600000000326. input_tokens=6426, output_tokens=888
22:01:11,986 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:01:11,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.327999999979511. input_tokens=9854, output_tokens=1103
22:01:12,136 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:01:12,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.5. input_tokens=3408, output_tokens=1002
22:01:29,495 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:01:29,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.171999999991385. input_tokens=9220, output_tokens=841
22:01:31,516 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:01:31,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.171999999991385. input_tokens=7503, output_tokens=1201
22:01:31,573 datashaper.workflow.workflow INFO executing verb window
22:01:31,576 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:01:31,798 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:01:31,799 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:01:31,853 datashaper.workflow.workflow INFO executing verb unroll
22:01:31,877 datashaper.workflow.workflow INFO executing verb select
22:01:31,899 datashaper.workflow.workflow INFO executing verb rename
22:01:31,922 datashaper.workflow.workflow INFO executing verb join
22:01:31,951 datashaper.workflow.workflow INFO executing verb aggregate_override
22:01:31,977 datashaper.workflow.workflow INFO executing verb join
22:01:32,27 datashaper.workflow.workflow INFO executing verb rename
22:01:32,55 datashaper.workflow.workflow INFO executing verb convert
22:01:32,115 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:01:32,355 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:01:32,355 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:01:32,412 datashaper.workflow.workflow INFO executing verb rename
22:01:32,414 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:01:32,507 graphrag.index.cli INFO All workflows completed successfully.
22:10:06,487 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:10:06,491 graphrag.index.cli INFO Starting pipeline run for: 20241201-221006, dryrun=False
22:10:06,491 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:10:06,494 graphrag.index.create_pipeline_config INFO skipping workflows 
22:10:06,494 graphrag.index.run.run INFO Running pipeline
22:10:06,494 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:10:06,495 graphrag.index.input.load_input INFO loading input from root_dir=input
22:10:06,496 graphrag.index.input.load_input INFO using file storage for input
22:10:06,497 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:10:06,498 graphrag.index.input.text INFO found text files from input, found [('1_copy0.txt', {}), ('1_copy1.txt', {}), ('1_copy2.txt', {}), ('1_copy3.txt', {}), ('1_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:10:06,514 graphrag.index.input.text INFO Found 6 files, loading 6
22:10:06,515 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:10:06,516 graphrag.index.run.run INFO Final # of rows loaded: 6
22:10:06,652 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:10:06,658 datashaper.workflow.workflow INFO executing verb orderby
22:10:06,662 datashaper.workflow.workflow INFO executing verb zip
22:10:06,667 datashaper.workflow.workflow INFO executing verb aggregate_override
22:10:06,674 datashaper.workflow.workflow INFO executing verb chunk
22:10:07,14 datashaper.workflow.workflow INFO executing verb select
22:10:07,20 datashaper.workflow.workflow INFO executing verb unroll
22:10:07,28 datashaper.workflow.workflow INFO executing verb rename
22:10:07,34 datashaper.workflow.workflow INFO executing verb genid
22:10:07,47 datashaper.workflow.workflow INFO executing verb unzip
22:10:07,55 datashaper.workflow.workflow INFO executing verb copy
22:10:07,62 datashaper.workflow.workflow INFO executing verb filter
22:10:07,80 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:10:07,260 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:10:07,260 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:10:07,296 datashaper.workflow.workflow INFO executing verb entity_extract
22:10:07,313 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:10:07,846 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:10:07,846 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:10:08,268 datashaper.workflow.workflow INFO executing verb merge_graphs
22:10:08,396 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:10:08,565 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:10:08,566 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:10:08,595 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:10:10,944 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:10,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.937000000005355. input_tokens=479, output_tokens=150
22:10:11,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:11,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.812999999994645. input_tokens=224, output_tokens=133
22:10:11,136 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:10:11,297 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:10:11,298 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:10:11,325 datashaper.workflow.workflow INFO executing verb cluster_graph
22:10:11,784 datashaper.workflow.workflow INFO executing verb select
22:10:11,788 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:10:11,995 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:10:11,996 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:10:12,32 datashaper.workflow.workflow INFO executing verb unpack_graph
22:10:12,230 datashaper.workflow.workflow INFO executing verb rename
22:10:12,240 datashaper.workflow.workflow INFO executing verb select
22:10:12,252 datashaper.workflow.workflow INFO executing verb dedupe
22:10:12,264 datashaper.workflow.workflow INFO executing verb rename
22:10:12,276 datashaper.workflow.workflow INFO executing verb filter
22:10:12,312 datashaper.workflow.workflow INFO executing verb text_split
22:10:12,332 datashaper.workflow.workflow INFO executing verb drop
22:10:12,349 datashaper.workflow.workflow INFO executing verb merge
22:10:12,491 datashaper.workflow.workflow INFO executing verb text_embed
22:10:12,493 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:10:12,950 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:10:12,950 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:10:13,0 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:10:13,455 datashaper.workflow.workflow INFO executing verb drop
22:10:13,469 datashaper.workflow.workflow INFO executing verb filter
22:10:13,496 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:10:13,791 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:10:13,791 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:10:13,832 datashaper.workflow.workflow INFO executing verb layout_graph
22:10:14,723 datashaper.workflow.workflow INFO executing verb unpack_graph
22:10:15,15 datashaper.workflow.workflow INFO executing verb unpack_graph
22:10:15,295 datashaper.workflow.workflow INFO executing verb drop
22:10:15,311 datashaper.workflow.workflow INFO executing verb filter
22:10:15,381 datashaper.workflow.workflow INFO executing verb select
22:10:15,399 datashaper.workflow.workflow INFO executing verb rename
22:10:15,418 datashaper.workflow.workflow INFO executing verb join
22:10:15,452 datashaper.workflow.workflow INFO executing verb convert
22:10:15,518 datashaper.workflow.workflow INFO executing verb rename
22:10:15,521 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:10:15,744 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:10:15,744 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:10:15,791 datashaper.workflow.workflow INFO executing verb create_final_communities
22:10:16,356 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:10:16,593 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:10:16,593 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:10:16,605 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:10:16,660 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:10:16,924 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:10:16,956 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:10:17,171 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
22:10:17,172 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:10:17,213 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:10:17,220 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:10:17,271 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:10:17,321 datashaper.workflow.workflow INFO executing verb select
22:10:17,324 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:10:17,531 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:10:17,532 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:10:17,540 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:10:17,588 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:10:17,635 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:10:17,672 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:10:17,707 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:10:17,708 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 405
22:10:17,743 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
22:10:17,902 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
22:10:18,158 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
22:10:18,354 datashaper.workflow.workflow INFO executing verb create_community_reports
22:10:30,768 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:30,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.39100000000326. input_tokens=7244, output_tokens=936
22:10:43,534 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:43,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.844000000011874. input_tokens=4375, output_tokens=799
22:10:43,998 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:44,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.328000000008615. input_tokens=4241, output_tokens=833
22:10:44,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:44,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.09399999998277. input_tokens=9891, output_tokens=882
22:10:45,74 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:45,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.375. input_tokens=8824, output_tokens=887
22:10:56,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:56,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.562000000005355. input_tokens=6313, output_tokens=849
22:10:56,693 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:56,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.594000000011874. input_tokens=3891, output_tokens=849
22:10:56,870 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:56,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.75. input_tokens=5915, output_tokens=899
22:10:57,43 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:57,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.937999999994645. input_tokens=5706, output_tokens=885
22:10:57,614 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:10:57,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.48399999999674. input_tokens=9826, output_tokens=888
22:11:02,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:11:02,875 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.796999999991385. input_tokens=4855, output_tokens=844
22:11:18,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:11:18,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.796999999991385. input_tokens=7381, output_tokens=879
22:11:18,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:11:18,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.875. input_tokens=9638, output_tokens=898
22:11:22,88 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:11:22,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.04700000002049. input_tokens=7082, output_tokens=883
22:11:22,164 datashaper.workflow.workflow INFO executing verb window
22:11:22,197 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:11:22,465 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:11:22,465 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:11:22,533 datashaper.workflow.workflow INFO executing verb unroll
22:11:22,562 datashaper.workflow.workflow INFO executing verb select
22:11:22,588 datashaper.workflow.workflow INFO executing verb rename
22:11:22,617 datashaper.workflow.workflow INFO executing verb join
22:11:22,650 datashaper.workflow.workflow INFO executing verb aggregate_override
22:11:22,679 datashaper.workflow.workflow INFO executing verb join
22:11:22,714 datashaper.workflow.workflow INFO executing verb rename
22:11:22,743 datashaper.workflow.workflow INFO executing verb convert
22:11:22,805 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:11:23,30 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:11:23,31 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:11:23,98 datashaper.workflow.workflow INFO executing verb rename
22:11:23,101 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:11:23,211 graphrag.index.cli INFO All workflows completed successfully.
22:12:02,925 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:12:02,930 graphrag.index.cli INFO Starting pipeline run for: 20241201-221202, dryrun=False
22:12:02,932 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:12:02,936 graphrag.index.create_pipeline_config INFO skipping workflows 
22:12:02,936 graphrag.index.run.run INFO Running pipeline
22:12:02,936 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:12:02,938 graphrag.index.input.load_input INFO loading input from root_dir=input
22:12:02,938 graphrag.index.input.load_input INFO using file storage for input
22:12:02,941 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:12:02,942 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:12:02,973 graphrag.index.input.text INFO Found 6 files, loading 6
22:12:02,976 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:12:02,976 graphrag.index.run.run INFO Final # of rows loaded: 6
22:12:03,174 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:12:03,181 datashaper.workflow.workflow INFO executing verb orderby
22:12:03,187 datashaper.workflow.workflow INFO executing verb zip
22:12:03,193 datashaper.workflow.workflow INFO executing verb aggregate_override
22:12:03,201 datashaper.workflow.workflow INFO executing verb chunk
22:12:03,641 datashaper.workflow.workflow INFO executing verb select
22:12:03,648 datashaper.workflow.workflow INFO executing verb unroll
22:12:03,660 datashaper.workflow.workflow INFO executing verb rename
22:12:03,667 datashaper.workflow.workflow INFO executing verb genid
22:12:03,704 datashaper.workflow.workflow INFO executing verb unzip
22:12:03,723 datashaper.workflow.workflow INFO executing verb copy
22:12:03,732 datashaper.workflow.workflow INFO executing verb filter
22:12:03,775 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:12:04,112 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:12:04,112 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:12:04,152 datashaper.workflow.workflow INFO executing verb entity_extract
22:12:04,172 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:12:04,645 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:12:04,645 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:12:05,218 datashaper.workflow.workflow INFO executing verb merge_graphs
22:12:05,384 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:12:05,568 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:12:05,569 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:12:05,601 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:12:06,448 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:12:06,612 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:12:06,613 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:12:06,644 datashaper.workflow.workflow INFO executing verb cluster_graph
22:12:07,23 datashaper.workflow.workflow INFO executing verb select
22:12:07,28 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:12:07,229 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:12:07,230 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:12:07,269 datashaper.workflow.workflow INFO executing verb unpack_graph
22:12:07,431 datashaper.workflow.workflow INFO executing verb rename
22:12:07,442 datashaper.workflow.workflow INFO executing verb select
22:12:07,454 datashaper.workflow.workflow INFO executing verb dedupe
22:12:07,467 datashaper.workflow.workflow INFO executing verb rename
22:12:07,479 datashaper.workflow.workflow INFO executing verb filter
22:12:07,513 datashaper.workflow.workflow INFO executing verb text_split
22:12:07,533 datashaper.workflow.workflow INFO executing verb drop
22:12:07,547 datashaper.workflow.workflow INFO executing verb merge
22:12:07,668 datashaper.workflow.workflow INFO executing verb text_embed
22:12:07,670 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:12:08,86 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:12:08,86 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:12:08,138 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:12:08,592 datashaper.workflow.workflow INFO executing verb drop
22:12:08,606 datashaper.workflow.workflow INFO executing verb filter
22:12:08,630 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:12:08,904 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:12:08,905 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:12:08,943 datashaper.workflow.workflow INFO executing verb layout_graph
22:12:09,504 datashaper.workflow.workflow INFO executing verb unpack_graph
22:12:09,689 datashaper.workflow.workflow INFO executing verb unpack_graph
22:12:10,26 datashaper.workflow.workflow INFO executing verb drop
22:12:10,42 datashaper.workflow.workflow INFO executing verb filter
22:12:10,112 datashaper.workflow.workflow INFO executing verb select
22:12:10,137 datashaper.workflow.workflow INFO executing verb rename
22:12:10,157 datashaper.workflow.workflow INFO executing verb convert
22:12:10,228 datashaper.workflow.workflow INFO executing verb join
22:12:10,256 datashaper.workflow.workflow INFO executing verb rename
22:12:10,259 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:12:10,474 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:12:10,483 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:12:10,528 datashaper.workflow.workflow INFO executing verb create_final_communities
22:12:10,934 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:12:11,161 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:12:11,162 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:12:11,177 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:12:11,223 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:12:11,430 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:12:11,441 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:12:11,676 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
22:12:11,676 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:12:11,691 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:12:11,698 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:12:11,793 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:12:11,845 datashaper.workflow.workflow INFO executing verb select
22:12:11,847 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:12:12,65 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:12:12,65 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:12:12,74 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:12:12,130 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:12:12,176 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:12:12,216 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:12:12,260 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:12:12,261 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
22:12:12,510 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
22:12:12,793 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
22:12:13,26 datashaper.workflow.workflow INFO executing verb create_community_reports
22:12:25,99 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:12:25,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.671999999991385. input_tokens=7555, output_tokens=778
22:12:25,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:12:25,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.812000000005355. input_tokens=5195, output_tokens=814
22:12:26,448 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:12:26,450 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.0. input_tokens=6528, output_tokens=899
22:12:30,276 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:12:30,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.797000000020489. input_tokens=9857, output_tokens=1189
22:12:42,186 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:12:42,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.75. input_tokens=8449, output_tokens=766
22:12:44,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:12:44,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.780999999988126. input_tokens=7938, output_tokens=908
22:12:48,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:12:48,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.98399999999674. input_tokens=3675, output_tokens=1207
22:12:59,491 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:12:59,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.796999999991385. input_tokens=4911, output_tokens=758
22:13:04,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:13:04,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.828000000008615. input_tokens=9395, output_tokens=1049
22:13:05,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:13:05,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.389999999984866. input_tokens=7570, output_tokens=1142
22:13:05,168 datashaper.workflow.workflow INFO executing verb window
22:13:05,172 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:13:05,464 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:13:05,464 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:13:05,540 datashaper.workflow.workflow INFO executing verb unroll
22:13:05,620 datashaper.workflow.workflow INFO executing verb select
22:13:05,657 datashaper.workflow.workflow INFO executing verb rename
22:13:05,694 datashaper.workflow.workflow INFO executing verb join
22:13:05,737 datashaper.workflow.workflow INFO executing verb aggregate_override
22:13:05,785 datashaper.workflow.workflow INFO executing verb join
22:13:05,831 datashaper.workflow.workflow INFO executing verb rename
22:13:05,866 datashaper.workflow.workflow INFO executing verb convert
22:13:05,993 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:13:06,299 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:13:06,300 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:13:06,381 datashaper.workflow.workflow INFO executing verb rename
22:13:06,384 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:13:06,537 graphrag.index.cli INFO All workflows completed successfully.
22:13:56,775 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:13:56,779 graphrag.index.cli INFO Starting pipeline run for: 20241201-221356, dryrun=False
22:13:56,780 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:13:56,784 graphrag.index.create_pipeline_config INFO skipping workflows 
22:13:56,785 graphrag.index.run.run INFO Running pipeline
22:13:56,785 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:13:56,786 graphrag.index.input.load_input INFO loading input from root_dir=input
22:13:56,786 graphrag.index.input.load_input INFO using file storage for input
22:13:56,789 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:13:56,790 graphrag.index.input.text INFO found text files from input, found [('3_copy0.txt', {}), ('3_copy1.txt', {}), ('3_copy2.txt', {}), ('3_copy3.txt', {}), ('3_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:13:56,808 graphrag.index.input.text INFO Found 6 files, loading 6
22:13:56,811 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:13:56,812 graphrag.index.run.run INFO Final # of rows loaded: 6
22:13:56,995 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:13:57,3 datashaper.workflow.workflow INFO executing verb orderby
22:13:57,8 datashaper.workflow.workflow INFO executing verb zip
22:13:57,13 datashaper.workflow.workflow INFO executing verb aggregate_override
22:13:57,22 datashaper.workflow.workflow INFO executing verb chunk
22:13:57,431 datashaper.workflow.workflow INFO executing verb select
22:13:57,439 datashaper.workflow.workflow INFO executing verb unroll
22:13:57,455 datashaper.workflow.workflow INFO executing verb rename
22:13:57,462 datashaper.workflow.workflow INFO executing verb genid
22:13:57,485 datashaper.workflow.workflow INFO executing verb unzip
22:13:57,495 datashaper.workflow.workflow INFO executing verb copy
22:13:57,503 datashaper.workflow.workflow INFO executing verb filter
22:13:57,525 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:13:57,813 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:13:57,814 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:13:57,862 datashaper.workflow.workflow INFO executing verb entity_extract
22:13:57,889 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:13:58,408 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:13:58,408 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:14:00,931 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:00,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.01500000001397. input_tokens=2126, output_tokens=148
22:14:02,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:02,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3910000000032596. input_tokens=34, output_tokens=108
22:14:02,342 datashaper.workflow.workflow INFO executing verb merge_graphs
22:14:02,530 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:14:02,759 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:14:02,759 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:14:02,798 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:14:04,618 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:04,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6410000000032596. input_tokens=423, output_tokens=142
22:14:04,703 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:14:04,989 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:14:04,990 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:14:05,39 datashaper.workflow.workflow INFO executing verb cluster_graph
22:14:05,813 datashaper.workflow.workflow INFO executing verb select
22:14:05,821 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:14:06,186 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:14:06,193 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:14:06,244 datashaper.workflow.workflow INFO executing verb unpack_graph
22:14:06,601 datashaper.workflow.workflow INFO executing verb rename
22:14:06,620 datashaper.workflow.workflow INFO executing verb select
22:14:06,647 datashaper.workflow.workflow INFO executing verb dedupe
22:14:06,677 datashaper.workflow.workflow INFO executing verb rename
22:14:06,705 datashaper.workflow.workflow INFO executing verb filter
22:14:06,772 datashaper.workflow.workflow INFO executing verb text_split
22:14:06,825 datashaper.workflow.workflow INFO executing verb drop
22:14:06,859 datashaper.workflow.workflow INFO executing verb merge
22:14:07,75 datashaper.workflow.workflow INFO executing verb text_embed
22:14:07,77 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:14:07,761 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:14:07,761 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:14:07,839 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 406 inputs via 406 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:14:09,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:14:09,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:14:09,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.360000000015134. input_tokens=168, output_tokens=0
22:14:09,308 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4530000000086147. input_tokens=1876, output_tokens=0
22:14:09,384 datashaper.workflow.workflow INFO executing verb drop
22:14:09,404 datashaper.workflow.workflow INFO executing verb filter
22:14:09,457 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:14:09,833 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:14:09,834 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:14:09,882 datashaper.workflow.workflow INFO executing verb layout_graph
22:14:11,130 datashaper.workflow.workflow INFO executing verb unpack_graph
22:14:11,545 datashaper.workflow.workflow INFO executing verb unpack_graph
22:14:12,2 datashaper.workflow.workflow INFO executing verb drop
22:14:12,26 datashaper.workflow.workflow INFO executing verb filter
22:14:12,155 datashaper.workflow.workflow INFO executing verb select
22:14:12,188 datashaper.workflow.workflow INFO executing verb rename
22:14:12,212 datashaper.workflow.workflow INFO executing verb convert
22:14:12,303 datashaper.workflow.workflow INFO executing verb join
22:14:12,351 datashaper.workflow.workflow INFO executing verb rename
22:14:12,357 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:14:12,733 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:14:12,734 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:14:12,790 datashaper.workflow.workflow INFO executing verb create_final_communities
22:14:13,628 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:14:13,924 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:14:13,925 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:14:13,945 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:14:14,24 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:14:14,454 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:14:14,466 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:14:14,793 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
22:14:14,798 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:14:14,818 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:14:14,873 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:14:14,930 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:14:15,1 datashaper.workflow.workflow INFO executing verb select
22:14:15,3 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:14:15,305 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:14:15,306 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:14:15,317 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:14:15,383 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:14:15,457 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:14:15,509 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:14:15,560 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:14:15,561 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 406
22:14:15,626 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 406
22:14:15,891 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 406
22:14:16,223 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 406
22:14:16,488 datashaper.workflow.workflow INFO executing verb create_community_reports
22:14:25,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:25,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.0. input_tokens=2412, output_tokens=722
22:14:31,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:31,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.437999999994645. input_tokens=2652, output_tokens=904
22:14:33,506 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:33,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.0. input_tokens=6965, output_tokens=951
22:14:33,862 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:33,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.32799999997951. input_tokens=5908, output_tokens=880
22:14:43,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:43,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.89000000001397. input_tokens=2150, output_tokens=558
22:14:45,570 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:45,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.73399999999674. input_tokens=9891, output_tokens=844
22:14:45,604 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:45,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.921999999991385. input_tokens=2550, output_tokens=672
22:14:46,3 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:46,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.35899999999674. input_tokens=3945, output_tokens=928
22:14:46,285 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:46,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.484000000025844. input_tokens=2636, output_tokens=728
22:14:46,584 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:46,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.76600000000326. input_tokens=2660, output_tokens=791
22:14:46,790 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:46,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.046999999991385. input_tokens=3736, output_tokens=767
22:14:47,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:47,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.48399999999674. input_tokens=6528, output_tokens=844
22:14:47,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:47,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.405999999988126. input_tokens=3258, output_tokens=807
22:14:47,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:47,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.60999999998603. input_tokens=4861, output_tokens=864
22:14:47,497 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:47,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.828000000008615. input_tokens=7552, output_tokens=878
22:14:47,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:47,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.140999999974156. input_tokens=4167, output_tokens=861
22:14:48,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:48,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.60899999999674. input_tokens=2122, output_tokens=605
22:14:48,892 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:48,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.062999999994645. input_tokens=3692, output_tokens=848
22:14:48,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:48,899 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.094000000011874. input_tokens=4787, output_tokens=941
22:14:49,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:49,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.25. input_tokens=2368, output_tokens=663
22:14:50,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:50,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.703000000008615. input_tokens=3342, output_tokens=777
22:14:50,889 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:50,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.10999999998603. input_tokens=2331, output_tokens=800
22:14:51,235 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:51,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.562000000005355. input_tokens=2254, output_tokens=677
22:14:52,717 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:52,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.01600000000326. input_tokens=3275, output_tokens=873
22:14:52,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:52,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.01500000001397. input_tokens=3297, output_tokens=908
22:14:52,890 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:52,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.15700000000652. input_tokens=3259, output_tokens=914
22:14:52,930 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:52,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.21799999999348. input_tokens=6285, output_tokens=955
22:14:59,956 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:14:59,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.671999999991385. input_tokens=2102, output_tokens=404
22:15:02,836 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:02,839 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.312999999994645. input_tokens=2079, output_tokens=662
22:15:03,31 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:03,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.53100000001723. input_tokens=2091, output_tokens=540
22:15:03,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:03,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.312999999994645. input_tokens=2225, output_tokens=573
22:15:03,759 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:03,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.40700000000652. input_tokens=2217, output_tokens=702
22:15:03,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:15:03,827 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:15:03,928 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:03,930 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.530999999988126. input_tokens=3880, output_tokens=744
22:15:03,975 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:03,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.530999999988126. input_tokens=5099, output_tokens=751
22:15:04,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:04,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.952999999979511. input_tokens=2870, output_tokens=763
22:15:04,443 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:04,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.953000000008615. input_tokens=2692, output_tokens=777
22:15:04,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:04,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.562999999994645. input_tokens=2277, output_tokens=674
22:15:05,29 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:05,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.65600000001723. input_tokens=3908, output_tokens=836
22:15:05,247 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:05,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.89100000000326. input_tokens=2971, output_tokens=861
22:15:05,288 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:05,290 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.905999999988126. input_tokens=3307, output_tokens=833
22:15:05,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:05,419 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.046999999991385. input_tokens=4543, output_tokens=854
22:15:05,421 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:05,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.155999999988126. input_tokens=4970, output_tokens=828
22:15:05,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:05,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.25. input_tokens=3541, output_tokens=894
22:15:06,559 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:06,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.14000000001397. input_tokens=3989, output_tokens=812
22:15:06,786 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:06,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.327999999979511. input_tokens=8316, output_tokens=887
22:15:07,372 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:07,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.89100000000326. input_tokens=3950, output_tokens=826
22:15:07,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:07,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.96799999999348. input_tokens=3268, output_tokens=771
22:15:07,862 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:07,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.546999999991385. input_tokens=7907, output_tokens=895
22:15:08,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:08,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.452999999979511. input_tokens=3075, output_tokens=885
22:15:09,816 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:09,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.875. input_tokens=2148, output_tokens=642
22:15:15,576 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:15,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.14100000000326. input_tokens=4295, output_tokens=768
22:15:17,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:17,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.42200000002049. input_tokens=8237, output_tokens=858
22:15:18,927 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:18,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.09299999999348. input_tokens=7547, output_tokens=832
22:15:19,480 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:19,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.046999999991385. input_tokens=4622, output_tokens=890
22:15:19,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:19,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.75. input_tokens=3975, output_tokens=848
22:15:21,207 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:21,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 16.078000000008615. input_tokens=9909, output_tokens=1109
22:15:29,236 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:29,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.764999999984866. input_tokens=2060, output_tokens=580
22:15:32,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:32,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.937000000005355. input_tokens=5094, output_tokens=759
22:15:33,247 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:33,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.75. input_tokens=9666, output_tokens=838
22:15:33,517 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:33,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.969000000011874. input_tokens=5967, output_tokens=891
22:15:34,183 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:34,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.71799999999348. input_tokens=2854, output_tokens=755
22:15:35,369 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:35,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.812999999994645. input_tokens=9457, output_tokens=849
22:15:35,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:35,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.98399999999674. input_tokens=8938, output_tokens=913
22:15:35,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:35,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.953000000008615. input_tokens=2340, output_tokens=806
22:15:35,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:35,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.375. input_tokens=3309, output_tokens=767
22:15:40,192 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:15:40,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.671999999991385. input_tokens=7115, output_tokens=1118
22:15:40,241 datashaper.workflow.workflow INFO executing verb window
22:15:40,244 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:15:40,488 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:15:40,488 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:15:40,550 datashaper.workflow.workflow INFO executing verb unroll
22:15:40,577 datashaper.workflow.workflow INFO executing verb select
22:15:40,607 datashaper.workflow.workflow INFO executing verb rename
22:15:40,632 datashaper.workflow.workflow INFO executing verb join
22:15:40,672 datashaper.workflow.workflow INFO executing verb aggregate_override
22:15:40,706 datashaper.workflow.workflow INFO executing verb join
22:15:40,741 datashaper.workflow.workflow INFO executing verb rename
22:15:40,768 datashaper.workflow.workflow INFO executing verb convert
22:15:40,863 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:15:41,93 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:15:41,93 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:15:41,160 datashaper.workflow.workflow INFO executing verb rename
22:15:41,162 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:15:41,303 graphrag.index.cli INFO All workflows completed successfully.
22:16:26,303 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:16:26,309 graphrag.index.cli INFO Starting pipeline run for: 20241201-221626, dryrun=False
22:16:26,309 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:16:26,313 graphrag.index.create_pipeline_config INFO skipping workflows 
22:16:26,314 graphrag.index.run.run INFO Running pipeline
22:16:26,314 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:16:26,316 graphrag.index.input.load_input INFO loading input from root_dir=input
22:16:26,316 graphrag.index.input.load_input INFO using file storage for input
22:16:26,319 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:16:26,321 graphrag.index.input.text INFO found text files from input, found [('4_copy0.txt', {}), ('4_copy1.txt', {}), ('4_copy2.txt', {}), ('4_copy3.txt', {}), ('4_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:16:26,351 graphrag.index.input.text INFO Found 6 files, loading 6
22:16:26,352 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:16:26,353 graphrag.index.run.run INFO Final # of rows loaded: 6
22:16:26,551 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:16:26,561 datashaper.workflow.workflow INFO executing verb orderby
22:16:26,566 datashaper.workflow.workflow INFO executing verb zip
22:16:26,573 datashaper.workflow.workflow INFO executing verb aggregate_override
22:16:26,584 datashaper.workflow.workflow INFO executing verb chunk
22:16:27,82 datashaper.workflow.workflow INFO executing verb select
22:16:27,92 datashaper.workflow.workflow INFO executing verb unroll
22:16:27,101 datashaper.workflow.workflow INFO executing verb rename
22:16:27,108 datashaper.workflow.workflow INFO executing verb genid
22:16:27,127 datashaper.workflow.workflow INFO executing verb unzip
22:16:27,137 datashaper.workflow.workflow INFO executing verb copy
22:16:27,146 datashaper.workflow.workflow INFO executing verb filter
22:16:27,168 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:16:27,457 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:16:27,457 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:16:27,501 datashaper.workflow.workflow INFO executing verb entity_extract
22:16:27,526 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:16:28,67 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:16:28,67 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:16:31,534 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:31,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.4219999999913853. input_tokens=2091, output_tokens=249
22:16:34,276 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:34,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.734000000025844. input_tokens=34, output_tokens=207
22:16:34,298 datashaper.workflow.workflow INFO executing verb merge_graphs
22:16:34,454 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:16:34,658 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:16:34,658 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:16:34,691 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:16:36,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:36,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:36,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4839999999967404. input_tokens=220, output_tokens=123
22:16:36,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1410000000032596. input_tokens=165, output_tokens=54
22:16:36,469 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:36,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2820000000065193. input_tokens=210, output_tokens=76
22:16:36,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:36,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.702999999979511. input_tokens=262, output_tokens=148
22:16:36,601 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:36,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.764999999984866. input_tokens=236, output_tokens=131
22:16:37,312 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:37,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9210000000020955. input_tokens=277, output_tokens=153
22:16:37,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:37,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2179999999934807. input_tokens=479, output_tokens=215
22:16:37,559 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:16:37,810 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:16:37,811 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:16:37,857 datashaper.workflow.workflow INFO executing verb cluster_graph
22:16:38,511 datashaper.workflow.workflow INFO executing verb select
22:16:38,518 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:16:38,761 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:16:38,762 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:16:38,806 datashaper.workflow.workflow INFO executing verb unpack_graph
22:16:39,141 datashaper.workflow.workflow INFO executing verb rename
22:16:39,156 datashaper.workflow.workflow INFO executing verb select
22:16:39,170 datashaper.workflow.workflow INFO executing verb dedupe
22:16:39,186 datashaper.workflow.workflow INFO executing verb rename
22:16:39,204 datashaper.workflow.workflow INFO executing verb filter
22:16:39,248 datashaper.workflow.workflow INFO executing verb text_split
22:16:39,274 datashaper.workflow.workflow INFO executing verb drop
22:16:39,293 datashaper.workflow.workflow INFO executing verb merge
22:16:39,429 datashaper.workflow.workflow INFO executing verb text_embed
22:16:39,431 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:16:39,928 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:16:39,928 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:16:39,999 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:16:40,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,385 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3909999999741558. input_tokens=868, output_tokens=0
22:16:40,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.42199999999138527. input_tokens=1420, output_tokens=0
22:16:40,485 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,489 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.45300000000861473. input_tokens=1327, output_tokens=0
22:16:40,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5150000000139698. input_tokens=415, output_tokens=0
22:16:40,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,602 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,603 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,604 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5620000000053551. input_tokens=606, output_tokens=0
22:16:40,651 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6090000000258442. input_tokens=832, output_tokens=0
22:16:40,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6559999999881256. input_tokens=661, output_tokens=0
22:16:40,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6560000000172295. input_tokens=612, output_tokens=0
22:16:40,742 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,744 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,745 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7190000000118744. input_tokens=883, output_tokens=0
22:16:40,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,808 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,809 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,811 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7960000000020955. input_tokens=917, output_tokens=0
22:16:40,851 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,875 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8589999999967404. input_tokens=495, output_tokens=0
22:16:40,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8590000000258442. input_tokens=854, output_tokens=0
22:16:40,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9370000000053551. input_tokens=614, output_tokens=0
22:16:40,982 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:40,983 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:41,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0. input_tokens=622, output_tokens=0
22:16:41,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9840000000258442. input_tokens=126, output_tokens=0
22:16:41,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0460000000020955. input_tokens=1275, output_tokens=0
22:16:41,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0929999999934807. input_tokens=1759, output_tokens=0
22:16:41,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1400000000139698. input_tokens=622, output_tokens=0
22:16:41,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1559999999881256. input_tokens=616, output_tokens=0
22:16:41,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:41,216 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:16:41,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.202999999979511. input_tokens=1268, output_tokens=0
22:16:41,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.25. input_tokens=925, output_tokens=0
22:16:41,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2809999999881256. input_tokens=1104, output_tokens=0
22:16:41,364 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.327999999979511. input_tokens=1018, output_tokens=0
22:16:41,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3900000000139698. input_tokens=1887, output_tokens=0
22:16:41,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3900000000139698. input_tokens=695, output_tokens=0
22:16:41,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0160000000032596. input_tokens=704, output_tokens=0
22:16:41,546 datashaper.workflow.workflow INFO executing verb drop
22:16:41,565 datashaper.workflow.workflow INFO executing verb filter
22:16:41,608 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:16:41,929 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:16:41,930 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:16:41,999 datashaper.workflow.workflow INFO executing verb layout_graph
22:16:43,41 datashaper.workflow.workflow INFO executing verb unpack_graph
22:16:43,321 datashaper.workflow.workflow INFO executing verb unpack_graph
22:16:43,596 datashaper.workflow.workflow INFO executing verb filter
22:16:43,662 datashaper.workflow.workflow INFO executing verb drop
22:16:43,680 datashaper.workflow.workflow INFO executing verb select
22:16:43,698 datashaper.workflow.workflow INFO executing verb rename
22:16:43,716 datashaper.workflow.workflow INFO executing verb convert
22:16:43,789 datashaper.workflow.workflow INFO executing verb join
22:16:43,835 datashaper.workflow.workflow INFO executing verb rename
22:16:43,839 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:16:44,101 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:16:44,115 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:16:44,167 datashaper.workflow.workflow INFO executing verb create_final_communities
22:16:44,814 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:16:45,72 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:16:45,72 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:16:45,92 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:16:45,142 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:16:45,492 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:16:45,503 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:16:45,754 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
22:16:45,755 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:16:45,770 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:16:45,777 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:16:45,874 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:16:45,930 datashaper.workflow.workflow INFO executing verb select
22:16:45,933 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:16:46,176 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:16:46,177 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:16:46,184 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:16:46,239 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:16:46,314 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:16:46,357 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:16:46,396 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:16:46,397 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 405
22:16:46,440 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
22:16:46,661 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
22:16:46,947 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
22:16:47,200 datashaper.workflow.workflow INFO executing verb create_community_reports
22:16:56,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:56,97 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.860000000015134. input_tokens=2550, output_tokens=650
22:16:58,900 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:16:58,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.687000000005355. input_tokens=7020, output_tokens=856
22:17:08,422 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:08,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.671999999991385. input_tokens=2122, output_tokens=482
22:17:10,330 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:10,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.594000000011874. input_tokens=2150, output_tokens=605
22:17:10,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:10,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.703000000008615. input_tokens=2368, output_tokens=669
22:17:10,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:10,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.078000000008615. input_tokens=2331, output_tokens=724
22:17:11,434 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:11,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.75. input_tokens=2390, output_tokens=766
22:17:11,711 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:11,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.110000000015134. input_tokens=2751, output_tokens=789
22:17:12,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:12,445 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.78100000001723. input_tokens=3607, output_tokens=828
22:17:12,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:12,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.952999999979511. input_tokens=3297, output_tokens=865
22:17:12,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:12,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.10899999999674. input_tokens=3807, output_tokens=875
22:17:12,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:12,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.328000000008615. input_tokens=2652, output_tokens=762
22:17:12,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:12,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.421999999991385. input_tokens=2561, output_tokens=766
22:17:13,149 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:13,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.578000000008615. input_tokens=4360, output_tokens=803
22:17:13,400 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:13,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.625. input_tokens=4779, output_tokens=865
22:17:13,745 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:13,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.98399999999674. input_tokens=3623, output_tokens=870
22:17:13,832 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:13,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.125. input_tokens=7229, output_tokens=815
22:17:13,972 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:13,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.375. input_tokens=2254, output_tokens=663
22:17:14,27 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:14,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.39000000001397. input_tokens=3338, output_tokens=830
22:17:14,35 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:14,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.35999999998603. input_tokens=4970, output_tokens=851
22:17:14,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:14,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.796999999991385. input_tokens=4864, output_tokens=893
22:17:14,691 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:14,694 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.046999999991385. input_tokens=6526, output_tokens=891
22:17:16,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:16,160 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.53100000001723. input_tokens=7470, output_tokens=898
22:17:17,321 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:17,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.594000000011874. input_tokens=3749, output_tokens=837
22:17:17,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:17,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.562999999994645. input_tokens=9811, output_tokens=953
22:17:18,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:18,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.375. input_tokens=9899, output_tokens=851
22:17:19,638 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:19,642 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.89100000000326. input_tokens=3344, output_tokens=971
22:17:20,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:20,442 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:20,455 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:20,456 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:20,614 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:20,617 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:20,627 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:20,628 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:22,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:22,117 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:22,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:22,531 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:22,565 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:22,566 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:24,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:24,962 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:25,130 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:25,131 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:28,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:28,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.10999999998603. input_tokens=2102, output_tokens=547
22:17:29,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:29,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.062000000005355. input_tokens=2079, output_tokens=620
22:17:29,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:29,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.73499999998603. input_tokens=2225, output_tokens=583
22:17:29,820 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:29,820 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:29,831 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:29,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.812000000005355. input_tokens=2148, output_tokens=658
22:17:30,17 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:17:30,18 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:17:30,587 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:30,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.703000000008615. input_tokens=2448, output_tokens=777
22:17:30,846 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:30,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:30,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.014999999984866. input_tokens=2636, output_tokens=714
22:17:30,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.952999999979511. input_tokens=2217, output_tokens=741
22:17:30,993 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:30,996 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.155999999988126. input_tokens=3956, output_tokens=796
22:17:31,181 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:31,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.25. input_tokens=2277, output_tokens=704
22:17:31,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:31,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.39000000001397. input_tokens=3617, output_tokens=825
22:17:31,612 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:31,617 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.64100000000326. input_tokens=2892, output_tokens=760
22:17:31,668 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:31,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.812999999994645. input_tokens=2692, output_tokens=753
22:17:31,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:31,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.10899999999674. input_tokens=8211, output_tokens=851
22:17:32,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:32,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.10899999999674. input_tokens=3414, output_tokens=827
22:17:32,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:32,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.5. input_tokens=5350, output_tokens=922
22:17:32,972 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:32,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.094000000011874. input_tokens=8292, output_tokens=919
22:17:33,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:33,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.047000000020489. input_tokens=3289, output_tokens=806
22:17:34,1 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:34,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 11.65700000000652. input_tokens=3201, output_tokens=848
22:17:34,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:34,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.21899999998277. input_tokens=2971, output_tokens=932
22:17:37,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:37,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.219000000011874. input_tokens=3950, output_tokens=845
22:17:39,669 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:39,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.640999999974156. input_tokens=6470, output_tokens=964
22:17:41,484 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:41,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.514999999984866. input_tokens=5049, output_tokens=872
22:17:41,937 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:41,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 17.625. input_tokens=3830, output_tokens=972
22:17:42,437 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:42,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.375. input_tokens=3681, output_tokens=872
22:17:42,627 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:42,631 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 11.546999999991385. input_tokens=7357, output_tokens=826
22:17:43,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:43,198 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.14100000000326. input_tokens=4421, output_tokens=951
22:17:44,346 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:44,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 14.985000000015134. input_tokens=3536, output_tokens=946
22:17:49,345 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:49,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 10.64100000000326. input_tokens=6298, output_tokens=886
22:17:58,670 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:17:58,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.062999999994645. input_tokens=4378, output_tokens=723
22:18:00,149 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:18:00,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.578000000008615. input_tokens=2060, output_tokens=643
22:18:00,262 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:18:00,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.687999999994645. input_tokens=2340, output_tokens=645
22:18:01,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:18:01,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.51600000000326. input_tokens=6905, output_tokens=836
22:18:02,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:18:02,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.0. input_tokens=3518, output_tokens=922
22:18:02,632 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:18:02,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.0. input_tokens=5550, output_tokens=935
22:18:04,5 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:18:04,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.453999999997905. input_tokens=2834, output_tokens=741
22:18:05,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:18:05,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.98499999998603. input_tokens=7355, output_tokens=949
22:18:07,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:18:07,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.985000000015134. input_tokens=7673, output_tokens=962
22:18:12,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:18:12,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.953999999997905. input_tokens=9143, output_tokens=914
22:18:12,580 datashaper.workflow.workflow INFO executing verb window
22:18:12,584 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:18:12,901 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:18:12,902 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:18:12,980 datashaper.workflow.workflow INFO executing verb unroll
22:18:13,32 datashaper.workflow.workflow INFO executing verb select
22:18:13,63 datashaper.workflow.workflow INFO executing verb rename
22:18:13,98 datashaper.workflow.workflow INFO executing verb join
22:18:13,143 datashaper.workflow.workflow INFO executing verb aggregate_override
22:18:13,181 datashaper.workflow.workflow INFO executing verb join
22:18:13,234 datashaper.workflow.workflow INFO executing verb rename
22:18:13,276 datashaper.workflow.workflow INFO executing verb convert
22:18:13,471 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:18:13,778 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:18:13,779 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:18:13,872 datashaper.workflow.workflow INFO executing verb rename
22:18:13,874 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:18:14,37 graphrag.index.cli INFO All workflows completed successfully.
22:19:00,502 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:19:00,506 graphrag.index.cli INFO Starting pipeline run for: 20241201-221900, dryrun=False
22:19:00,507 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:19:00,510 graphrag.index.create_pipeline_config INFO skipping workflows 
22:19:00,510 graphrag.index.run.run INFO Running pipeline
22:19:00,510 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:19:00,512 graphrag.index.input.load_input INFO loading input from root_dir=input
22:19:00,512 graphrag.index.input.load_input INFO using file storage for input
22:19:00,514 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:19:00,515 graphrag.index.input.text INFO found text files from input, found [('5_copy0.txt', {}), ('5_copy1.txt', {}), ('5_copy2.txt', {}), ('5_copy3.txt', {}), ('5_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:19:00,542 graphrag.index.input.text INFO Found 6 files, loading 6
22:19:00,545 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:19:00,545 graphrag.index.run.run INFO Final # of rows loaded: 6
22:19:00,706 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:19:00,713 datashaper.workflow.workflow INFO executing verb orderby
22:19:00,717 datashaper.workflow.workflow INFO executing verb zip
22:19:00,722 datashaper.workflow.workflow INFO executing verb aggregate_override
22:19:00,730 datashaper.workflow.workflow INFO executing verb chunk
22:19:01,129 datashaper.workflow.workflow INFO executing verb select
22:19:01,136 datashaper.workflow.workflow INFO executing verb unroll
22:19:01,143 datashaper.workflow.workflow INFO executing verb rename
22:19:01,152 datashaper.workflow.workflow INFO executing verb genid
22:19:01,168 datashaper.workflow.workflow INFO executing verb unzip
22:19:01,176 datashaper.workflow.workflow INFO executing verb copy
22:19:01,186 datashaper.workflow.workflow INFO executing verb filter
22:19:01,210 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:19:01,434 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:19:01,434 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:19:01,472 datashaper.workflow.workflow INFO executing verb entity_extract
22:19:01,488 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:19:02,6 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:19:02,6 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:19:04,776 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:04,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.297000000020489. input_tokens=2106, output_tokens=171
22:19:06,237 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:06,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.452999999979511. input_tokens=34, output_tokens=135
22:19:06,261 datashaper.workflow.workflow INFO executing verb merge_graphs
22:19:06,454 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:19:06,689 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:19:06,693 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:19:06,732 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:19:08,915 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:08,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3599999999860302. input_tokens=184, output_tokens=64
22:19:09,6 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:09,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9539999999979045. input_tokens=207, output_tokens=107
22:19:10,687 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:10,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.75. input_tokens=1465, output_tokens=343
22:19:11,238 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:11,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6719999999913853. input_tokens=276, output_tokens=150
22:19:11,324 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:19:11,538 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:19:11,538 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:19:11,577 datashaper.workflow.workflow INFO executing verb cluster_graph
22:19:12,281 datashaper.workflow.workflow INFO executing verb select
22:19:12,286 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:19:12,577 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:19:12,578 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:19:12,624 datashaper.workflow.workflow INFO executing verb unpack_graph
22:19:12,966 datashaper.workflow.workflow INFO executing verb rename
22:19:12,983 datashaper.workflow.workflow INFO executing verb select
22:19:13,0 datashaper.workflow.workflow INFO executing verb dedupe
22:19:13,23 datashaper.workflow.workflow INFO executing verb rename
22:19:13,40 datashaper.workflow.workflow INFO executing verb filter
22:19:13,98 datashaper.workflow.workflow INFO executing verb text_split
22:19:13,133 datashaper.workflow.workflow INFO executing verb drop
22:19:13,153 datashaper.workflow.workflow INFO executing verb merge
22:19:13,329 datashaper.workflow.workflow INFO executing verb text_embed
22:19:13,332 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:19:13,875 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:19:13,875 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:19:13,979 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 407 inputs via 407 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:19:14,884 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:19:15,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0160000000032596. input_tokens=149, output_tokens=0
22:19:15,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:19:15,50 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:19:15,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2339999999967404. input_tokens=1839, output_tokens=0
22:19:15,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2660000000032596. input_tokens=1031, output_tokens=0
22:19:15,335 datashaper.workflow.workflow INFO executing verb drop
22:19:15,362 datashaper.workflow.workflow INFO executing verb filter
22:19:15,398 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:19:15,799 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:19:15,800 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:19:15,848 datashaper.workflow.workflow INFO executing verb layout_graph
22:19:17,157 datashaper.workflow.workflow INFO executing verb unpack_graph
22:19:17,533 datashaper.workflow.workflow INFO executing verb unpack_graph
22:19:17,929 datashaper.workflow.workflow INFO executing verb filter
22:19:18,68 datashaper.workflow.workflow INFO executing verb drop
22:19:18,96 datashaper.workflow.workflow INFO executing verb select
22:19:18,144 datashaper.workflow.workflow INFO executing verb rename
22:19:18,170 datashaper.workflow.workflow INFO executing verb join
22:19:18,212 datashaper.workflow.workflow INFO executing verb convert
22:19:18,301 datashaper.workflow.workflow INFO executing verb rename
22:19:18,306 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:19:18,620 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:19:18,621 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:19:18,675 datashaper.workflow.workflow INFO executing verb create_final_communities
22:19:19,559 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:19:20,56 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:19:20,56 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:19:20,77 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:19:20,143 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:19:20,586 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:19:20,598 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:19:20,839 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
22:19:20,839 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:19:20,847 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:19:20,862 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:19:20,948 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:19:21,7 datashaper.workflow.workflow INFO executing verb select
22:19:21,10 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:19:21,274 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:19:21,283 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:19:21,291 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:19:21,342 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:19:21,389 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:19:21,429 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:19:21,463 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:19:21,464 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 407
22:19:21,502 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 407
22:19:21,691 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 407
22:19:21,938 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 407
22:19:22,134 datashaper.workflow.workflow INFO executing verb create_community_reports
22:19:31,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:31,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.952999999979511. input_tokens=3637, output_tokens=775
22:19:38,47 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:38,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.890999999974156. input_tokens=6831, output_tokens=858
22:19:48,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:48,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.672000000020489. input_tokens=2150, output_tokens=624
22:19:49,369 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:49,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.35899999999674. input_tokens=2550, output_tokens=665
22:19:49,551 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:49,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.546000000002095. input_tokens=2181, output_tokens=599
22:19:50,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:50,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.78100000001723. input_tokens=4274, output_tokens=883
22:19:50,868 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:50,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.98399999999674. input_tokens=2470, output_tokens=678
22:19:51,18 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:51,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.14100000000326. input_tokens=4245, output_tokens=763
22:19:51,765 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:51,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.812999999994645. input_tokens=3607, output_tokens=848
22:19:51,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:51,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.96899999998277. input_tokens=8245, output_tokens=774
22:19:52,186 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:52,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.25. input_tokens=6540, output_tokens=1019
22:19:52,316 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:52,320 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.328000000008615. input_tokens=2652, output_tokens=798
22:19:53,348 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:53,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.452999999979511. input_tokens=3275, output_tokens=826
22:19:53,384 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:53,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.39100000000326. input_tokens=3623, output_tokens=930
22:19:53,853 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:53,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.780999999988126. input_tokens=9890, output_tokens=833
22:19:54,72 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:54,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.03100000001723. input_tokens=9885, output_tokens=858
22:19:54,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:54,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.671999999991385. input_tokens=4973, output_tokens=823
22:19:55,970 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:19:55,977 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.937999999994645. input_tokens=7686, output_tokens=939
22:20:02,696 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:02,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.4060000000172295. input_tokens=2102, output_tokens=540
22:20:03,710 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:03,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.453000000008615. input_tokens=2225, output_tokens=559
22:20:03,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:03,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.578000000008615. input_tokens=2368, output_tokens=658
22:20:03,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:03,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.639999999984866. input_tokens=2515, output_tokens=658
22:20:04,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:04,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.7189999999827705. input_tokens=2834, output_tokens=666
22:20:04,160 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:04,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.812000000005355. input_tokens=2122, output_tokens=590
22:20:05,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:05,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.844000000011874. input_tokens=2277, output_tokens=669
22:20:05,297 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:05,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.890999999974156. input_tokens=2254, output_tokens=753
22:20:06,137 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:06,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.703999999997905. input_tokens=2340, output_tokens=738
22:20:06,220 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:06,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.90700000000652. input_tokens=3891, output_tokens=754
22:20:06,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:06,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.844000000011874. input_tokens=2217, output_tokens=749
22:20:06,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:06,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.10899999999674. input_tokens=2870, output_tokens=759
22:20:06,444 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:06,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.171999999991385. input_tokens=2971, output_tokens=765
22:20:06,928 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:06,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.562000000005355. input_tokens=3006, output_tokens=811
22:20:07,553 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:07,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.21799999999348. input_tokens=3459, output_tokens=784
22:20:07,798 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:07,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.437999999994645. input_tokens=3785, output_tokens=826
22:20:07,927 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:07,930 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.73399999999674. input_tokens=3956, output_tokens=845
22:20:07,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:07,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.687999999994645. input_tokens=4970, output_tokens=929
22:20:08,74 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:08,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.687000000005355. input_tokens=8932, output_tokens=889
22:20:08,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:08,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.047000000020489. input_tokens=7790, output_tokens=906
22:20:08,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:08,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.15700000000652. input_tokens=3518, output_tokens=929
22:20:08,617 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:08,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.405999999988126. input_tokens=4073, output_tokens=915
22:20:08,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:08,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.360000000015134. input_tokens=5516, output_tokens=931
22:20:10,889 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:10,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.187999999994645. input_tokens=3387, output_tokens=795
22:20:10,894 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:10,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.562999999994645. input_tokens=5976, output_tokens=929
22:20:11,363 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:11,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.98399999999674. input_tokens=3289, output_tokens=843
22:20:12,851 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:12,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.0. input_tokens=4316, output_tokens=823
22:20:14,669 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:14,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.546999999991385. input_tokens=5804, output_tokens=773
22:20:15,558 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:15,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.84299999999348. input_tokens=6419, output_tokens=982
22:20:15,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:15,611 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.764999999984866. input_tokens=3200, output_tokens=871
22:20:21,691 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:21,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.937000000005355. input_tokens=2060, output_tokens=576
22:20:26,589 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:26,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.828000000008615. input_tokens=3379, output_tokens=854
22:20:26,883 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:26,885 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.078999999997905. input_tokens=6045, output_tokens=883
22:20:27,732 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:27,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.96799999999348. input_tokens=8836, output_tokens=904
22:20:28,102 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:28,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.327999999979511. input_tokens=8907, output_tokens=922
22:20:28,633 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:28,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.844000000011874. input_tokens=6918, output_tokens=939
22:20:28,672 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:28,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.875. input_tokens=6814, output_tokens=960
22:20:28,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:20:28,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.094000000011874. input_tokens=8352, output_tokens=925
22:20:28,955 datashaper.workflow.workflow INFO executing verb window
22:20:28,958 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:20:29,301 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:20:29,302 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:20:29,371 datashaper.workflow.workflow INFO executing verb unroll
22:20:29,407 datashaper.workflow.workflow INFO executing verb select
22:20:29,439 datashaper.workflow.workflow INFO executing verb rename
22:20:29,474 datashaper.workflow.workflow INFO executing verb join
22:20:29,516 datashaper.workflow.workflow INFO executing verb aggregate_override
22:20:29,560 datashaper.workflow.workflow INFO executing verb join
22:20:29,601 datashaper.workflow.workflow INFO executing verb rename
22:20:29,634 datashaper.workflow.workflow INFO executing verb convert
22:20:29,839 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:20:30,176 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:20:30,177 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:20:30,278 datashaper.workflow.workflow INFO executing verb rename
22:20:30,282 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:20:30,463 graphrag.index.cli INFO All workflows completed successfully.
22:21:17,216 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:21:17,220 graphrag.index.cli INFO Starting pipeline run for: 20241201-222117, dryrun=False
22:21:17,221 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:21:17,224 graphrag.index.create_pipeline_config INFO skipping workflows 
22:21:17,225 graphrag.index.run.run INFO Running pipeline
22:21:17,225 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:21:17,226 graphrag.index.input.load_input INFO loading input from root_dir=input
22:21:17,226 graphrag.index.input.load_input INFO using file storage for input
22:21:17,230 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:21:17,231 graphrag.index.input.text INFO found text files from input, found [('6_copy0.txt', {}), ('6_copy1.txt', {}), ('6_copy2.txt', {}), ('6_copy3.txt', {}), ('6_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:21:17,253 graphrag.index.input.text INFO Found 6 files, loading 6
22:21:17,255 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:21:17,255 graphrag.index.run.run INFO Final # of rows loaded: 6
22:21:17,422 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:21:17,432 datashaper.workflow.workflow INFO executing verb orderby
22:21:17,440 datashaper.workflow.workflow INFO executing verb zip
22:21:17,447 datashaper.workflow.workflow INFO executing verb aggregate_override
22:21:17,455 datashaper.workflow.workflow INFO executing verb chunk
22:21:17,915 datashaper.workflow.workflow INFO executing verb select
22:21:17,926 datashaper.workflow.workflow INFO executing verb unroll
22:21:17,934 datashaper.workflow.workflow INFO executing verb rename
22:21:17,941 datashaper.workflow.workflow INFO executing verb genid
22:21:17,963 datashaper.workflow.workflow INFO executing verb unzip
22:21:17,972 datashaper.workflow.workflow INFO executing verb copy
22:21:17,980 datashaper.workflow.workflow INFO executing verb filter
22:21:18,4 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:21:18,220 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:21:18,227 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:21:18,268 datashaper.workflow.workflow INFO executing verb entity_extract
22:21:18,288 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:21:18,806 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:21:18,807 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:21:23,990 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:23,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.625. input_tokens=2090, output_tokens=298
22:21:25,395 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:25,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4060000000172295. input_tokens=34, output_tokens=90
22:21:25,422 datashaper.workflow.workflow INFO executing verb merge_graphs
22:21:25,614 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:21:25,866 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:21:25,867 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:21:25,902 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:21:27,345 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:27,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.235000000015134. input_tokens=261, output_tokens=81
22:21:27,755 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:27,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=198, output_tokens=74
22:21:28,525 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:28,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.437000000005355. input_tokens=373, output_tokens=204
22:21:28,558 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:28,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.01500000001397. input_tokens=479, output_tokens=163
22:21:28,815 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:28,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2960000000020955. input_tokens=237, output_tokens=118
22:21:29,285 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:29,286 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.187000000005355. input_tokens=964, output_tokens=305
22:21:29,341 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:21:29,536 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:21:29,537 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:21:29,571 datashaper.workflow.workflow INFO executing verb cluster_graph
22:21:30,231 datashaper.workflow.workflow INFO executing verb select
22:21:30,237 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:21:30,520 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:21:30,521 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:21:30,578 datashaper.workflow.workflow INFO executing verb unpack_graph
22:21:30,824 datashaper.workflow.workflow INFO executing verb rename
22:21:30,835 datashaper.workflow.workflow INFO executing verb select
22:21:30,848 datashaper.workflow.workflow INFO executing verb dedupe
22:21:30,864 datashaper.workflow.workflow INFO executing verb rename
22:21:30,878 datashaper.workflow.workflow INFO executing verb filter
22:21:30,919 datashaper.workflow.workflow INFO executing verb text_split
22:21:30,941 datashaper.workflow.workflow INFO executing verb drop
22:21:30,956 datashaper.workflow.workflow INFO executing verb merge
22:21:31,117 datashaper.workflow.workflow INFO executing verb text_embed
22:21:31,119 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:21:31,623 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:21:31,623 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:21:31,688 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:21:32,412 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:21:32,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8129999999946449. input_tokens=1890, output_tokens=0
22:21:32,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:21:32,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8910000000032596. input_tokens=136, output_tokens=0
22:21:32,627 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:21:32,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:21:32,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.125. input_tokens=1941, output_tokens=0
22:21:32,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1410000000032596. input_tokens=602, output_tokens=0
22:21:32,919 datashaper.workflow.workflow INFO executing verb drop
22:21:32,935 datashaper.workflow.workflow INFO executing verb filter
22:21:32,967 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:21:33,333 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:21:33,333 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:21:33,380 datashaper.workflow.workflow INFO executing verb layout_graph
22:21:34,490 datashaper.workflow.workflow INFO executing verb unpack_graph
22:21:34,946 datashaper.workflow.workflow INFO executing verb unpack_graph
22:21:35,440 datashaper.workflow.workflow INFO executing verb drop
22:21:35,464 datashaper.workflow.workflow INFO executing verb filter
22:21:35,558 datashaper.workflow.workflow INFO executing verb select
22:21:35,579 datashaper.workflow.workflow INFO executing verb rename
22:21:35,600 datashaper.workflow.workflow INFO executing verb convert
22:21:35,685 datashaper.workflow.workflow INFO executing verb join
22:21:35,726 datashaper.workflow.workflow INFO executing verb rename
22:21:35,729 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:21:36,2 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:21:36,3 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:21:36,56 datashaper.workflow.workflow INFO executing verb create_final_communities
22:21:36,751 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:21:37,86 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:21:37,86 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:21:37,107 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:21:37,164 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:21:37,503 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:21:37,514 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:21:37,753 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
22:21:37,754 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:21:37,769 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:21:37,777 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:21:37,875 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:21:37,968 datashaper.workflow.workflow INFO executing verb select
22:21:37,971 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:21:38,238 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:21:38,238 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:21:38,248 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:21:38,307 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:21:38,366 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:21:38,408 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:21:38,454 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:21:38,455 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 405
22:21:38,509 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
22:21:38,708 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
22:21:39,65 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
22:21:39,313 datashaper.workflow.workflow INFO executing verb create_community_reports
22:21:47,956 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:47,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:47,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.594000000011874. input_tokens=2550, output_tokens=700
22:21:47,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.64100000000326. input_tokens=2181, output_tokens=739
22:21:50,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:21:50,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.687000000005355. input_tokens=7604, output_tokens=888
22:22:01,774 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:01,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.03100000001723. input_tokens=2515, output_tokens=686
22:22:02,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:02,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.25. input_tokens=2390, output_tokens=703
22:22:02,510 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:02,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.64100000000326. input_tokens=3623, output_tokens=791
22:22:03,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:03,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.51500000001397. input_tokens=2254, output_tokens=799
22:22:03,369 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:03,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.51600000000326. input_tokens=3259, output_tokens=824
22:22:03,433 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:03,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.671000000002095. input_tokens=3258, output_tokens=830
22:22:03,481 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:03,485 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.671999999991385. input_tokens=6804, output_tokens=811
22:22:03,497 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:03,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.76600000000326. input_tokens=4252, output_tokens=826
22:22:03,619 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:03,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.828000000008615. input_tokens=8263, output_tokens=893
22:22:04,288 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:04,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.515999999974156. input_tokens=5516, output_tokens=845
22:22:04,384 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:04,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.546999999991385. input_tokens=3938, output_tokens=890
22:22:04,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:04,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.047000000020489. input_tokens=4669, output_tokens=925
22:22:04,962 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:04,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.14100000000326. input_tokens=4962, output_tokens=941
22:22:05,546 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:05,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.655999999988126. input_tokens=9854, output_tokens=967
22:22:05,713 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:05,716 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.89100000000326. input_tokens=4861, output_tokens=796
22:22:07,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:07,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.296000000002095. input_tokens=9891, output_tokens=1022
22:22:08,45 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:22:08,46 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:22:09,731 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:22:09,732 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:22:15,237 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:15,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.75. input_tokens=2122, output_tokens=613
22:22:15,407 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:15,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.875. input_tokens=2368, output_tokens=630
22:22:15,815 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:15,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.375. input_tokens=2834, output_tokens=677
22:22:15,913 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:15,915 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.375. input_tokens=2225, output_tokens=638
22:22:16,480 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:16,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.062000000005355. input_tokens=2217, output_tokens=722
22:22:16,516 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:16,518 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.10999999998603. input_tokens=4295, output_tokens=718
22:22:17,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:17,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.90600000001723. input_tokens=3201, output_tokens=791
22:22:17,434 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:17,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.96799999999348. input_tokens=3414, output_tokens=812
22:22:17,827 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:17,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.312000000005355. input_tokens=3353, output_tokens=828
22:22:18,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:18,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.719000000011874. input_tokens=3297, output_tokens=749
22:22:18,110 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:18,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.60899999999674. input_tokens=2892, output_tokens=841
22:22:18,133 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:18,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.60999999998603. input_tokens=6380, output_tokens=840
22:22:18,180 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:18,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.812000000005355. input_tokens=4722, output_tokens=885
22:22:19,88 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:19,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.625. input_tokens=3830, output_tokens=820
22:22:19,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:19,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.76500000001397. input_tokens=2971, output_tokens=829
22:22:19,448 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:19,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.969000000011874. input_tokens=4820, output_tokens=806
22:22:20,62 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:20,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.655999999988126. input_tokens=8498, output_tokens=836
22:22:20,325 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:20,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.875. input_tokens=9787, output_tokens=849
22:22:20,552 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:20,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.14000000001397. input_tokens=4021, output_tokens=879
22:22:21,132 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:21,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.578999999997905. input_tokens=6291, output_tokens=1038
22:22:23,77 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:23,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.687000000005355. input_tokens=6688, output_tokens=1020
22:22:23,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:23,313 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.85899999999674. input_tokens=7245, output_tokens=983
22:22:31,276 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:31,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 19.03100000001723. input_tokens=5509, output_tokens=967
22:22:42,184 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:42,186 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.687000000005355. input_tokens=3309, output_tokens=768
22:22:42,893 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:42,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.375. input_tokens=4539, output_tokens=816
22:22:43,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:43,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.719000000011874. input_tokens=8510, output_tokens=844
22:22:43,469 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:43,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.90700000000652. input_tokens=8596, output_tokens=833
22:22:43,594 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:43,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.078000000008615. input_tokens=4378, output_tokens=877
22:22:44,541 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:44,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.96899999998277. input_tokens=8969, output_tokens=977
22:22:45,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:45,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.327999999979511. input_tokens=9230, output_tokens=990
22:22:47,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:22:47,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.60899999999674. input_tokens=8836, output_tokens=915
22:22:47,170 datashaper.workflow.workflow INFO executing verb window
22:22:47,173 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:22:47,529 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:22:47,530 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:22:47,609 datashaper.workflow.workflow INFO executing verb unroll
22:22:47,653 datashaper.workflow.workflow INFO executing verb select
22:22:47,692 datashaper.workflow.workflow INFO executing verb rename
22:22:47,725 datashaper.workflow.workflow INFO executing verb join
22:22:47,768 datashaper.workflow.workflow INFO executing verb aggregate_override
22:22:47,802 datashaper.workflow.workflow INFO executing verb join
22:22:47,852 datashaper.workflow.workflow INFO executing verb rename
22:22:47,884 datashaper.workflow.workflow INFO executing verb convert
22:22:48,1 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:22:48,256 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:22:48,257 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:22:48,367 datashaper.workflow.workflow INFO executing verb rename
22:22:48,372 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:22:48,514 graphrag.index.cli INFO All workflows completed successfully.
22:23:35,368 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:23:35,373 graphrag.index.cli INFO Starting pipeline run for: 20241201-222335, dryrun=False
22:23:35,374 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:23:35,378 graphrag.index.create_pipeline_config INFO skipping workflows 
22:23:35,378 graphrag.index.run.run INFO Running pipeline
22:23:35,378 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:23:35,380 graphrag.index.input.load_input INFO loading input from root_dir=input
22:23:35,380 graphrag.index.input.load_input INFO using file storage for input
22:23:35,383 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:23:35,385 graphrag.index.input.text INFO found text files from input, found [('7_copy0.txt', {}), ('7_copy1.txt', {}), ('7_copy2.txt', {}), ('7_copy3.txt', {}), ('7_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:23:35,413 graphrag.index.input.text INFO Found 6 files, loading 6
22:23:35,418 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:23:35,419 graphrag.index.run.run INFO Final # of rows loaded: 6
22:23:35,583 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:23:35,591 datashaper.workflow.workflow INFO executing verb orderby
22:23:35,596 datashaper.workflow.workflow INFO executing verb zip
22:23:35,601 datashaper.workflow.workflow INFO executing verb aggregate_override
22:23:35,609 datashaper.workflow.workflow INFO executing verb chunk
22:23:36,43 datashaper.workflow.workflow INFO executing verb select
22:23:36,50 datashaper.workflow.workflow INFO executing verb unroll
22:23:36,58 datashaper.workflow.workflow INFO executing verb rename
22:23:36,65 datashaper.workflow.workflow INFO executing verb genid
22:23:36,82 datashaper.workflow.workflow INFO executing verb unzip
22:23:36,90 datashaper.workflow.workflow INFO executing verb copy
22:23:36,98 datashaper.workflow.workflow INFO executing verb filter
22:23:36,121 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:23:36,351 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:23:36,352 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:23:36,401 datashaper.workflow.workflow INFO executing verb entity_extract
22:23:36,421 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:23:36,899 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:23:36,899 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:23:40,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:23:40,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.687000000005355. input_tokens=2106, output_tokens=224
22:23:43,364 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:23:43,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.327999999979511. input_tokens=34, output_tokens=263
22:23:43,385 datashaper.workflow.workflow INFO executing verb merge_graphs
22:23:43,567 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:23:43,769 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:23:43,770 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:23:43,805 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:23:45,378 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:23:45,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.187999999994645. input_tokens=195, output_tokens=76
22:23:46,468 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:23:46,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0780000000086147. input_tokens=479, output_tokens=192
22:23:46,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:23:46,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5469999999913853. input_tokens=1174, output_tokens=265
22:23:46,595 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:23:46,786 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:23:46,787 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:23:46,820 datashaper.workflow.workflow INFO executing verb cluster_graph
22:23:47,396 datashaper.workflow.workflow INFO executing verb select
22:23:47,401 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:23:47,658 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:23:47,659 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:23:47,704 datashaper.workflow.workflow INFO executing verb unpack_graph
22:23:48,2 datashaper.workflow.workflow INFO executing verb rename
22:23:48,13 datashaper.workflow.workflow INFO executing verb select
22:23:48,28 datashaper.workflow.workflow INFO executing verb dedupe
22:23:48,46 datashaper.workflow.workflow INFO executing verb rename
22:23:48,69 datashaper.workflow.workflow INFO executing verb filter
22:23:48,126 datashaper.workflow.workflow INFO executing verb text_split
22:23:48,159 datashaper.workflow.workflow INFO executing verb drop
22:23:48,177 datashaper.workflow.workflow INFO executing verb merge
22:23:48,338 datashaper.workflow.workflow INFO executing verb text_embed
22:23:48,340 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:23:48,855 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:23:48,855 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:23:48,917 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 407 inputs via 407 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:23:49,782 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:23:49,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9689999999827705. input_tokens=169, output_tokens=0
22:23:49,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:23:50,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2349999999860302. input_tokens=885, output_tokens=0
22:23:50,203 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:23:50,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4539999999979045. input_tokens=1957, output_tokens=0
22:23:50,448 datashaper.workflow.workflow INFO executing verb drop
22:23:50,467 datashaper.workflow.workflow INFO executing verb filter
22:23:50,498 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:23:50,869 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:23:50,870 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:23:50,920 datashaper.workflow.workflow INFO executing verb layout_graph
22:23:52,11 datashaper.workflow.workflow INFO executing verb unpack_graph
22:23:52,395 datashaper.workflow.workflow INFO executing verb unpack_graph
22:23:52,737 datashaper.workflow.workflow INFO executing verb drop
22:23:52,755 datashaper.workflow.workflow INFO executing verb filter
22:23:52,840 datashaper.workflow.workflow INFO executing verb select
22:23:52,863 datashaper.workflow.workflow INFO executing verb rename
22:23:52,885 datashaper.workflow.workflow INFO executing verb convert
22:23:52,986 datashaper.workflow.workflow INFO executing verb join
22:23:53,30 datashaper.workflow.workflow INFO executing verb rename
22:23:53,35 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:23:53,294 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:23:53,295 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:23:53,345 datashaper.workflow.workflow INFO executing verb create_final_communities
22:23:54,20 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:23:54,266 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:23:54,266 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:23:54,280 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:23:54,341 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:23:54,690 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:23:54,709 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:23:55,29 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
22:23:55,64 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:23:55,84 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:23:55,94 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:23:55,275 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:23:55,418 datashaper.workflow.workflow INFO executing verb select
22:23:55,424 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:23:55,761 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:23:55,762 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:23:55,773 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:23:55,850 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:23:55,927 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:23:55,974 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:23:56,21 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:23:56,22 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 407
22:23:56,197 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 407
22:23:56,434 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 407
22:23:56,715 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 407
22:23:56,964 datashaper.workflow.workflow INFO executing verb create_community_reports
22:24:07,739 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:07,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.35899999999674. input_tokens=3722, output_tokens=815
22:24:07,929 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:07,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.5. input_tokens=3924, output_tokens=832
22:24:08,52 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:08,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.64000000001397. input_tokens=2652, output_tokens=880
22:24:08,712 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:08,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.28200000000652. input_tokens=2550, output_tokens=671
22:24:10,242 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:10,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.796999999991385. input_tokens=9855, output_tokens=848
22:24:10,618 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:10,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.23399999999674. input_tokens=6543, output_tokens=782
22:24:12,119 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:12,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.703000000008615. input_tokens=6965, output_tokens=864
22:24:21,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:21,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.171999999991385. input_tokens=2254, output_tokens=597
22:24:22,4 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:22,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.671999999991385. input_tokens=2368, output_tokens=632
22:24:23,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:23,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.046999999991385. input_tokens=8069, output_tokens=887
22:24:23,394 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:23,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.969000000011874. input_tokens=3807, output_tokens=748
22:24:23,688 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:23,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.39000000001397. input_tokens=2150, output_tokens=594
22:24:24,548 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:24,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.280999999988126. input_tokens=4362, output_tokens=830
22:24:24,818 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:24,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.453000000008615. input_tokens=3259, output_tokens=872
22:24:24,977 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:24,984 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.60899999999674. input_tokens=3063, output_tokens=809
22:24:25,249 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:25,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.84399999998277. input_tokens=4307, output_tokens=935
22:24:25,387 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:25,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.984000000025844. input_tokens=2331, output_tokens=802
22:24:26,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:26,396 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.953000000008615. input_tokens=2751, output_tokens=781
22:24:26,947 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:26,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.64100000000326. input_tokens=8002, output_tokens=795
22:24:27,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:27,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.702999999979511. input_tokens=3297, output_tokens=844
22:24:27,397 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:27,415 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.078000000008615. input_tokens=3210, output_tokens=775
22:24:27,623 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:27,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.280999999988126. input_tokens=6109, output_tokens=825
22:24:28,127 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:28,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.671999999991385. input_tokens=9894, output_tokens=942
22:24:28,893 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:28,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:28,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.453000000008615. input_tokens=4646, output_tokens=868
22:24:28,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.547000000020489. input_tokens=2338, output_tokens=689
22:24:31,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:31,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.437999999994645. input_tokens=2649, output_tokens=783
22:24:33,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:33,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.171999999991385. input_tokens=5008, output_tokens=818
22:24:34,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:34,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.10899999999674. input_tokens=3623, output_tokens=841
22:24:35,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:35,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.280999999988126. input_tokens=4779, output_tokens=984
22:24:36,763 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:36,764 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:36,765 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:36,765 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:36,786 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:36,787 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:36,798 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:36,799 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:36,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:36,819 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:36,829 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:36,830 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:38,164 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:38,165 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:38,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:38,357 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:38,386 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:38,387 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:38,422 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:38,423 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:38,657 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:38,658 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:40,606 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:40,607 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:40,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:40,824 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:41,158 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:41,159 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:41,241 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:41,243 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:41,330 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:41,331 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:44,723 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:44,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.59299999999348. input_tokens=2122, output_tokens=598
22:24:45,183 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:45,185 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:45,258 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:45,260 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:45,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:45,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.625. input_tokens=2870, output_tokens=780
22:24:45,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:45,899 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:46,2 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:46,4 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:46,263 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:46,264 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:46,507 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:46,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.312999999994645. input_tokens=2636, output_tokens=707
22:24:46,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:46,715 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.625. input_tokens=2225, output_tokens=620
22:24:47,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:47,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.0. input_tokens=3382, output_tokens=754
22:24:47,588 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:47,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.485000000015134. input_tokens=4010, output_tokens=780
22:24:47,864 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:47,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.84399999998277. input_tokens=4970, output_tokens=815
22:24:48,1 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:48,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.875. input_tokens=3414, output_tokens=813
22:24:48,372 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:48,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.34399999998277. input_tokens=2102, output_tokens=599
22:24:49,296 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:49,297 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:49,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 11.265999999974156. input_tokens=3289, output_tokens=876
22:24:49,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.171000000002095. input_tokens=4820, output_tokens=846
22:24:49,306 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:49,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.0. input_tokens=3609, output_tokens=868
22:24:50,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:50,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.75. input_tokens=6245, output_tokens=991
22:24:51,720 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:51,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.594000000011874. input_tokens=3847, output_tokens=986
22:24:52,386 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:52,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.328999999997905. input_tokens=2217, output_tokens=732
22:24:52,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:52,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.57799999997951. input_tokens=6470, output_tokens=982
22:24:54,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:54,281 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:54,314 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:54,315 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:54,732 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:54,733 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:54,790 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:24:54,792 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:24:56,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:56,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.85899999999674. input_tokens=3518, output_tokens=894
22:24:56,237 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:24:56,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.155999999988126. input_tokens=7666, output_tokens=930
22:25:04,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:04,942 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:04,987 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:04,987 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:06,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:25:06,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 12.14100000000326. input_tokens=8237, output_tokens=820
22:25:15,154 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:15,155 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:15,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:15,191 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:15,306 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:25:15,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 11.01500000001397. input_tokens=5954, output_tokens=894
22:25:16,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:25:16,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 12.421999999991385. input_tokens=3956, output_tokens=839
22:25:25,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:25,415 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:35,813 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:25:35,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 10.64000000001397. input_tokens=4295, output_tokens=749
22:25:47,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:25:47,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 11.687000000005355. input_tokens=5177, output_tokens=927
22:25:47,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,600 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:47,616 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,616 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:47,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,640 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:47,641 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:47,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,656 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:47,662 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,662 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:47,672 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,673 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,673 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:47,674 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:47,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,708 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:47,766 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:47,767 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,6 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,7 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,41 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,245 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,245 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,283 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,392 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,397 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,487 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,561 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,709 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,715 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,716 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:49,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:49,843 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:51,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:51,655 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:51,772 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:51,773 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:51,882 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:51,883 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:51,899 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:51,899 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:51,943 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:51,944 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:52,149 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:52,154 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:52,184 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:52,185 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:52,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:52,306 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:55,236 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:55,236 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:56,53 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:25:56,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.73399999999674. input_tokens=2971, output_tokens=787
22:25:56,713 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:56,713 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:57,538 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:57,538 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:58,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:58,174 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:25:59,265 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:25:59,265 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:00,21 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:00,22 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:02,739 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:02,739 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:05,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:05,789 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:06,899 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:06,900 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:07,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:07,168 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:07,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:26:07,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 11.296999999991385. input_tokens=6995, output_tokens=867
22:26:08,223 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:08,224 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:09,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:09,79 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:11,97 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:11,98 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:15,964 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:15,965 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:17,62 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:17,63 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:17,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:17,294 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:18,366 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:18,367 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:18,412 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:26:18,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 13.75. input_tokens=6998, output_tokens=941
22:26:19,213 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:19,214 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:21,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:21,455 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:24,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:24,116 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:26,103 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:26,104 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:27,193 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:27,194 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:27,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:27,415 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:28,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:28,496 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:29,276 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:26:29,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 15.484000000025844. input_tokens=9384, output_tokens=979
22:26:29,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:29,368 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:32,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:32,834 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:36,289 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:36,290 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:37,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:37,342 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:37,551 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:37,551 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:38,624 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:38,625 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:42,415 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:26:42,417 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 10.96899999998277. input_tokens=6692, output_tokens=877
22:26:43,5 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:43,6 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:46,441 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:46,442 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:47,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:26:47,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 8.062000000005355. input_tokens=2342, output_tokens=730
22:26:47,474 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:47,475 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:47,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:47,698 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:48,739 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:48,740 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:53,171 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:53,172 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:57,689 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:57,689 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:57,690 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:26:57,708 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:26:57,708 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 4
22:26:57,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:57,864 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:57,864 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 196115, Requested 4329. Please try again in 133ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:26:57,868 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:26:57,868 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 5
22:26:58,884 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:26:58,885 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:26:58,885 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:26:58,889 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:26:58,889 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 9
22:27:03,345 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:27:03,346 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:27:04,338 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:27:04,340 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 7.875. input_tokens=2340, output_tokens=706
22:27:24,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:27:24,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 11.125. input_tokens=8853, output_tokens=980
22:27:24,518 datashaper.workflow.workflow INFO executing verb window
22:27:24,520 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:27:24,740 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:27:24,741 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:27:24,800 datashaper.workflow.workflow INFO executing verb unroll
22:27:24,824 datashaper.workflow.workflow INFO executing verb select
22:27:24,856 datashaper.workflow.workflow INFO executing verb rename
22:27:24,879 datashaper.workflow.workflow INFO executing verb join
22:27:24,914 datashaper.workflow.workflow INFO executing verb aggregate_override
22:27:24,938 datashaper.workflow.workflow INFO executing verb join
22:27:24,969 datashaper.workflow.workflow INFO executing verb rename
22:27:24,992 datashaper.workflow.workflow INFO executing verb convert
22:27:25,51 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:27:25,268 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:27:25,269 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:27:25,326 datashaper.workflow.workflow INFO executing verb rename
22:27:25,328 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:27:25,446 graphrag.index.cli INFO All workflows completed successfully.
22:28:00,107 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:28:00,111 graphrag.index.cli INFO Starting pipeline run for: 20241201-222800, dryrun=False
22:28:00,111 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:28:00,114 graphrag.index.create_pipeline_config INFO skipping workflows 
22:28:00,114 graphrag.index.run.run INFO Running pipeline
22:28:00,114 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:28:00,115 graphrag.index.input.load_input INFO loading input from root_dir=input
22:28:00,115 graphrag.index.input.load_input INFO using file storage for input
22:28:00,117 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:28:00,118 graphrag.index.input.text INFO found text files from input, found [('8_copy0.txt', {}), ('8_copy1.txt', {}), ('8_copy2.txt', {}), ('8_copy3.txt', {}), ('8_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:28:00,132 graphrag.index.input.text INFO Found 6 files, loading 6
22:28:00,134 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:28:00,134 graphrag.index.run.run INFO Final # of rows loaded: 6
22:28:00,275 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:28:00,282 datashaper.workflow.workflow INFO executing verb orderby
22:28:00,286 datashaper.workflow.workflow INFO executing verb zip
22:28:00,290 datashaper.workflow.workflow INFO executing verb aggregate_override
22:28:00,298 datashaper.workflow.workflow INFO executing verb chunk
22:28:00,647 datashaper.workflow.workflow INFO executing verb select
22:28:00,654 datashaper.workflow.workflow INFO executing verb unroll
22:28:00,663 datashaper.workflow.workflow INFO executing verb rename
22:28:00,669 datashaper.workflow.workflow INFO executing verb genid
22:28:00,684 datashaper.workflow.workflow INFO executing verb unzip
22:28:00,692 datashaper.workflow.workflow INFO executing verb copy
22:28:00,703 datashaper.workflow.workflow INFO executing verb filter
22:28:00,721 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:28:00,911 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:28:00,911 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:28:00,946 datashaper.workflow.workflow INFO executing verb entity_extract
22:28:00,963 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:28:01,413 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:28:01,414 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:28:04,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:28:04,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.0469999999913853. input_tokens=2056, output_tokens=229
22:28:07,671 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:28:07,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.812999999994645. input_tokens=34, output_tokens=221
22:28:07,689 datashaper.workflow.workflow INFO executing verb merge_graphs
22:28:07,807 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:28:07,971 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:28:07,972 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:28:08,3 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:28:08,424 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:08,429 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:08,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:08,452 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:08,553 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:08,558 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:08,564 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:08,568 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:08,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:08,641 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:08,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:08,651 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:09,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:09,754 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:09,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:09,849 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:09,856 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:09,856 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:10,142 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:10,146 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:10,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:10,268 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:10,317 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:10,318 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:12,197 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:12,202 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:12,333 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:12,334 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:12,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:12,497 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:12,689 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:12,689 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:12,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:12,795 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:13,13 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:13,14 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:13,816 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:28:13,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.703000000008615. input_tokens=972, output_tokens=300
22:28:16,956 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:16,957 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:17,39 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:17,39 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:17,202 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:17,202 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:17,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:28:17,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 3 retries took 0.5629999999946449. input_tokens=147, output_tokens=21
22:28:17,404 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:17,406 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:18,30 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:18,31 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:25,617 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:25,618 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:26,88 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:26,89 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:26,504 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:26,505 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:26,882 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:26,883 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:27,92 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:28:27,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 4 retries took 1.7820000000065193. input_tokens=261, output_tokens=120
22:28:36,293 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:36,293 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:36,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:36,629 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:37,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:37,19 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:37,513 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:28:37,514 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 5 retries took 1.8910000000032596. input_tokens=189, output_tokens=121
22:28:46,820 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:46,820 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:47,129 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:47,130 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:48,162 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:28:48,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 6 retries took 1.875. input_tokens=173, output_tokens=114
22:28:57,4 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:28:57,4 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:28:59,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:28:59,474 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 7 retries took 2.327999999979511. input_tokens=205, output_tokens=112
22:29:08,601 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:29:08,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 8 retries took 1.5929999999934807. input_tokens=588, output_tokens=132
22:29:08,651 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:29:08,810 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:29:08,810 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:29:08,838 datashaper.workflow.workflow INFO executing verb cluster_graph
22:29:09,279 datashaper.workflow.workflow INFO executing verb select
22:29:09,283 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:29:09,475 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:29:09,475 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:29:09,509 datashaper.workflow.workflow INFO executing verb unpack_graph
22:29:09,732 datashaper.workflow.workflow INFO executing verb rename
22:29:09,741 datashaper.workflow.workflow INFO executing verb select
22:29:09,752 datashaper.workflow.workflow INFO executing verb dedupe
22:29:09,763 datashaper.workflow.workflow INFO executing verb rename
22:29:09,773 datashaper.workflow.workflow INFO executing verb filter
22:29:09,807 datashaper.workflow.workflow INFO executing verb text_split
22:29:09,826 datashaper.workflow.workflow INFO executing verb drop
22:29:09,840 datashaper.workflow.workflow INFO executing verb merge
22:29:09,952 datashaper.workflow.workflow INFO executing verb text_embed
22:29:09,953 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:29:10,336 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:29:10,336 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:29:10,381 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 404 inputs via 404 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:29:11,64 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:29:11,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6879999999946449. input_tokens=102, output_tokens=0
22:29:11,143 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:29:11,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7809999999881256. input_tokens=686, output_tokens=0
22:29:11,198 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:29:11,333 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:29:11,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9679999999934807. input_tokens=1885, output_tokens=0
22:29:11,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:29:11,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0150000000139698. input_tokens=943, output_tokens=0
22:29:11,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2179999999934807. input_tokens=641, output_tokens=0
22:29:11,655 datashaper.workflow.workflow INFO executing verb drop
22:29:11,667 datashaper.workflow.workflow INFO executing verb filter
22:29:11,690 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:29:11,956 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:29:11,957 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:29:11,993 datashaper.workflow.workflow INFO executing verb layout_graph
22:29:12,874 datashaper.workflow.workflow INFO executing verb unpack_graph
22:29:13,134 datashaper.workflow.workflow INFO executing verb unpack_graph
22:29:13,395 datashaper.workflow.workflow INFO executing verb filter
22:29:13,467 datashaper.workflow.workflow INFO executing verb drop
22:29:13,485 datashaper.workflow.workflow INFO executing verb select
22:29:13,501 datashaper.workflow.workflow INFO executing verb rename
22:29:13,518 datashaper.workflow.workflow INFO executing verb convert
22:29:13,574 datashaper.workflow.workflow INFO executing verb join
22:29:13,602 datashaper.workflow.workflow INFO executing verb rename
22:29:13,605 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:29:13,805 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:29:13,806 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:29:13,842 datashaper.workflow.workflow INFO executing verb create_final_communities
22:29:14,346 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:29:14,557 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:29:14,557 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:29:14,564 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:29:14,613 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:29:14,883 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:29:14,892 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:29:15,113 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
22:29:15,113 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:29:15,127 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:29:15,132 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:29:15,202 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:29:15,245 datashaper.workflow.workflow INFO executing verb select
22:29:15,248 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:29:15,444 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:29:15,461 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:29:15,467 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:29:15,511 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:29:15,551 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:29:15,581 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:29:15,612 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:29:15,614 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 404
22:29:15,667 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 404
22:29:15,796 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 404
22:29:16,8 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 404
22:29:16,173 datashaper.workflow.workflow INFO executing verb create_community_reports
22:29:16,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:16,390 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:16,408 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:16,408 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:16,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:16,415 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:16,419 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:16,420 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:17,955 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:17,956 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:18,146 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:18,147 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:18,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:18,460 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:18,545 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:18,545 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:20,417 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:20,418 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:20,790 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:20,791 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:21,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:21,40 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:21,242 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:21,243 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:25,584 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:25,584 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:25,793 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:25,794 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:26,209 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:26,209 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:34,366 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:34,366 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:35,68 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:35,68 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:41,688 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:29:41,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 16.85899999999674. input_tokens=7637, output_tokens=867
22:29:44,509 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:44,510 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:45,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:45,211 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:29:45,615 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:29:45,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 11.59399999998277. input_tokens=3833, output_tokens=820
22:29:55,400 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:29:55,400 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:05,617 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:30:05,618 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:05,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 11.09399999998277. input_tokens=2550, output_tokens=745
22:30:05,620 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:33,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:30:33,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 17.828000000008615. input_tokens=6666, output_tokens=793
22:30:36,225 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,225 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,230 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,231 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,231 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,232 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,255 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,261 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,262 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,262 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,263 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,273 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,295 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,311 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,325 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,343 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,344 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,360 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,361 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,366 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,367 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,409 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:36,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:36,495 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:37,391 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:37,391 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:37,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:37,495 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:37,711 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:37,712 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:37,907 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:37,913 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:37,987 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:37,988 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:37,998 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:37,999 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:38,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:38,80 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:38,169 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:38,175 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:38,201 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:38,206 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:38,295 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:38,296 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:38,298 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:38,299 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:38,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:38,321 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:38,375 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:38,376 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:38,379 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:38,380 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:38,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:38,403 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:39,799 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:39,801 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:40,318 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:40,322 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:40,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:40,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:40,412 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:40,413 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:40,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:40,509 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:40,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:40,630 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:40,663 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:40,664 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:40,747 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:40,747 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:40,765 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:40,766 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:41,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:41,47 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:41,88 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:41,88 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:41,111 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:41,112 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:41,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:41,339 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:41,420 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:41,421 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:43,970 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:43,971 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:44,711 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:44,712 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:44,901 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:44,906 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:45,34 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:45,35 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:45,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:45,295 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:46,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:30:46,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.15600000001723. input_tokens=2150, output_tokens=653
22:30:46,235 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:46,236 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:47,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:47,61 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:48,378 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:48,379 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:48,930 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:30:48,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.14000000001397. input_tokens=4980, output_tokens=857
22:30:49,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:49,340 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:51,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:51,96 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:52,315 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:52,316 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:53,361 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:53,361 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:53,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:53,520 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:54,517 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:54,517 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:55,335 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:55,336 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:56,71 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:56,72 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:56,763 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:56,764 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:57,775 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:57,776 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:30:59,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:30:59,411 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:00,609 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:00,610 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:01,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:01,276 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:03,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:03,480 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:03,637 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:03,638 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:04,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:04,643 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:04,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:04,958 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:06,201 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:06,202 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:06,893 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:06,894 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:07,72 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:07,73 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:07,915 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:07,916 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:09,536 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:09,537 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:10,741 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:10,742 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:12,895 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:12,896 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:13,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:13,598 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:13,773 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:13,774 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:14,771 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:14,772 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:16,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:16,412 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:16,995 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:16,996 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:17,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:17,958 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:18,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:18,958 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:20,953 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:20,954 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:21,47 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:21,47 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:23,54 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:23,54 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:23,726 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:23,727 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:24,950 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:24,951 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:26,709 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:26,710 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:27,322 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:27,322 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:28,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:28,141 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:29,146 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:29,147 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:29,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:31:29,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 13.172000000020489. input_tokens=4362, output_tokens=817
22:31:31,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:31,84 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:31,204 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:31,204 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:33,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:33,197 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:33,866 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:33,867 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:34,592 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:31:34,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 10.047000000020489. input_tokens=2470, output_tokens=736
22:31:35,68 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:35,69 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:36,673 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:36,673 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:37,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:37,443 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:38,270 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:38,271 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:39,260 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:39,262 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:40,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:40,200 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:41,232 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:41,233 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:42,585 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:42,586 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:44,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:44,11 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:44,11 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:31:44,30 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:31:44,30 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 59
22:31:44,832 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:44,833 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:45,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:45,199 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:45,200 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:31:45,204 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:31:45,204 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 48
22:31:47,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:47,575 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:47,575 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:31:47,579 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:31:47,580 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 58
22:31:48,394 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:48,395 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:48,395 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:31:48,399 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:31:48,400 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 49
22:31:49,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:49,377 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:49,377 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:31:49,381 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:31:49,382 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 52
22:31:50,218 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:50,219 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:51,271 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:51,272 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:51,272 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:31:51,276 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:31:51,276 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 46
22:31:52,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:52,497 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:52,497 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 194811, Requested 6276. Please try again in 326ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:31:52,501 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:31:52,501 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 45
22:31:53,964 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:53,964 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:53,965 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:31:53,972 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:31:53,972 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 47
22:31:54,955 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:54,956 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:31:54,956 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 193372, Requested 7734. Please try again in 331ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:31:54,960 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:31:54,961 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 57
22:31:57,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:31:57,934 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:32:03,455 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:32:03,455 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:32:05,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:32:05,858 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:32:09,763 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:32:09,764 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:32:12,714 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:32:12,716 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 12.485000000015134. input_tokens=8087, output_tokens=860
22:32:13,636 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:32:13,637 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:32:19,971 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:32:19,971 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:32:23,779 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:32:23,780 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:32:27,546 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:32:27,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 11.687999999994645. input_tokens=8246, output_tokens=821
22:32:30,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:32:30,106 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:32:40,397 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:32:40,398 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:32:47,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:32:47,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 13.39000000001397. input_tokens=9895, output_tokens=833
22:33:05,759 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:33:05,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 15.344000000011874. input_tokens=9864, output_tokens=863
22:33:06,360 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,360 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,368 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,369 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,369 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,382 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,383 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,390 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,390 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,391 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,412 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,415 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,416 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,419 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,420 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,421 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,421 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,425 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,426 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,428 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,429 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,432 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,451 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,452 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,466 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,467 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,468 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,470 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,478 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,495 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,513 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,513 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,514 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,515 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,534 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,535 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,641 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,814 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,815 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:06,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:06,843 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:07,567 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:07,568 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:07,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:07,576 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:07,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:07,643 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:07,654 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:07,655 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:07,715 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:07,716 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:07,739 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:07,740 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:07,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:07,758 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:08,7 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:08,8 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:08,709 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:08,711 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:09,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:09,374 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:09,832 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:09,833 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:10,586 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:10,588 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:11,350 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:11,352 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:11,861 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:11,862 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:12,471 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:12,471 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:13,483 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:13,484 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:14,287 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:14,288 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:15,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:15,52 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:15,594 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:15,595 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:17,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:17,123 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:17,799 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:17,799 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:18,572 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:18,572 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:19,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:19,372 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:19,963 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:19,964 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:20,258 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:33:20,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.375. input_tokens=5019, output_tokens=815
22:33:21,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:21,101 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:22,0 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:22,0 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:22,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:22,795 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:23,507 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:23,508 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:24,271 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:24,272 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:25,162 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:25,163 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:25,869 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:25,870 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:25,983 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:33:25,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 9.655999999988126. input_tokens=2079, output_tokens=615
22:33:26,795 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:26,797 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:27,338 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:27,339 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:28,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:28,273 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:29,3 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:29,4 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:29,925 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:29,926 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:30,659 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:30,659 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:31,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:31,521 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:32,476 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:32,476 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:33,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:33,269 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:34,772 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:34,773 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:35,719 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:35,720 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:36,495 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:36,496 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:37,57 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:37,58 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:37,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:37,851 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:38,373 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:38,373 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:39,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:39,196 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:40,78 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:40,79 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:40,645 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:40,646 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:41,289 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:41,289 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:42,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:42,339 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:42,750 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:33:42,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 8.812999999994645. input_tokens=2368, output_tokens=619
22:33:43,395 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:43,395 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:44,147 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:44,147 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:44,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:44,942 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:45,829 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:45,830 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:46,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:46,835 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:47,518 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:47,518 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:48,509 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:48,509 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:49,182 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:49,182 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:49,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:49,907 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:50,654 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:50,655 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:52,200 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:52,201 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:53,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:53,80 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:53,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:53,543 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:54,488 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:54,489 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:55,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:55,516 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:56,444 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:56,445 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:57,432 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:57,432 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:58,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:58,219 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:59,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:59,191 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:33:59,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:33:59,957 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:00,574 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:00,575 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:00,976 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:34:00,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 9.59299999999348. input_tokens=2148, output_tokens=645
22:34:01,387 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:01,388 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:02,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:02,157 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:03,39 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:03,40 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:03,674 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:03,675 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:04,620 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:04,621 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:05,340 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:05,340 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:06,197 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:06,198 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:07,241 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:07,242 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:09,99 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:09,100 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:09,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:09,918 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:10,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:10,725 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:11,372 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:11,372 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:12,235 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:12,235 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:12,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:12,820 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:13,675 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:13,676 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:14,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:14,445 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:15,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:15,136 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:16,189 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:16,190 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:18,139 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:18,140 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:18,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:34:18,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 10.375. input_tokens=2340, output_tokens=700
22:34:19,30 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:19,31 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:19,949 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:19,950 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:20,721 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:20,722 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:21,397 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:21,398 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:22,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:22,142 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:22,142 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:34:22,151 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:34:22,151 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 23
22:34:22,964 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:22,965 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:23,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:23,719 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:24,321 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:24,322 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:25,178 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:25,178 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:26,748 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:26,749 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:27,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:27,294 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:28,360 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:28,361 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:29,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:29,176 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:29,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:34:29,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 12.078999999997905. input_tokens=3201, output_tokens=785
22:34:30,157 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:30,158 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:31,0 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:31,1 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:31,758 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:31,759 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:32,840 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:32,841 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:32,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:34:32,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 7.062999999994645. input_tokens=2225, output_tokens=656
22:34:33,862 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:33,863 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:35,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:35,341 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:35,342 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 195981, Requested 4585. Please try again in 169ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:34:35,349 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:34:35,349 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 24
22:34:36,148 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:36,148 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:37,193 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:37,194 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:38,187 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:38,188 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:39,242 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:39,242 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:40,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:40,135 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:40,820 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:40,820 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:41,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:41,432 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:42,500 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:42,501 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:43,366 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:43,367 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:43,924 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:34:43,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 9.577999999979511. input_tokens=2892, output_tokens=775
22:34:44,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:44,20 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:44,20 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:34:44,24 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:34:44,24 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 40
22:34:45,328 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:45,329 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:46,179 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:46,180 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:46,984 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:46,985 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:48,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:48,141 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:49,170 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:49,171 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:50,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:50,255 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:50,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:50,960 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:51,776 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:51,777 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:52,719 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:52,720 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:53,476 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:53,477 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:54,277 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:54,278 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:54,278 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 196976, Requested 4897. Please try again in 561ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:34:54,284 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:34:54,285 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 34
22:34:55,271 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:55,272 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:56,321 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:56,322 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:56,322 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 195866, Requested 5334. Please try again in 360ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:34:56,327 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:34:56,327 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 39
22:34:57,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:57,105 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:58,208 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:58,209 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:34:59,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:34:59,197 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:01,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:01,158 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:01,909 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:01,910 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:02,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:02,844 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:02,844 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 196658, Requested 4992. Please try again in 495ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:35:02,854 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:35:02,854 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 12
22:35:03,588 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:03,588 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:04,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:04,614 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:05,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:05,404 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:06,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:06,834 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:08,330 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:08,331 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:09,57 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:35:09,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 8.780999999988126. input_tokens=2870, output_tokens=722
22:35:09,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:09,312 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:10,415 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:10,416 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:11,285 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:11,285 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:12,32 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:12,33 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:13,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:13,281 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:14,608 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:14,609 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:15,559 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:15,560 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:16,532 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:16,532 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:19,5 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:19,5 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:19,6 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:35:19,11 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:35:19,11 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 28
22:35:19,447 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:19,448 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:19,448 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 197438, Requested 5443. Please try again in 864ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:35:19,455 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:35:19,455 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 13
22:35:20,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:20,455 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:21,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:21,424 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:22,158 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:22,159 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:23,491 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:23,493 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:24,777 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:24,778 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:25,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:25,500 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:26,680 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:26,681 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:27,823 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:27,824 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:29,58 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:29,59 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:30,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:30,412 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:31,550 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:31,551 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:31,552 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:35:31,560 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:35:31,561 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 43
22:35:32,287 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:32,288 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:33,546 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:33,547 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:34,954 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:34,954 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:36,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:36,96 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:36,357 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:35:36,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 18.780999999988126. input_tokens=3275, output_tokens=840
22:35:36,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:36,824 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:38,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:38,291 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:39,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:39,698 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:40,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:40,944 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:42,212 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:42,212 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:44,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:44,544 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:44,545 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:35:44,549 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:35:44,549 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 26
22:35:45,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:45,979 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:46,948 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:46,948 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:46,948 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:35:46,954 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:35:46,954 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 37
22:35:48,225 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:48,226 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:49,392 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:49,394 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:50,595 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:50,596 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:53,444 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:53,445 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:53,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:35:53,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 10.312999999994645. input_tokens=3508, output_tokens=902
22:35:55,170 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:55,171 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:56,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:56,524 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:58,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:58,267 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:35:59,357 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:35:59,358 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:01,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:01,897 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:02,866 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:02,866 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:04,401 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:04,402 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:06,18 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:06,18 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:07,560 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:07,561 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:09,450 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:09,450 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:10,686 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:10,686 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:11,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:11,969 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:12,682 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:36:12,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 12.35899999999674. input_tokens=3459, output_tokens=756
22:36:13,964 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:13,965 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:15,334 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:15,335 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:16,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:36:16,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 24.09299999999348. input_tokens=4970, output_tokens=916
22:36:17,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:17,220 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:18,504 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:18,505 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:19,580 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:19,580 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:20,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:20,853 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:22,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:22,213 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:24,359 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:24,359 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:25,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:25,487 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:28,704 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:28,706 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:30,516 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:30,517 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:32,176 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:32,177 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:33,297 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:33,298 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:35,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:35,296 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:35,622 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:35,624 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:35,624 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:36:35,629 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:36:35,629 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 41
22:36:38,325 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:38,326 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:39,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:39,803 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:40,335 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:36:40,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.75. input_tokens=7720, output_tokens=936
22:36:41,164 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:41,165 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:41,165 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:36:41,170 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:36:41,170 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 11
22:36:43,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:43,141 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:46,224 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:46,225 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:48,400 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:48,401 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:49,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:49,940 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:50,507 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:50,508 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:55,326 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:55,327 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:57,926 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:57,927 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:58,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:36:58,531 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:36:59,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:36:59,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 14.76600000000326. input_tokens=3830, output_tokens=927
22:37:00,61 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:00,62 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:05,535 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:05,535 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:05,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:37:05,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 12.859000000025844. input_tokens=8883, output_tokens=846
22:37:08,119 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:08,119 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:08,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:08,679 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:15,743 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:15,744 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:18,927 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:18,927 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:18,928 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 194023, Requested 7659. Please try again in 504ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:37:18,932 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:37:18,932 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 35
22:37:21,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:37:21,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 11.26600000000326. input_tokens=4820, output_tokens=846
22:37:23,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:37:23,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 21.17200000002049. input_tokens=5851, output_tokens=821
22:37:25,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:25,874 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:25,875 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:37:25,882 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:37:25,884 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 16
22:37:28,370 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:37:28,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 10.25. input_tokens=5096, output_tokens=860
22:37:28,830 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:28,831 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:28,907 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:28,907 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:28,912 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:28,912 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:28,916 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:28,917 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:28,920 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:28,921 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:28,924 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:28,926 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:30,146 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:30,146 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:30,835 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:30,836 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:32,544 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:32,545 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:34,45 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:34,47 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:34,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:37:34,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.562000000005355. input_tokens=2060, output_tokens=397
22:37:35,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:35,257 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:37,397 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:37,398 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:38,293 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:38,294 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:39,730 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:39,731 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:41,726 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:41,727 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:43,904 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:43,904 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:46,412 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:46,413 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:47,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:47,328 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:48,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:48,275 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:50,741 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:50,742 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:52,646 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:52,647 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:56,570 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:56,570 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:58,397 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:58,398 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:37:58,815 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:37:58,816 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:00,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:00,857 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:02,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:02,854 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:04,749 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:04,750 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:06,661 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:38:06,665 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 12.96899999998277. input_tokens=7609, output_tokens=899
22:38:07,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:07,301 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:08,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:08,544 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:11,693 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:11,693 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:13,525 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:13,525 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:15,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:15,909 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:18,580 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:18,580 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:19,680 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:19,681 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:20,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:38:20,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 10.030999999988126. input_tokens=4378, output_tokens=796
22:38:22,270 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:22,270 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:23,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:23,656 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:26,70 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:26,71 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:29,903 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:29,904 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:32,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:32,440 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:33,813 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:33,814 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:36,466 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:36,466 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:38,876 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:38,877 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:40,42 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:40,42 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:40,44 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:38:40,49 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:38:40,49 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 5
22:38:42,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:42,577 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:43,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:43,968 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:45,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:38:45,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 17.796999999991385. input_tokens=6896, output_tokens=914
22:38:49,92 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:49,92 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:52,725 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:52,725 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:59,365 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:38:59,366 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:38:59,836 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:38:59,839 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 14.01600000000326. input_tokens=8836, output_tokens=929
22:39:06,359 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:39:06,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 12.389999999984866. input_tokens=6237, output_tokens=848
22:39:09,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:39:09,522 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:39:16,112 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:39:16,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 13.35899999999674. input_tokens=6586, output_tokens=939
22:39:33,732 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:39:33,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 14.202999999979511. input_tokens=7995, output_tokens=977
22:39:33,788 datashaper.workflow.workflow INFO executing verb window
22:39:33,793 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:39:34,160 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:39:34,160 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:39:34,231 datashaper.workflow.workflow INFO executing verb unroll
22:39:34,277 datashaper.workflow.workflow INFO executing verb select
22:39:34,306 datashaper.workflow.workflow INFO executing verb rename
22:39:34,338 datashaper.workflow.workflow INFO executing verb join
22:39:34,376 datashaper.workflow.workflow INFO executing verb aggregate_override
22:39:34,412 datashaper.workflow.workflow INFO executing verb join
22:39:34,454 datashaper.workflow.workflow INFO executing verb rename
22:39:34,481 datashaper.workflow.workflow INFO executing verb convert
22:39:34,551 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:39:34,871 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:39:34,872 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:39:34,947 datashaper.workflow.workflow INFO executing verb rename
22:39:34,950 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:39:35,82 graphrag.index.cli INFO All workflows completed successfully.
22:40:22,616 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:40:22,628 graphrag.index.cli INFO Starting pipeline run for: 20241201-224022, dryrun=False
22:40:22,631 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:40:22,635 graphrag.index.create_pipeline_config INFO skipping workflows 
22:40:22,635 graphrag.index.run.run INFO Running pipeline
22:40:22,637 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:40:22,639 graphrag.index.input.load_input INFO loading input from root_dir=input
22:40:22,639 graphrag.index.input.load_input INFO using file storage for input
22:40:22,644 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:40:22,645 graphrag.index.input.text INFO found text files from input, found [('9_copy0.txt', {}), ('9_copy1.txt', {}), ('9_copy2.txt', {}), ('9_copy3.txt', {}), ('9_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:40:22,684 graphrag.index.input.text INFO Found 6 files, loading 6
22:40:22,686 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:40:22,686 graphrag.index.run.run INFO Final # of rows loaded: 6
22:40:22,885 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:40:22,892 datashaper.workflow.workflow INFO executing verb orderby
22:40:22,899 datashaper.workflow.workflow INFO executing verb zip
22:40:22,905 datashaper.workflow.workflow INFO executing verb aggregate_override
22:40:22,916 datashaper.workflow.workflow INFO executing verb chunk
22:40:23,380 datashaper.workflow.workflow INFO executing verb select
22:40:23,387 datashaper.workflow.workflow INFO executing verb unroll
22:40:23,397 datashaper.workflow.workflow INFO executing verb rename
22:40:23,406 datashaper.workflow.workflow INFO executing verb genid
22:40:23,434 datashaper.workflow.workflow INFO executing verb unzip
22:40:23,457 datashaper.workflow.workflow INFO executing verb copy
22:40:23,476 datashaper.workflow.workflow INFO executing verb filter
22:40:23,512 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:40:23,831 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:40:23,831 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:40:23,879 datashaper.workflow.workflow INFO executing verb entity_extract
22:40:23,908 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:40:24,451 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:40:24,452 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:40:26,685 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:40:26,692 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6559999999881256. input_tokens=2110, output_tokens=97
22:40:29,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:40:29,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.2339999999967404. input_tokens=34, output_tokens=246
22:40:29,970 datashaper.workflow.workflow INFO executing verb merge_graphs
22:40:30,179 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:40:30,412 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:40:30,413 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:40:30,449 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:40:33,547 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:40:33,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.437999999994645. input_tokens=479, output_tokens=202
22:40:33,589 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:40:33,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9690000000118744. input_tokens=974, output_tokens=250
22:40:33,679 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:40:33,921 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:40:33,922 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:40:33,958 datashaper.workflow.workflow INFO executing verb cluster_graph
22:40:34,567 datashaper.workflow.workflow INFO executing verb select
22:40:34,572 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:40:34,844 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:40:34,844 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:40:34,890 datashaper.workflow.workflow INFO executing verb unpack_graph
22:40:35,190 datashaper.workflow.workflow INFO executing verb rename
22:40:35,202 datashaper.workflow.workflow INFO executing verb select
22:40:35,217 datashaper.workflow.workflow INFO executing verb dedupe
22:40:35,234 datashaper.workflow.workflow INFO executing verb rename
22:40:35,252 datashaper.workflow.workflow INFO executing verb filter
22:40:35,308 datashaper.workflow.workflow INFO executing verb text_split
22:40:35,335 datashaper.workflow.workflow INFO executing verb drop
22:40:35,352 datashaper.workflow.workflow INFO executing verb merge
22:40:35,504 datashaper.workflow.workflow INFO executing verb text_embed
22:40:35,506 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:40:36,70 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:40:36,71 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:40:36,161 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 408 inputs via 408 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:40:37,44 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:40:37,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8900000000139698. input_tokens=1835, output_tokens=0
22:40:37,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:40:37,219 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0320000000065193. input_tokens=218, output_tokens=0
22:40:37,279 datashaper.workflow.workflow INFO executing verb drop
22:40:37,302 datashaper.workflow.workflow INFO executing verb filter
22:40:37,344 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:40:37,762 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:40:37,764 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:40:37,817 datashaper.workflow.workflow INFO executing verb layout_graph
22:40:39,287 datashaper.workflow.workflow INFO executing verb unpack_graph
22:40:39,673 datashaper.workflow.workflow INFO executing verb unpack_graph
22:40:40,144 datashaper.workflow.workflow INFO executing verb filter
22:40:40,235 datashaper.workflow.workflow INFO executing verb drop
22:40:40,255 datashaper.workflow.workflow INFO executing verb select
22:40:40,285 datashaper.workflow.workflow INFO executing verb rename
22:40:40,336 datashaper.workflow.workflow INFO executing verb join
22:40:40,382 datashaper.workflow.workflow INFO executing verb convert
22:40:40,467 datashaper.workflow.workflow INFO executing verb rename
22:40:40,471 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:40:40,757 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:40:40,758 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:40:40,818 datashaper.workflow.workflow INFO executing verb create_final_communities
22:40:41,639 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:40:42,30 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:40:42,30 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:40:42,44 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:40:42,117 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:40:42,590 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:40:42,604 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:40:42,897 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
22:40:42,898 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:40:42,906 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:40:42,928 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:40:43,32 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:40:43,94 datashaper.workflow.workflow INFO executing verb select
22:40:43,97 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:40:43,381 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:40:43,382 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:40:43,393 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:40:43,457 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:40:43,520 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:40:43,569 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:40:43,651 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:40:43,652 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 408
22:40:43,746 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 408
22:40:43,996 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 408
22:40:44,332 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 408
22:40:44,915 datashaper.workflow.workflow INFO executing verb create_community_reports
22:40:54,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:40:54,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.60899999999674. input_tokens=6838, output_tokens=800
22:40:57,580 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:57,580 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:57,581 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:40:57,581 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:40:57,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:57,600 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:40:57,602 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:57,604 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:40:57,608 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:57,609 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:40:58,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:58,939 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:40:58,970 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:58,971 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:40:59,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:59,492 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:40:59,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:59,575 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:40:59,649 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:40:59,650 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:01,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:01,281 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:01,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:01,864 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:02,29 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:02,30 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:02,175 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:02,176 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:02,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:02,375 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:06,687 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:06,688 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:06,763 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:06,764 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:06,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:06,855 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:07,16 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:07,16 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:09,138 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:41:09,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.89100000000326. input_tokens=7689, output_tokens=861
22:41:09,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:41:09,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.34399999998277. input_tokens=4252, output_tokens=823
22:41:15,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:15,359 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:15,721 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:15,722 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:15,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:15,859 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:17,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:41:17,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 11.65700000000652. input_tokens=4976, output_tokens=723
22:41:25,937 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:25,938 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:26,86 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:26,87 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:33,895 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:41:33,899 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 18.859000000025844. input_tokens=9795, output_tokens=936
22:41:36,622 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:41:36,622 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:41:41,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:41:41,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 15.937000000005355. input_tokens=9861, output_tokens=883
22:41:55,18 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:41:55,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 19.062999999994645. input_tokens=8252, output_tokens=943
22:42:01,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:42:01,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 14.75. input_tokens=4291, output_tokens=763
22:42:01,887 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:01,888 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:01,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:01,908 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:01,913 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:01,913 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:01,921 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:01,922 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:01,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:01,933 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:03,109 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:03,110 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:03,231 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:03,233 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:03,926 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:03,926 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:04,4 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:04,5 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:05,615 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:05,616 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:05,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:05,834 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:06,214 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:06,214 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:06,382 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:06,384 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:10,284 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:10,284 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:10,916 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:10,917 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:11,92 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:11,92 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:11,164 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:11,166 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:11,460 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:42:11,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.90700000000652. input_tokens=8935, output_tokens=801
22:42:14,354 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:42:14,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 11.030999999988126. input_tokens=4406, output_tokens=919
22:42:19,35 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:19,36 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:19,318 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:19,318 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:19,388 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:19,389 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:19,622 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:19,623 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:29,596 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:29,597 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:29,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:29,917 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:40,126 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:42:40,127 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:42:40,765 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:42:40,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 11.421999999991385. input_tokens=5974, output_tokens=952
22:42:43,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:42:43,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 14.359000000025844. input_tokens=3225, output_tokens=911
22:42:51,732 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:42:51,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 12.139999999984866. input_tokens=2854, output_tokens=779
22:43:02,611 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:43:02,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 12.46799999999348. input_tokens=6443, output_tokens=802
22:43:03,214 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:43:03,215 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:43:03,279 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:43:03,280 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:43:03,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:43:03,291 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:43:04,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:43:04,846 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:43:04,974 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:43:04,975 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:43:07,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:43:07,801 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:43:08,114 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:43:08,114 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:43:12,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:43:12,413 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:43:19,659 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:43:19,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 15.39000000001397. input_tokens=8814, output_tokens=901
22:43:25,741 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:43:25,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 13.35899999999674. input_tokens=7072, output_tokens=978
22:43:32,171 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:43:32,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 10.921999999991385. input_tokens=5981, output_tokens=793
22:43:32,228 datashaper.workflow.workflow INFO executing verb window
22:43:32,232 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:43:32,547 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:43:32,548 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:43:32,609 datashaper.workflow.workflow INFO executing verb unroll
22:43:32,638 datashaper.workflow.workflow INFO executing verb select
22:43:32,663 datashaper.workflow.workflow INFO executing verb rename
22:43:32,688 datashaper.workflow.workflow INFO executing verb join
22:43:32,721 datashaper.workflow.workflow INFO executing verb aggregate_override
22:43:32,752 datashaper.workflow.workflow INFO executing verb join
22:43:32,787 datashaper.workflow.workflow INFO executing verb rename
22:43:32,814 datashaper.workflow.workflow INFO executing verb convert
22:43:32,879 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:43:33,129 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:43:33,129 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:43:33,221 datashaper.workflow.workflow INFO executing verb rename
22:43:33,225 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:43:33,340 graphrag.index.cli INFO All workflows completed successfully.
22:44:24,435 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:44:24,440 graphrag.index.cli INFO Starting pipeline run for: 20241201-224424, dryrun=False
22:44:24,441 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:44:24,445 graphrag.index.create_pipeline_config INFO skipping workflows 
22:44:24,445 graphrag.index.run.run INFO Running pipeline
22:44:24,445 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:44:24,447 graphrag.index.input.load_input INFO loading input from root_dir=input
22:44:24,447 graphrag.index.input.load_input INFO using file storage for input
22:44:24,449 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:44:24,450 graphrag.index.input.text INFO found text files from input, found [('10_copy0.txt', {}), ('10_copy1.txt', {}), ('10_copy2.txt', {}), ('10_copy3.txt', {}), ('10_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:44:24,472 graphrag.index.input.text INFO Found 6 files, loading 6
22:44:24,475 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:44:24,475 graphrag.index.run.run INFO Final # of rows loaded: 6
22:44:24,642 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:44:24,650 datashaper.workflow.workflow INFO executing verb orderby
22:44:24,656 datashaper.workflow.workflow INFO executing verb zip
22:44:24,663 datashaper.workflow.workflow INFO executing verb aggregate_override
22:44:24,671 datashaper.workflow.workflow INFO executing verb chunk
22:44:25,217 datashaper.workflow.workflow INFO executing verb select
22:44:25,224 datashaper.workflow.workflow INFO executing verb unroll
22:44:25,234 datashaper.workflow.workflow INFO executing verb rename
22:44:25,241 datashaper.workflow.workflow INFO executing verb genid
22:44:25,258 datashaper.workflow.workflow INFO executing verb unzip
22:44:25,267 datashaper.workflow.workflow INFO executing verb copy
22:44:25,277 datashaper.workflow.workflow INFO executing verb filter
22:44:25,298 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:44:25,650 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:44:25,659 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:44:25,754 datashaper.workflow.workflow INFO executing verb entity_extract
22:44:25,785 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:44:26,445 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:44:26,445 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:44:29,132 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:44:29,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.9539999999979045. input_tokens=2125, output_tokens=139
22:44:32,206 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:44:32,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.062000000005355. input_tokens=34, output_tokens=240
22:44:32,222 datashaper.workflow.workflow INFO executing verb merge_graphs
22:44:32,342 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:44:32,508 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:44:32,509 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:44:32,536 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:44:33,510 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:33,511 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:34,892 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:44:34,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7190000000118744. input_tokens=237, output_tokens=103
22:44:35,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:44:35,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7809999999881256. input_tokens=730, output_tokens=251
22:44:35,488 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:35,489 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:37,569 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:44:37,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.921999999991385. input_tokens=1171, output_tokens=256
22:44:37,702 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:37,702 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:42,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:44:42,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 3 retries took 1.0. input_tokens=164, output_tokens=70
22:44:42,881 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:44:43,49 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:44:43,49 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:44:43,75 datashaper.workflow.workflow INFO executing verb cluster_graph
22:44:43,504 datashaper.workflow.workflow INFO executing verb select
22:44:43,507 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:44:43,703 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:44:43,704 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:44:43,740 datashaper.workflow.workflow INFO executing verb unpack_graph
22:44:43,921 datashaper.workflow.workflow INFO executing verb rename
22:44:43,931 datashaper.workflow.workflow INFO executing verb select
22:44:43,942 datashaper.workflow.workflow INFO executing verb dedupe
22:44:43,955 datashaper.workflow.workflow INFO executing verb rename
22:44:43,965 datashaper.workflow.workflow INFO executing verb filter
22:44:43,994 datashaper.workflow.workflow INFO executing verb text_split
22:44:44,12 datashaper.workflow.workflow INFO executing verb drop
22:44:44,23 datashaper.workflow.workflow INFO executing verb merge
22:44:44,138 datashaper.workflow.workflow INFO executing verb text_embed
22:44:44,140 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:44:44,514 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:44:44,514 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:44:44,558 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 406 inputs via 406 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:44:45,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:44:45,418 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:44:45,423 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8599999999860302. input_tokens=1913, output_tokens=0
22:44:45,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.875. input_tokens=878, output_tokens=0
22:44:45,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:44:45,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9219999999913853. input_tokens=157, output_tokens=0
22:44:45,528 datashaper.workflow.workflow INFO executing verb drop
22:44:45,541 datashaper.workflow.workflow INFO executing verb filter
22:44:45,562 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:44:45,814 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:44:45,815 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:44:45,848 datashaper.workflow.workflow INFO executing verb layout_graph
22:44:46,647 datashaper.workflow.workflow INFO executing verb unpack_graph
22:44:46,848 datashaper.workflow.workflow INFO executing verb unpack_graph
22:44:47,59 datashaper.workflow.workflow INFO executing verb drop
22:44:47,74 datashaper.workflow.workflow INFO executing verb filter
22:44:47,124 datashaper.workflow.workflow INFO executing verb select
22:44:47,141 datashaper.workflow.workflow INFO executing verb rename
22:44:47,156 datashaper.workflow.workflow INFO executing verb convert
22:44:47,205 datashaper.workflow.workflow INFO executing verb join
22:44:47,246 datashaper.workflow.workflow INFO executing verb rename
22:44:47,249 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:44:47,437 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:44:47,437 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:44:47,474 datashaper.workflow.workflow INFO executing verb create_final_communities
22:44:47,892 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:44:48,77 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:44:48,77 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:44:48,93 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:44:48,132 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:44:48,354 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:44:48,362 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:44:48,546 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
22:44:48,546 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:44:48,552 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:44:48,589 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:44:48,640 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:44:48,680 datashaper.workflow.workflow INFO executing verb select
22:44:48,682 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:44:48,874 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:44:48,875 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:44:48,882 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:44:48,923 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:44:48,961 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:44:48,992 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:44:49,21 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:44:49,21 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 406
22:44:49,63 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 406
22:44:49,220 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 406
22:44:49,442 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 406
22:44:49,605 datashaper.workflow.workflow INFO executing verb create_community_reports
22:44:49,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:49,834 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:49,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:49,845 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:49,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:49,853 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:51,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:51,622 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:51,878 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:51,878 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:51,976 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:51,977 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:54,35 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:54,36 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:54,340 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:54,340 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:54,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:54,569 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:59,132 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:59,132 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:59,307 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:44:59,307 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:44:59,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:44:59,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.34299999999348. input_tokens=2652, output_tokens=782
22:45:07,723 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:07,724 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:12,32 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:45:12,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 13.39000000001397. input_tokens=4078, output_tokens=786
22:45:19,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:45:19,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 12.469000000011874. input_tokens=4177, output_tokens=903
22:45:28,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:45:28,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 10.703000000008615. input_tokens=6968, output_tokens=886
22:45:30,393 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,394 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,514 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,514 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,531 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,547 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,548 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,551 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,551 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,564 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,565 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,569 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,569 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,579 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,579 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,579 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,587 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,588 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,600 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,603 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,603 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,620 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,621 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,641 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,641 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:30,692 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:30,693 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:31,851 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:31,851 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:31,852 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:31,853 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:31,991 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:31,991 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:31,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:31,995 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,80 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,253 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,257 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,281 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,338 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,355 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,355 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,430 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,431 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,434 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,435 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,457 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,458 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,462 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,463 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,562 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,563 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,580 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,581 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:32,692 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:32,693 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,336 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,340 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,365 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,370 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,484 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,541 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,546 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,576 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,581 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,598 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,604 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,605 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,728 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,732 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,845 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:34,869 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:34,870 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:35,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:35,899 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:36,704 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:36,704 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:37,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:37,740 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:38,900 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:38,901 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:40,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:40,106 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:41,11 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:41,12 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:41,915 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:41,916 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:42,742 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:42,743 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:43,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:43,858 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:45,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:45,20 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:45,910 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:45:45,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.672000000020489. input_tokens=9839, output_tokens=1030
22:45:46,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:46,630 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:47,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:47,531 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:48,333 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:48,333 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:49,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:49,254 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:49,798 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:49,799 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:50,762 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:45:50,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 11.187999999994645. input_tokens=3607, output_tokens=881
22:45:50,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:50,897 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:52,34 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:52,35 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:53,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:53,467 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:54,982 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:54,982 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:56,168 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:56,168 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:57,665 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:57,666 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:58,330 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:58,331 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:58,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:58,502 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:45:59,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:45:59,424 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:00,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:00,853 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:01,768 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:01,769 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:02,823 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:02,823 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:04,541 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:04,542 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:06,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:06,753 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:07,811 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:07,812 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:08,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:08,500 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:09,407 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:09,408 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:10,203 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:10,203 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:11,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:11,909 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:12,184 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:12,185 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:14,738 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:14,738 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:16,552 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:16,553 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:17,934 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:17,935 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:18,622 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:18,623 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:19,612 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:19,613 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:20,159 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:46:20,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 13.969000000011874. input_tokens=3855, output_tokens=890
22:46:20,364 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:20,365 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:21,407 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:21,408 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:22,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:22,61 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:23,840 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:23,841 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:24,884 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:24,884 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:26,774 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:26,776 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:28,91 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:28,92 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:28,514 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:46:28,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 14.76600000000326. input_tokens=4779, output_tokens=929
22:46:28,776 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:28,777 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:29,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:29,754 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:30,512 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:30,512 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:31,536 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:31,537 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:32,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:32,952 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:33,975 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:33,976 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:35,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:35,25 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:37,240 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:37,240 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:38,227 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:38,228 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:38,229 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:46:38,247 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:46:38,248 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 41
22:46:38,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:38,943 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:40,677 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:40,678 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:40,678 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:46:40,683 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:46:40,683 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 50
22:46:41,697 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:41,698 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:43,81 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:43,82 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:43,82 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:46:43,86 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:46:43,86 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 43
22:46:44,132 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:44,132 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:45,148 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:45,149 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:45,149 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 195456, Requested 6582. Please try again in 611ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:46:45,155 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:46:45,155 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 49
22:46:47,550 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:47,551 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:48,890 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:46:48,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 9.14100000000326. input_tokens=2550, output_tokens=742
22:46:50,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:50,292 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:51,839 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:51,840 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:51,840 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 194078, Requested 7295. Please try again in 411ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
22:46:51,846 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:46:51,846 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 47
22:46:53,830 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:53,831 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:55,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:46:55,741 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:46:59,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:46:59,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 10.375. input_tokens=3275, output_tokens=786
22:47:00,725 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:00,725 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:03,316 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:03,316 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:03,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:03,995 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:08,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:08,933 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:10,878 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:10,878 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:12,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:47:12,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 14.34299999999348. input_tokens=7555, output_tokens=881
22:47:13,430 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:13,430 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:17,648 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:17,648 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:21,26 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:21,27 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:22,204 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:47:22,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 16.453000000008615. input_tokens=4864, output_tokens=1013
22:47:27,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:27,940 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:29,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:47:29,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 13.344000000011874. input_tokens=6630, output_tokens=861
22:47:31,165 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:31,166 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:34,980 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:47:34,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 11.546999999991385. input_tokens=7180, output_tokens=802
22:47:38,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:38,103 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:48,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:47:48,418 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:47:55,818 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:47:55,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 14.64100000000326. input_tokens=6147, output_tokens=770
22:48:13,159 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:48:13,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 14.719000000011874. input_tokens=9882, output_tokens=956
22:48:14,54 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,56 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,57 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,58 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,97 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,147 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,148 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,151 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,173 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,174 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,179 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,180 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,185 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,186 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,189 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,190 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,201 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,205 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,205 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,220 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,221 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,281 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,282 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,292 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,298 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,299 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,303 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,305 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:14,325 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:14,326 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:15,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:15,520 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:15,525 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:15,526 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:15,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:15,655 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:15,750 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:15,751 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:15,752 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:15,755 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:15,937 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:15,938 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:15,946 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:15,947 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:15,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:15,962 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:16,120 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:16,121 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:16,815 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:16,817 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:17,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:17,819 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:18,262 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:18,263 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:19,122 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:19,125 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:20,168 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:20,169 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:21,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:21,138 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:21,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:21,875 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:22,584 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:22,585 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:23,52 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:48:23,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 7.85899999999674. input_tokens=2277, output_tokens=652
22:48:24,793 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:24,794 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:25,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:48:25,585 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.187999999994645. input_tokens=6465, output_tokens=800
22:48:26,223 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:26,224 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:26,840 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:26,841 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:27,907 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:27,908 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:28,943 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:28,944 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:29,899 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:29,900 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:30,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:30,952 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:33,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:33,352 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:34,298 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:34,298 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:34,335 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:48:34,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 10.797000000020489. input_tokens=3307, output_tokens=819
22:48:35,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:35,105 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:36,165 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:36,167 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:37,312 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:37,313 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:38,526 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:38,527 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:39,450 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:39,451 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:41,9 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:41,10 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:41,182 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:41,185 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:42,338 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:42,338 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:43,677 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:43,678 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:44,447 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:44,448 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:45,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:45,220 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:46,302 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:46,303 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:47,471 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:48:47,474 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 15.297000000020489. input_tokens=3289, output_tokens=835
22:48:47,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:47,557 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:48,891 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:48,892 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:50,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:50,725 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:51,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:51,880 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:53,364 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:53,365 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:54,595 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:54,596 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:54,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:54,959 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:56,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:56,132 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:48:56,434 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:48:56,435 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:00,137 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:00,138 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:00,529 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:49:00,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 11.062000000005355. input_tokens=2971, output_tokens=843
22:49:01,418 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:01,418 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:02,441 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:02,441 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:03,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:03,486 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:04,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:04,757 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:05,612 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:05,613 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:06,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:06,294 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:07,338 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:07,339 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:08,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:08,20 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:10,204 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:10,205 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:11,544 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:11,544 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:12,576 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:12,577 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:12,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:49:12,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 14.59399999998277. input_tokens=6518, output_tokens=889
22:49:13,623 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:13,624 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:14,905 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:14,906 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:15,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:15,622 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:16,453 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:16,453 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:17,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:17,493 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:18,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:18,141 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:19,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:19,382 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:21,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:21,41 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:22,717 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:22,718 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:23,763 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:23,764 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:25,344 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:25,345 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:26,610 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:26,611 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:26,611 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:49:26,616 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:49:26,616 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 14
22:49:27,658 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:27,659 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:27,660 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:49:27,665 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:49:27,665 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 36
22:49:28,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:28,302 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:29,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:29,557 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:31,118 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:31,118 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:32,598 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:32,598 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:35,70 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:35,71 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:35,71 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:49:35,77 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:49:35,77 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 28
22:49:36,619 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:36,620 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:36,633 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:49:36,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 11.719000000011874. input_tokens=2892, output_tokens=779
22:49:37,877 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:37,877 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:39,667 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:39,667 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:40,948 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:40,949 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:43,972 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:43,973 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:46,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:46,369 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:47,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:49:47,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 13.562999999994645. input_tokens=3358, output_tokens=1057
22:49:48,39 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:48,40 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:48,40 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:49:48,45 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:49:48,45 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 13
22:49:49,820 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:49,821 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:53,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:53,99 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:54,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:49:54,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 11.60899999999674. input_tokens=4437, output_tokens=968
22:49:54,813 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:54,814 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:56,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:56,785 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:49:59,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:49:59,276 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:01,221 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:01,222 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:01,222 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:50:01,235 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:50:01,236 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 29
22:50:02,875 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:50:02,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 11.921999999991385. input_tokens=4073, output_tokens=868
22:50:03,278 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:03,279 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:04,963 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:04,964 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:07,81 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:07,82 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:09,623 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:09,624 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:13,432 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:13,433 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:15,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:15,105 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:20,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:20,368 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:23,622 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:23,623 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:28,223 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:50:28,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 10.780999999988126. input_tokens=8240, output_tokens=873
22:50:30,532 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:30,533 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:33,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:50:33,581 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 8.469000000011874. input_tokens=5350, output_tokens=816
22:50:40,756 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:50:40,757 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:50:44,390 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:50:44,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 10.75. input_tokens=7503, output_tokens=802
22:51:08,638 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:51:08,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 17.875. input_tokens=8252, output_tokens=984
22:51:09,193 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:09,194 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:09,237 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:09,238 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:09,238 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:09,239 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:09,244 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:09,245 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:09,250 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:09,251 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:09,261 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:09,262 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:10,462 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:10,472 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:10,497 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:10,498 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:10,619 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:10,620 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:10,864 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:10,865 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:11,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:11,142 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:11,148 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:11,149 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:13,111 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:13,111 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:13,348 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:13,353 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:13,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:13,483 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:13,523 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:13,523 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:13,612 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:13,614 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:13,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:13,873 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:17,317 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:17,318 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:17,982 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:17,982 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:18,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:18,413 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:19,809 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:19,809 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:21,476 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:51:21,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.625. input_tokens=3518, output_tokens=887
22:51:22,351 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:51:22,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.46799999999348. input_tokens=4378, output_tokens=783
22:51:24,468 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:24,469 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:27,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:27,219 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:28,527 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:28,528 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:30,765 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:30,766 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:33,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:33,474 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:37,440 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:37,440 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:37,732 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:51:37,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 11.577999999979511. input_tokens=7598, output_tokens=871
22:51:38,660 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:38,661 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:40,927 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:40,929 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:47,628 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:47,629 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:48,818 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:48,819 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:50,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:51:50,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 14.578000000008615. input_tokens=9062, output_tokens=1046
22:51:51,71 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:51,72 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:57,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:57,864 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:51:59,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:51:59,19 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:52:01,260 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:52:01,260 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:52:03,579 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:52:03,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 18.73399999999674. input_tokens=8836, output_tokens=961
22:52:07,987 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:52:07,987 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:52:11,517 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:52:11,518 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:52:21,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:52:21,719 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:52:21,719 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:52:21,725 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:52:21,726 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 2
22:52:21,811 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:52:21,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 12.780999999988126. input_tokens=5634, output_tokens=1007
22:52:34,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:52:34,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 16.812000000005355. input_tokens=2854, output_tokens=801
22:52:34,860 datashaper.workflow.workflow INFO executing verb window
22:52:34,864 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:52:35,161 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:52:35,183 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:52:35,255 datashaper.workflow.workflow INFO executing verb unroll
22:52:35,284 datashaper.workflow.workflow INFO executing verb select
22:52:35,307 datashaper.workflow.workflow INFO executing verb rename
22:52:35,332 datashaper.workflow.workflow INFO executing verb join
22:52:35,361 datashaper.workflow.workflow INFO executing verb aggregate_override
22:52:35,387 datashaper.workflow.workflow INFO executing verb join
22:52:35,425 datashaper.workflow.workflow INFO executing verb rename
22:52:35,453 datashaper.workflow.workflow INFO executing verb convert
22:52:35,510 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:52:35,744 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:52:35,745 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:52:35,814 datashaper.workflow.workflow INFO executing verb rename
22:52:35,817 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:52:35,919 graphrag.index.cli INFO All workflows completed successfully.
22:53:28,543 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:53:28,547 graphrag.index.cli INFO Starting pipeline run for: 20241201-225328, dryrun=False
22:53:28,548 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:53:28,551 graphrag.index.create_pipeline_config INFO skipping workflows 
22:53:28,551 graphrag.index.run.run INFO Running pipeline
22:53:28,552 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:53:28,553 graphrag.index.input.load_input INFO loading input from root_dir=input
22:53:28,553 graphrag.index.input.load_input INFO using file storage for input
22:53:28,555 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:53:28,556 graphrag.index.input.text INFO found text files from input, found [('11_copy0.txt', {}), ('11_copy1.txt', {}), ('11_copy2.txt', {}), ('11_copy3.txt', {}), ('11_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:53:28,574 graphrag.index.input.text INFO Found 6 files, loading 6
22:53:28,576 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:53:28,576 graphrag.index.run.run INFO Final # of rows loaded: 6
22:53:28,765 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:53:28,774 datashaper.workflow.workflow INFO executing verb orderby
22:53:28,780 datashaper.workflow.workflow INFO executing verb zip
22:53:28,787 datashaper.workflow.workflow INFO executing verb aggregate_override
22:53:28,797 datashaper.workflow.workflow INFO executing verb chunk
22:53:29,312 datashaper.workflow.workflow INFO executing verb select
22:53:29,322 datashaper.workflow.workflow INFO executing verb unroll
22:53:29,333 datashaper.workflow.workflow INFO executing verb rename
22:53:29,342 datashaper.workflow.workflow INFO executing verb genid
22:53:29,357 datashaper.workflow.workflow INFO executing verb unzip
22:53:29,371 datashaper.workflow.workflow INFO executing verb copy
22:53:29,381 datashaper.workflow.workflow INFO executing verb filter
22:53:29,406 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:53:29,699 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:53:29,703 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:53:29,759 datashaper.workflow.workflow INFO executing verb entity_extract
22:53:29,784 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:53:30,333 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:53:30,334 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:53:36,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:53:36,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.14100000000326. input_tokens=2096, output_tokens=366
22:53:39,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:53:39,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.640999999974156. input_tokens=34, output_tokens=238
22:53:39,200 datashaper.workflow.workflow INFO executing verb merge_graphs
22:53:39,427 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:53:39,695 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:53:39,696 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:53:39,743 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:53:41,238 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,239 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,240 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,241 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,243 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,249 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:41,250 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:41,251 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:41,252 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:41,253 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:41,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,273 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:41,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,283 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:41,500 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,542 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:41,544 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:41,547 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:41,562 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:42,330 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:53:42,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3910000000032596. input_tokens=521, output_tokens=138
22:53:42,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:42,471 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:42,543 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:42,543 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:42,580 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:42,582 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:42,730 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:53:42,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7969999999913853. input_tokens=381, output_tokens=218
22:53:42,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:42,827 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:42,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:42,873 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:43,66 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:43,67 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:43,338 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:43,339 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:43,361 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:43,362 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:43,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:43,440 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:43,570 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:53:43,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6560000000172295. input_tokens=1179, output_tokens=281
22:53:44,237 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:53:44,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.312000000005355. input_tokens=969, output_tokens=295
22:53:44,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:53:44,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.172000000020489. input_tokens=229, output_tokens=71
22:53:44,711 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:44,713 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:44,770 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:44,772 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:44,949 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:44,955 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:45,61 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:45,64 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:45,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:45,255 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:45,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:45,576 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:46,267 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:46,269 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:46,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:46,284 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:46,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:46,355 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:49,121 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:49,122 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:49,212 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:49,213 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:49,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:49,579 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:49,684 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:49,684 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:49,952 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:49,953 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:50,564 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:50,565 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:50,985 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:50,986 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:51,173 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:51,173 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:51,336 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:51,337 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:57,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:57,725 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:57,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:57,988 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:58,344 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:58,344 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:58,671 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:58,672 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:58,729 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:58,730 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:59,307 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:59,308 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:53:59,404 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:53:59,404 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:00,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:00,114 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:00,208 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:00,208 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:08,169 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:08,169 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:08,466 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:08,467 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:08,817 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:08,818 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:08,966 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:08,967 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:09,45 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:54:09,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 5 retries took 1.2969999999913853. input_tokens=219, output_tokens=105
22:54:09,544 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:09,545 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:10,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:10,257 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:10,387 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:10,388 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:11,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:54:11,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 5 retries took 1.9530000000086147. input_tokens=362, output_tokens=189
22:54:18,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:18,350 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:18,603 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:18,603 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:18,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:18,965 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:19,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:19,156 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:19,674 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:19,674 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:20,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:20,410 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:20,507 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:20,508 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:28,802 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:28,803 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:29,128 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:29,129 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:29,312 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:29,313 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:29,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:29,806 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:30,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:54:30,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 7 retries took 1.937000000005355. input_tokens=254, output_tokens=104
22:54:30,516 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:30,521 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:30,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:30,664 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:39,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:39,331 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:39,485 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:39,486 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:39,955 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:39,957 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:40,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:54:40,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 8 retries took 1.25. input_tokens=211, output_tokens=87
22:54:40,666 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:40,672 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:40,803 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:40,804 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:49,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
22:54:49,502 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
22:54:49,503 datashaper.workflow.workflow ERROR Error executing verb "summarize_descriptions" in create_summarized_entities: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\datashaper\workflow\workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\description_summarize.py", line 184, in summarize_descriptions
    await get_resolved_entities(row, semaphore) for row in output.itertuples()
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\description_summarize.py", line 147, in get_resolved_entities
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\description_summarize.py", line 167, in do_summarize_descriptions
    results = await strategy_exec(
              ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\strategies\graph_intelligence\run_graph_intelligence.py", line 34, in run
    return await run_summarize_descriptions(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\strategies\graph_intelligence\run_graph_intelligence.py", line 67, in run_summarize_descriptions
    result = await extractor(items=items, descriptions=descriptions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\summarize\description_summary_extractor.py", line 73, in __call__
    result = await self._summarize_descriptions(items, descriptions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\summarize\description_summary_extractor.py", line 110, in _summarize_descriptions
    result = await self._summarize_descriptions_with_llm(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\summarize\description_summary_extractor.py", line 129, in _summarize_descriptions_with_llm
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:54:49,528 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "summarize_descriptions" in create_summarized_entities: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}} details=None
22:54:49,538 graphrag.index.run.run ERROR error running workflow create_summarized_entities
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\run\run.py", line 227, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\run\workflow.py", line 91, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\datashaper\workflow\workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\datashaper\workflow\workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\description_summarize.py", line 184, in summarize_descriptions
    await get_resolved_entities(row, semaphore) for row in output.itertuples()
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\description_summarize.py", line 147, in get_resolved_entities
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\description_summarize.py", line 167, in do_summarize_descriptions
    results = await strategy_exec(
              ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\strategies\graph_intelligence\run_graph_intelligence.py", line 34, in run
    return await run_summarize_descriptions(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\verbs\entities\summarize\strategies\graph_intelligence\run_graph_intelligence.py", line 67, in run_summarize_descriptions
    result = await extractor(items=items, descriptions=descriptions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\summarize\description_summary_extractor.py", line 73, in __call__
    result = await self._summarize_descriptions(items, descriptions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\summarize\description_summary_extractor.py", line 110, in _summarize_descriptions
    result = await self._summarize_descriptions_with_llm(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\summarize\description_summary_extractor.py", line 129, in _summarize_descriptions_with_llm
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
22:54:49,547 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
22:54:49,584 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
22:55:42,871 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
22:55:42,876 graphrag.index.cli INFO Starting pipeline run for: 20241201-225542, dryrun=False
22:55:42,877 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:55:42,880 graphrag.index.create_pipeline_config INFO skipping workflows 
22:55:42,880 graphrag.index.run.run INFO Running pipeline
22:55:42,880 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
22:55:42,882 graphrag.index.input.load_input INFO loading input from root_dir=input
22:55:42,882 graphrag.index.input.load_input INFO using file storage for input
22:55:42,884 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
22:55:42,885 graphrag.index.input.text INFO found text files from input, found [('12_copy0.txt', {}), ('12_copy1.txt', {}), ('12_copy2.txt', {}), ('12_copy3.txt', {}), ('12_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
22:55:42,907 graphrag.index.input.text INFO Found 6 files, loading 6
22:55:42,910 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:55:42,917 graphrag.index.run.run INFO Final # of rows loaded: 6
22:55:43,135 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:55:43,142 datashaper.workflow.workflow INFO executing verb orderby
22:55:43,148 datashaper.workflow.workflow INFO executing verb zip
22:55:43,157 datashaper.workflow.workflow INFO executing verb aggregate_override
22:55:43,167 datashaper.workflow.workflow INFO executing verb chunk
22:55:43,664 datashaper.workflow.workflow INFO executing verb select
22:55:43,672 datashaper.workflow.workflow INFO executing verb unroll
22:55:43,681 datashaper.workflow.workflow INFO executing verb rename
22:55:43,689 datashaper.workflow.workflow INFO executing verb genid
22:55:43,705 datashaper.workflow.workflow INFO executing verb unzip
22:55:43,716 datashaper.workflow.workflow INFO executing verb copy
22:55:43,723 datashaper.workflow.workflow INFO executing verb filter
22:55:43,745 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:55:43,976 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:55:43,977 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:55:44,26 datashaper.workflow.workflow INFO executing verb entity_extract
22:55:44,47 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:55:44,600 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
22:55:44,600 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
22:55:48,136 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:55:48,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.0320000000065193. input_tokens=2106, output_tokens=228
22:55:52,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:55:52,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.14000000001397. input_tokens=34, output_tokens=309
22:55:52,305 datashaper.workflow.workflow INFO executing verb merge_graphs
22:55:52,588 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:55:52,921 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:55:52,922 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:55:52,979 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:55:55,527 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:55:55,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.109000000025844. input_tokens=193, output_tokens=75
22:55:57,654 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:55:57,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.453000000008615. input_tokens=1470, output_tokens=400
22:55:57,722 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:55:57,983 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:55:57,984 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:55:58,21 datashaper.workflow.workflow INFO executing verb cluster_graph
22:55:58,688 datashaper.workflow.workflow INFO executing verb select
22:55:58,697 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:55:58,993 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:55:58,994 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:55:59,41 datashaper.workflow.workflow INFO executing verb unpack_graph
22:55:59,370 datashaper.workflow.workflow INFO executing verb rename
22:55:59,384 datashaper.workflow.workflow INFO executing verb select
22:55:59,399 datashaper.workflow.workflow INFO executing verb dedupe
22:55:59,422 datashaper.workflow.workflow INFO executing verb rename
22:55:59,437 datashaper.workflow.workflow INFO executing verb filter
22:55:59,494 datashaper.workflow.workflow INFO executing verb text_split
22:55:59,526 datashaper.workflow.workflow INFO executing verb drop
22:55:59,544 datashaper.workflow.workflow INFO executing verb merge
22:55:59,709 datashaper.workflow.workflow INFO executing verb text_embed
22:55:59,711 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:56:00,302 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:56:00,302 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:56:00,404 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 409 inputs via 409 snippets using 26 batches. max_batch_size=16, max_tokens=8191
22:56:01,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:56:01,232 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8120000000053551. input_tokens=837, output_tokens=0
22:56:01,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:56:01,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:56:01,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9220000000204891. input_tokens=1896, output_tokens=0
22:56:01,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0150000000139698. input_tokens=219, output_tokens=0
22:56:01,506 datashaper.workflow.workflow INFO executing verb drop
22:56:01,536 datashaper.workflow.workflow INFO executing verb filter
22:56:01,583 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:56:01,998 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:56:01,998 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:56:02,49 datashaper.workflow.workflow INFO executing verb layout_graph
22:56:03,515 datashaper.workflow.workflow INFO executing verb unpack_graph
22:56:03,957 datashaper.workflow.workflow INFO executing verb unpack_graph
22:56:04,306 datashaper.workflow.workflow INFO executing verb drop
22:56:04,326 datashaper.workflow.workflow INFO executing verb filter
22:56:04,406 datashaper.workflow.workflow INFO executing verb select
22:56:04,438 datashaper.workflow.workflow INFO executing verb rename
22:56:04,727 datashaper.workflow.workflow INFO executing verb join
22:56:04,775 datashaper.workflow.workflow INFO executing verb convert
22:56:04,848 datashaper.workflow.workflow INFO executing verb rename
22:56:04,855 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:56:05,251 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:56:05,252 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:56:05,322 datashaper.workflow.workflow INFO executing verb create_final_communities
22:56:06,84 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:56:06,453 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:56:06,454 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:56:06,465 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:56:06,530 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:56:06,871 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:56:06,883 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:56:07,128 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
22:56:07,129 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:56:07,139 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:56:07,188 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:56:07,255 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:56:07,328 datashaper.workflow.workflow INFO executing verb select
22:56:07,332 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:56:07,590 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:56:07,591 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:56:07,600 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:56:07,661 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:56:07,713 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:56:07,759 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:56:07,800 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:56:07,801 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 409
22:56:07,840 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 409
22:56:08,21 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 409
22:56:08,711 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 409
22:56:09,22 datashaper.workflow.workflow INFO executing verb create_community_reports
22:56:21,627 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:56:21,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.578000000008615. input_tokens=4150, output_tokens=825
22:56:34,57 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
22:56:34,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.01500000001397. input_tokens=3977, output_tokens=808
23:58:10,433 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
23:58:10,437 graphrag.index.cli INFO Starting pipeline run for: 20241201-235810, dryrun=False
23:58:10,437 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
23:58:10,442 graphrag.index.create_pipeline_config INFO skipping workflows 
23:58:10,442 graphrag.index.run.run INFO Running pipeline
23:58:10,442 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
23:58:10,444 graphrag.index.input.load_input INFO loading input from root_dir=input
23:58:10,444 graphrag.index.input.load_input INFO using file storage for input
23:58:10,446 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
23:58:10,447 graphrag.index.input.text INFO found text files from input, found [('1_copy0.txt', {}), ('1_copy1.txt', {}), ('1_copy2.txt', {}), ('1_copy3.txt', {}), ('1_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
23:58:10,467 graphrag.index.input.text INFO Found 6 files, loading 6
23:58:10,470 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
23:58:10,471 graphrag.index.run.run INFO Final # of rows loaded: 6
23:58:10,649 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:58:10,657 datashaper.workflow.workflow INFO executing verb orderby
23:58:10,662 datashaper.workflow.workflow INFO executing verb zip
23:58:10,668 datashaper.workflow.workflow INFO executing verb aggregate_override
23:58:10,678 datashaper.workflow.workflow INFO executing verb chunk
23:58:11,125 datashaper.workflow.workflow INFO executing verb select
23:58:11,134 datashaper.workflow.workflow INFO executing verb unroll
23:58:11,147 datashaper.workflow.workflow INFO executing verb rename
23:58:11,155 datashaper.workflow.workflow INFO executing verb genid
23:58:11,173 datashaper.workflow.workflow INFO executing verb unzip
23:58:11,187 datashaper.workflow.workflow INFO executing verb copy
23:58:11,196 datashaper.workflow.workflow INFO executing verb filter
23:58:11,233 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:58:11,523 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
23:58:11,524 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:58:11,563 datashaper.workflow.workflow INFO executing verb entity_extract
23:58:11,587 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
23:58:12,37 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
23:58:12,37 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
23:58:14,975 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:14,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.4839999999967404. input_tokens=2101, output_tokens=165
23:58:18,524 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:18,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.547000000020489. input_tokens=34, output_tokens=363
23:58:18,559 datashaper.workflow.workflow INFO executing verb merge_graphs
23:58:18,807 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
23:58:19,34 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
23:58:19,34 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
23:58:19,71 datashaper.workflow.workflow INFO executing verb summarize_descriptions
23:58:21,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:21,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.827999999979511. input_tokens=460, output_tokens=148
23:58:21,57 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:21,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.375. input_tokens=167, output_tokens=96
23:58:21,67 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:21,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8440000000118744. input_tokens=272, output_tokens=133
23:58:22,666 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:22,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.875. input_tokens=479, output_tokens=175
23:58:22,739 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
23:58:22,950 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
23:58:22,951 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
23:58:22,986 datashaper.workflow.workflow INFO executing verb cluster_graph
23:58:23,595 datashaper.workflow.workflow INFO executing verb select
23:58:23,601 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:58:23,878 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:58:23,879 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:58:23,929 datashaper.workflow.workflow INFO executing verb unpack_graph
23:58:24,272 datashaper.workflow.workflow INFO executing verb rename
23:58:24,287 datashaper.workflow.workflow INFO executing verb select
23:58:24,307 datashaper.workflow.workflow INFO executing verb dedupe
23:58:24,327 datashaper.workflow.workflow INFO executing verb rename
23:58:24,350 datashaper.workflow.workflow INFO executing verb filter
23:58:24,404 datashaper.workflow.workflow INFO executing verb text_split
23:58:24,443 datashaper.workflow.workflow INFO executing verb drop
23:58:24,469 datashaper.workflow.workflow INFO executing verb merge
23:58:24,669 datashaper.workflow.workflow INFO executing verb text_embed
23:58:24,672 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
23:58:25,274 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
23:58:25,274 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
23:58:25,343 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 406 inputs via 406 snippets using 26 batches. max_batch_size=16, max_tokens=8191
23:58:26,153 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
23:58:26,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
23:58:26,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9379999999946449. input_tokens=216, output_tokens=0
23:58:26,350 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
23:58:26,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0780000000086147. input_tokens=951, output_tokens=0
23:58:26,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.125. input_tokens=1882, output_tokens=0
23:58:26,555 datashaper.workflow.workflow INFO executing verb drop
23:58:26,582 datashaper.workflow.workflow INFO executing verb filter
23:58:26,633 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:58:27,24 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:58:27,25 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:58:27,72 datashaper.workflow.workflow INFO executing verb layout_graph
23:58:28,439 datashaper.workflow.workflow INFO executing verb unpack_graph
23:58:28,894 datashaper.workflow.workflow INFO executing verb unpack_graph
23:58:29,442 datashaper.workflow.workflow INFO executing verb drop
23:58:29,466 datashaper.workflow.workflow INFO executing verb filter
23:58:29,610 datashaper.workflow.workflow INFO executing verb select
23:58:29,646 datashaper.workflow.workflow INFO executing verb rename
23:58:29,677 datashaper.workflow.workflow INFO executing verb join
23:58:29,730 datashaper.workflow.workflow INFO executing verb convert
23:58:29,878 datashaper.workflow.workflow INFO executing verb rename
23:58:29,883 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:58:30,203 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:58:30,204 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:58:30,262 datashaper.workflow.workflow INFO executing verb create_final_communities
23:58:31,262 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:58:31,594 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
23:58:31,601 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:58:31,619 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:58:31,705 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
23:58:32,202 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
23:58:32,222 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:58:32,523 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
23:58:32,524 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:58:32,531 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:58:32,587 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:58:32,700 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
23:58:32,805 datashaper.workflow.workflow INFO executing verb select
23:58:32,808 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:58:33,118 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
23:58:33,119 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:58:33,128 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:58:33,189 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
23:58:33,253 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
23:58:33,304 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
23:58:33,344 datashaper.workflow.workflow INFO executing verb prepare_community_reports
23:58:33,345 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 406
23:58:33,449 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 406
23:58:33,790 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 406
23:58:34,98 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 406
23:58:34,359 datashaper.workflow.workflow INFO executing verb create_community_reports
23:58:43,33 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:43,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.609000000025844. input_tokens=2550, output_tokens=666
23:58:45,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:45,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.187000000005355. input_tokens=3610, output_tokens=814
23:58:46,21 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:46,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.64000000001397. input_tokens=6674, output_tokens=848
23:58:49,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:58:49,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.96899999998277. input_tokens=7446, output_tokens=1104
23:59:01,170 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:01,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.155999999988126. input_tokens=2254, output_tokens=686
23:59:01,351 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:01,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.389999999984866. input_tokens=2150, output_tokens=655
23:59:02,26 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:02,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.047000000020489. input_tokens=2390, output_tokens=744
23:59:02,308 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:02,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.25. input_tokens=2660, output_tokens=742
23:59:02,737 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:02,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.73399999999674. input_tokens=2368, output_tokens=674
23:59:02,947 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:02,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.90600000001723. input_tokens=3989, output_tokens=775
23:59:02,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:02,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.921999999991385. input_tokens=2652, output_tokens=817
23:59:03,126 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:03,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.062999999994645. input_tokens=4252, output_tokens=789
23:59:03,181 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:03,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.23399999999674. input_tokens=4973, output_tokens=788
23:59:03,194 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:03,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.10899999999674. input_tokens=3807, output_tokens=778
23:59:03,348 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:03,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.25. input_tokens=3030, output_tokens=825
23:59:03,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:03,419 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.390999999974156. input_tokens=3617, output_tokens=819
23:59:03,806 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:03,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.828000000008615. input_tokens=3210, output_tokens=813
23:59:03,970 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:03,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.875. input_tokens=3012, output_tokens=831
23:59:04,136 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:04,142 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.203999999997905. input_tokens=4362, output_tokens=885
23:59:04,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:04,201 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:04,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.25. input_tokens=8088, output_tokens=872
23:59:04,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.172000000020489. input_tokens=3289, output_tokens=827
23:59:04,952 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:04,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.98399999999674. input_tokens=4970, output_tokens=906
23:59:08,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:08,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.703000000008615. input_tokens=9863, output_tokens=864
23:59:09,355 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:09,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.34299999999348. input_tokens=7896, output_tokens=824
23:59:10,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:10,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.26600000000326. input_tokens=3706, output_tokens=835
23:59:11,888 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:11,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.89100000000326. input_tokens=5908, output_tokens=948
23:59:12,817 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:12,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.76600000000326. input_tokens=4369, output_tokens=832
23:59:14,681 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:14,686 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.562000000005355. input_tokens=9886, output_tokens=1086
23:59:15,703 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
23:59:15,704 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:59:22,161 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:22,163 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.265999999974156. input_tokens=2102, output_tokens=492
23:59:25,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:25,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.219000000011874. input_tokens=2277, output_tokens=673
23:59:26,86 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:26,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.03200000000652. input_tokens=3382, output_tokens=754
23:59:26,580 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:26,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.578000000008615. input_tokens=2225, output_tokens=661
23:59:26,703 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:26,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.60899999999674. input_tokens=7717, output_tokens=779
23:59:27,63 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:27,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.030999999988126. input_tokens=3767, output_tokens=819
23:59:27,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:27,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.296999999991385. input_tokens=4010, output_tokens=821
23:59:27,527 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:27,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.453000000008615. input_tokens=3719, output_tokens=864
23:59:27,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:27,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.60899999999674. input_tokens=2448, output_tokens=680
23:59:27,719 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:27,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.65700000000652. input_tokens=3518, output_tokens=851
23:59:27,746 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:27,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.73399999999674. input_tokens=3908, output_tokens=874
23:59:28,5 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:28,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.85999999998603. input_tokens=5772, output_tokens=869
23:59:28,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:28,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.359000000025844. input_tokens=5350, output_tokens=928
23:59:29,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:29,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.172000000020489. input_tokens=3342, output_tokens=793
23:59:30,87 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:30,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.047000000020489. input_tokens=3971, output_tokens=852
23:59:30,240 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:30,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.09399999998277. input_tokens=5902, output_tokens=864
23:59:30,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:30,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 13.187999999994645. input_tokens=2217, output_tokens=781
23:59:31,114 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:31,116 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.171999999991385. input_tokens=8718, output_tokens=904
23:59:32,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:32,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.0. input_tokens=7465, output_tokens=900
23:59:33,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:33,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.10899999999674. input_tokens=6755, output_tokens=990
23:59:35,177 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:35,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.09299999999348. input_tokens=2636, output_tokens=750
23:59:35,673 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:35,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.530999999988126. input_tokens=5289, output_tokens=1082
23:59:44,5 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:44,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.03200000000652. input_tokens=2060, output_tokens=493
23:59:45,781 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:45,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.78100000001723. input_tokens=2971, output_tokens=751
23:59:46,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:46,425 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.46899999998277. input_tokens=2854, output_tokens=807
23:59:47,195 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:47,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.187000000005355. input_tokens=2340, output_tokens=768
23:59:47,883 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:47,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.89100000000326. input_tokens=3309, output_tokens=771
23:59:48,475 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:48,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.514999999984866. input_tokens=6636, output_tokens=911
23:59:49,103 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:49,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.030999999988126. input_tokens=8131, output_tokens=866
23:59:51,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:51,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.90700000000652. input_tokens=9343, output_tokens=905
23:59:52,890 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:52,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.937999999994645. input_tokens=9565, output_tokens=986
23:59:53,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
23:59:53,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.594000000011874. input_tokens=7507, output_tokens=1001
23:59:53,689 datashaper.workflow.workflow INFO executing verb window
23:59:53,693 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:59:53,963 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
23:59:53,964 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:59:54,37 datashaper.workflow.workflow INFO executing verb unroll
23:59:54,69 datashaper.workflow.workflow INFO executing verb select
23:59:54,101 datashaper.workflow.workflow INFO executing verb rename
23:59:54,130 datashaper.workflow.workflow INFO executing verb join
23:59:54,178 datashaper.workflow.workflow INFO executing verb aggregate_override
23:59:54,207 datashaper.workflow.workflow INFO executing verb join
23:59:54,247 datashaper.workflow.workflow INFO executing verb rename
23:59:54,276 datashaper.workflow.workflow INFO executing verb convert
23:59:54,398 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
23:59:54,691 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
23:59:54,712 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
23:59:54,799 datashaper.workflow.workflow INFO executing verb rename
23:59:54,802 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:59:54,957 graphrag.index.cli INFO All workflows completed successfully.
00:00:46,469 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
00:00:46,479 graphrag.index.cli INFO Starting pipeline run for: 20241202-000046, dryrun=False
00:00:46,481 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
00:00:46,487 graphrag.index.create_pipeline_config INFO skipping workflows 
00:00:46,489 graphrag.index.run.run INFO Running pipeline
00:00:46,490 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
00:00:46,491 graphrag.index.input.load_input INFO loading input from root_dir=input
00:00:46,492 graphrag.index.input.load_input INFO using file storage for input
00:00:46,497 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
00:00:46,502 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
00:00:46,528 graphrag.index.input.text INFO Found 6 files, loading 6
00:00:46,534 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
00:00:46,536 graphrag.index.run.run INFO Final # of rows loaded: 6
00:00:46,811 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
00:00:46,819 datashaper.workflow.workflow INFO executing verb orderby
00:00:46,825 datashaper.workflow.workflow INFO executing verb zip
00:00:46,832 datashaper.workflow.workflow INFO executing verb aggregate_override
00:00:46,843 datashaper.workflow.workflow INFO executing verb chunk
00:00:47,361 datashaper.workflow.workflow INFO executing verb select
00:00:47,367 datashaper.workflow.workflow INFO executing verb unroll
00:00:47,376 datashaper.workflow.workflow INFO executing verb rename
00:00:47,383 datashaper.workflow.workflow INFO executing verb genid
00:00:47,401 datashaper.workflow.workflow INFO executing verb unzip
00:00:47,412 datashaper.workflow.workflow INFO executing verb copy
00:00:47,422 datashaper.workflow.workflow INFO executing verb filter
00:00:47,462 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
00:00:47,709 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
00:00:47,710 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:00:47,751 datashaper.workflow.workflow INFO executing verb entity_extract
00:00:47,772 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
00:00:48,307 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
00:00:48,307 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
00:00:51,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:00:51,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.1560000000172295. input_tokens=2056, output_tokens=137
00:00:53,688 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:00:53,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.1559999999881256. input_tokens=34, output_tokens=98
00:00:53,718 datashaper.workflow.workflow INFO executing verb merge_graphs
00:00:53,953 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
00:00:54,195 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
00:00:54,196 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
00:00:54,258 datashaper.workflow.workflow INFO executing verb summarize_descriptions
00:00:56,11 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:00:56,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5469999999913853. input_tokens=449, output_tokens=152
00:00:56,461 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:00:56,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5. input_tokens=183, output_tokens=61
00:00:56,532 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
00:00:56,749 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
00:00:56,756 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
00:00:56,819 datashaper.workflow.workflow INFO executing verb cluster_graph
00:00:57,753 datashaper.workflow.workflow INFO executing verb select
00:00:57,759 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
00:00:58,192 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
00:00:58,192 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:00:58,263 datashaper.workflow.workflow INFO executing verb unpack_graph
00:00:58,686 datashaper.workflow.workflow INFO executing verb rename
00:00:58,708 datashaper.workflow.workflow INFO executing verb select
00:00:58,731 datashaper.workflow.workflow INFO executing verb dedupe
00:00:58,756 datashaper.workflow.workflow INFO executing verb rename
00:00:58,775 datashaper.workflow.workflow INFO executing verb filter
00:00:58,847 datashaper.workflow.workflow INFO executing verb text_split
00:00:58,879 datashaper.workflow.workflow INFO executing verb drop
00:00:58,902 datashaper.workflow.workflow INFO executing verb merge
00:00:59,151 datashaper.workflow.workflow INFO executing verb text_embed
00:00:59,153 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
00:00:59,762 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
00:00:59,762 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
00:00:59,826 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
00:01:00,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:01:00,672 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:01:00,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9070000000065193. input_tokens=1608, output_tokens=0
00:01:00,830 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:01:00,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.985000000015134. input_tokens=1487, output_tokens=0
00:01:01,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1570000000065193. input_tokens=1738, output_tokens=0
00:01:01,73 datashaper.workflow.workflow INFO executing verb drop
00:01:01,89 datashaper.workflow.workflow INFO executing verb filter
00:01:01,132 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
00:01:01,479 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
00:01:01,480 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:01:01,526 datashaper.workflow.workflow INFO executing verb layout_graph
00:01:02,693 datashaper.workflow.workflow INFO executing verb unpack_graph
00:01:03,55 datashaper.workflow.workflow INFO executing verb unpack_graph
00:01:03,407 datashaper.workflow.workflow INFO executing verb filter
00:01:03,480 datashaper.workflow.workflow INFO executing verb drop
00:01:03,502 datashaper.workflow.workflow INFO executing verb select
00:01:03,522 datashaper.workflow.workflow INFO executing verb rename
00:01:03,548 datashaper.workflow.workflow INFO executing verb join
00:01:03,587 datashaper.workflow.workflow INFO executing verb convert
00:01:03,668 datashaper.workflow.workflow INFO executing verb rename
00:01:03,671 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
00:01:03,956 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
00:01:03,957 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:01:04,9 datashaper.workflow.workflow INFO executing verb create_final_communities
00:01:04,770 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
00:01:05,44 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
00:01:05,45 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:01:05,55 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
00:01:05,116 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
00:01:05,535 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
00:01:05,548 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
00:01:05,817 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
00:01:05,818 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
00:01:05,862 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
00:01:05,879 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:01:05,933 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
00:01:05,995 datashaper.workflow.workflow INFO executing verb select
00:01:05,998 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
00:01:06,289 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
00:01:06,295 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
00:01:06,302 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
00:01:06,363 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
00:01:06,420 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
00:01:06,464 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
00:01:06,502 datashaper.workflow.workflow INFO executing verb prepare_community_reports
00:01:06,503 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 405
00:01:06,551 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
00:01:07,34 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
00:01:07,614 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
00:01:07,872 datashaper.workflow.workflow INFO executing verb create_community_reports
00:08:01,528 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
00:08:01,531 graphrag.index.cli INFO Starting pipeline run for: 20241202-000801, dryrun=False
00:08:01,532 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
00:08:01,534 graphrag.index.create_pipeline_config INFO skipping workflows 
00:08:01,535 graphrag.index.run.run INFO Running pipeline
00:08:01,535 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
00:08:01,536 graphrag.index.input.load_input INFO loading input from root_dir=input
00:08:01,536 graphrag.index.input.load_input INFO using file storage for input
00:08:01,538 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
00:08:01,539 graphrag.index.input.text INFO found text files from input, found [('1_copy0.txt', {}), ('1_copy1.txt', {}), ('1_copy2.txt', {}), ('1_copy3.txt', {}), ('1_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
00:08:01,553 graphrag.index.input.text INFO Found 6 files, loading 6
00:08:01,555 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
00:08:01,556 graphrag.index.run.run INFO Final # of rows loaded: 6
00:08:01,691 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
00:08:01,697 datashaper.workflow.workflow INFO executing verb orderby
00:08:01,701 datashaper.workflow.workflow INFO executing verb zip
00:08:01,705 datashaper.workflow.workflow INFO executing verb aggregate_override
00:08:01,712 datashaper.workflow.workflow INFO executing verb chunk
00:08:02,56 datashaper.workflow.workflow INFO executing verb select
00:08:02,62 datashaper.workflow.workflow INFO executing verb unroll
00:08:02,71 datashaper.workflow.workflow INFO executing verb rename
00:08:02,77 datashaper.workflow.workflow INFO executing verb genid
00:08:02,92 datashaper.workflow.workflow INFO executing verb unzip
00:08:02,101 datashaper.workflow.workflow INFO executing verb copy
00:08:02,108 datashaper.workflow.workflow INFO executing verb filter
00:08:02,126 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
00:08:02,336 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
00:08:02,336 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:08:02,371 datashaper.workflow.workflow INFO executing verb entity_extract
00:08:02,387 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
00:08:02,831 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
00:08:02,831 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
00:08:03,325 datashaper.workflow.workflow INFO executing verb merge_graphs
00:08:03,498 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
00:08:03,677 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
00:08:03,680 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
00:08:03,710 datashaper.workflow.workflow INFO executing verb summarize_descriptions
00:08:04,618 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
00:08:04,793 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
00:08:04,793 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
00:08:04,825 datashaper.workflow.workflow INFO executing verb cluster_graph
00:08:05,319 datashaper.workflow.workflow INFO executing verb select
00:08:05,323 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
00:08:05,522 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
00:08:05,523 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:08:05,559 datashaper.workflow.workflow INFO executing verb unpack_graph
00:08:05,792 datashaper.workflow.workflow INFO executing verb rename
00:08:05,803 datashaper.workflow.workflow INFO executing verb select
00:08:05,828 datashaper.workflow.workflow INFO executing verb dedupe
00:08:05,843 datashaper.workflow.workflow INFO executing verb rename
00:08:05,856 datashaper.workflow.workflow INFO executing verb filter
00:08:05,894 datashaper.workflow.workflow INFO executing verb text_split
00:08:05,916 datashaper.workflow.workflow INFO executing verb drop
00:08:05,933 datashaper.workflow.workflow INFO executing verb merge
00:08:06,62 datashaper.workflow.workflow INFO executing verb text_embed
00:08:06,64 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
00:08:06,507 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
00:08:06,507 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
00:08:06,566 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 406 inputs via 406 snippets using 26 batches. max_batch_size=16, max_tokens=8191
00:08:07,87 datashaper.workflow.workflow INFO executing verb drop
00:08:07,101 datashaper.workflow.workflow INFO executing verb filter
00:08:07,130 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
00:08:07,410 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
00:08:07,410 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:08:07,449 datashaper.workflow.workflow INFO executing verb layout_graph
00:08:08,348 datashaper.workflow.workflow INFO executing verb unpack_graph
00:08:08,605 datashaper.workflow.workflow INFO executing verb unpack_graph
00:08:08,870 datashaper.workflow.workflow INFO executing verb filter
00:08:08,941 datashaper.workflow.workflow INFO executing verb drop
00:08:08,959 datashaper.workflow.workflow INFO executing verb select
00:08:08,979 datashaper.workflow.workflow INFO executing verb rename
00:08:09,3 datashaper.workflow.workflow INFO executing verb convert
00:08:09,94 datashaper.workflow.workflow INFO executing verb join
00:08:09,124 datashaper.workflow.workflow INFO executing verb rename
00:08:09,128 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
00:08:09,338 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
00:08:09,339 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:08:09,381 datashaper.workflow.workflow INFO executing verb create_final_communities
00:08:09,919 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
00:08:10,163 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
00:08:10,172 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
00:08:10,189 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:08:10,239 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
00:08:10,508 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
00:08:10,518 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
00:08:10,732 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
00:08:10,732 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
00:08:10,747 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
00:08:10,792 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:08:10,836 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
00:08:10,905 datashaper.workflow.workflow INFO executing verb select
00:08:10,908 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
00:08:11,117 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
00:08:11,117 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
00:08:11,124 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
00:08:11,174 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
00:08:11,243 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
00:08:11,282 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
00:08:11,323 datashaper.workflow.workflow INFO executing verb prepare_community_reports
00:08:11,324 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 406
00:08:11,372 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 406
00:08:11,565 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 406
00:08:11,786 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 406
00:08:11,989 datashaper.workflow.workflow INFO executing verb create_community_reports
00:08:26,61 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:08:26,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.046000000002095. input_tokens=6667, output_tokens=866
00:08:42,279 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:08:42,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.672000000020489. input_tokens=4379, output_tokens=738
00:08:45,669 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:08:45,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.07799999997951. input_tokens=8081, output_tokens=847
00:08:46,972 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:08:46,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.312999999994645. input_tokens=9889, output_tokens=913
00:08:55,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:08:55,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.328000000008615. input_tokens=4248, output_tokens=814
00:09:09,592 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:09,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.89100000000326. input_tokens=9856, output_tokens=925
00:09:21,676 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:21,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.875. input_tokens=5871, output_tokens=842
00:09:25,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:25,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.344000000011874. input_tokens=7713, output_tokens=833
00:09:26,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:26,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.98499999998603. input_tokens=5152, output_tokens=915
00:09:30,63 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:30,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.280999999988126. input_tokens=3891, output_tokens=892
00:09:32,727 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:32,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.905999999988126. input_tokens=5843, output_tokens=939
00:09:55,26 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:55,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.062000000005355. input_tokens=7357, output_tokens=917
00:09:57,387 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.40700000000652. input_tokens=8258, output_tokens=924
00:09:58,981 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.030999999988126. input_tokens=9548, output_tokens=1020
00:09:59,53 datashaper.workflow.workflow INFO executing verb window
00:09:59,57 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
00:09:59,390 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
00:09:59,390 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
00:09:59,475 datashaper.workflow.workflow INFO executing verb unroll
00:09:59,532 datashaper.workflow.workflow INFO executing verb select
00:09:59,563 datashaper.workflow.workflow INFO executing verb rename
00:09:59,608 datashaper.workflow.workflow INFO executing verb join
00:09:59,650 datashaper.workflow.workflow INFO executing verb aggregate_override
00:09:59,688 datashaper.workflow.workflow INFO executing verb join
00:09:59,745 datashaper.workflow.workflow INFO executing verb rename
00:09:59,779 datashaper.workflow.workflow INFO executing verb convert
00:09:59,894 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
00:10:00,193 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
00:10:00,194 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
00:10:00,279 datashaper.workflow.workflow INFO executing verb rename
00:10:00,282 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
00:10:00,425 graphrag.index.cli INFO All workflows completed successfully.
00:10:49,479 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
00:10:49,483 graphrag.index.cli INFO Starting pipeline run for: 20241202-001049, dryrun=False
00:10:49,484 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
00:10:49,487 graphrag.index.create_pipeline_config INFO skipping workflows 
00:10:49,487 graphrag.index.run.run INFO Running pipeline
00:10:49,487 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
00:10:49,488 graphrag.index.input.load_input INFO loading input from root_dir=input
00:10:49,488 graphrag.index.input.load_input INFO using file storage for input
00:10:49,490 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
00:10:49,491 graphrag.index.input.text INFO found text files from input, found [('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
00:10:49,517 graphrag.index.input.text INFO Found 6 files, loading 6
00:10:49,519 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
00:10:49,519 graphrag.index.run.run INFO Final # of rows loaded: 6
00:10:49,675 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
00:10:49,682 datashaper.workflow.workflow INFO executing verb orderby
00:10:49,687 datashaper.workflow.workflow INFO executing verb zip
00:10:49,693 datashaper.workflow.workflow INFO executing verb aggregate_override
00:10:49,703 datashaper.workflow.workflow INFO executing verb chunk
00:10:50,103 datashaper.workflow.workflow INFO executing verb select
00:10:50,109 datashaper.workflow.workflow INFO executing verb unroll
00:10:50,116 datashaper.workflow.workflow INFO executing verb rename
00:10:50,124 datashaper.workflow.workflow INFO executing verb genid
00:10:50,141 datashaper.workflow.workflow INFO executing verb unzip
00:10:50,154 datashaper.workflow.workflow INFO executing verb copy
00:10:50,165 datashaper.workflow.workflow INFO executing verb filter
00:10:50,197 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
00:10:50,412 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
00:10:50,413 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:10:50,451 datashaper.workflow.workflow INFO executing verb entity_extract
00:10:50,469 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
00:10:50,971 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
00:10:50,971 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
00:10:51,457 datashaper.workflow.workflow INFO executing verb merge_graphs
00:10:51,641 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
00:10:51,827 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
00:10:51,828 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
00:10:51,861 datashaper.workflow.workflow INFO executing verb summarize_descriptions
00:10:52,913 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
00:10:53,94 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
00:10:53,95 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
00:10:53,126 datashaper.workflow.workflow INFO executing verb cluster_graph
00:10:53,679 datashaper.workflow.workflow INFO executing verb select
00:10:53,683 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
00:10:53,908 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
00:10:53,909 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:10:53,949 datashaper.workflow.workflow INFO executing verb unpack_graph
00:10:54,195 datashaper.workflow.workflow INFO executing verb rename
00:10:54,207 datashaper.workflow.workflow INFO executing verb select
00:10:54,221 datashaper.workflow.workflow INFO executing verb dedupe
00:10:54,235 datashaper.workflow.workflow INFO executing verb rename
00:10:54,250 datashaper.workflow.workflow INFO executing verb filter
00:10:54,300 datashaper.workflow.workflow INFO executing verb text_split
00:10:54,326 datashaper.workflow.workflow INFO executing verb drop
00:10:54,342 datashaper.workflow.workflow INFO executing verb merge
00:10:54,487 datashaper.workflow.workflow INFO executing verb text_embed
00:10:54,489 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
00:10:55,97 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
00:10:55,97 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
00:10:55,164 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 405 inputs via 405 snippets using 26 batches. max_batch_size=16, max_tokens=8191
00:10:55,765 datashaper.workflow.workflow INFO executing verb drop
00:10:55,783 datashaper.workflow.workflow INFO executing verb filter
00:10:55,823 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
00:10:56,183 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
00:10:56,183 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:10:56,228 datashaper.workflow.workflow INFO executing verb layout_graph
00:10:57,432 datashaper.workflow.workflow INFO executing verb unpack_graph
00:10:57,862 datashaper.workflow.workflow INFO executing verb unpack_graph
00:10:58,235 datashaper.workflow.workflow INFO executing verb drop
00:10:58,254 datashaper.workflow.workflow INFO executing verb filter
00:10:58,353 datashaper.workflow.workflow INFO executing verb select
00:10:58,377 datashaper.workflow.workflow INFO executing verb rename
00:10:58,413 datashaper.workflow.workflow INFO executing verb join
00:10:58,468 datashaper.workflow.workflow INFO executing verb convert
00:10:58,575 datashaper.workflow.workflow INFO executing verb rename
00:10:58,581 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
00:10:58,895 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
00:10:58,909 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:10:58,973 datashaper.workflow.workflow INFO executing verb create_final_communities
00:10:59,741 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
00:11:00,57 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
00:11:00,62 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:11:00,77 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
00:11:00,147 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
00:11:00,570 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
00:11:00,584 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
00:11:00,867 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
00:11:00,868 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
00:11:00,885 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:11:00,894 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
00:11:00,995 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
00:11:01,62 datashaper.workflow.workflow INFO executing verb select
00:11:01,91 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
00:11:01,403 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
00:11:01,403 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
00:11:01,412 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
00:11:01,477 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
00:11:01,547 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
00:11:01,600 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
00:11:01,653 datashaper.workflow.workflow INFO executing verb prepare_community_reports
00:11:01,654 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 405
00:11:01,699 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 405
00:11:01,947 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 405
00:11:02,274 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 405
00:11:02,558 datashaper.workflow.workflow INFO executing verb create_community_reports
00:11:15,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:15,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.96899999998277. input_tokens=4861, output_tokens=899
00:11:15,884 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:15,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.296999999991385. input_tokens=7089, output_tokens=914
00:11:28,235 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:28,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.296999999991385. input_tokens=2150, output_tokens=666
00:11:28,815 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:28,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.98399999999674. input_tokens=2116, output_tokens=625
00:11:29,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:29,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.46799999999348. input_tokens=2368, output_tokens=631
00:11:30,71 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:30,73 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.23399999999674. input_tokens=2254, output_tokens=670
00:11:30,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:30,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.46899999998277. input_tokens=2470, output_tokens=763
00:11:30,342 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:30,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.547000000020489. input_tokens=2331, output_tokens=707
00:11:30,633 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:30,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.828999999997905. input_tokens=4368, output_tokens=858
00:11:30,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:30,808 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.921000000002095. input_tokens=3012, output_tokens=848
00:11:31,50 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:31,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.187999999994645. input_tokens=3210, output_tokens=853
00:11:31,547 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:31,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.577999999979511. input_tokens=4779, output_tokens=869
00:11:31,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:31,700 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.78100000001723. input_tokens=9552, output_tokens=867
00:11:32,517 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:32,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.60999999998603. input_tokens=3607, output_tokens=825
00:11:32,637 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:32,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.671999999991385. input_tokens=3334, output_tokens=943
00:11:32,997 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:33,1 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.046999999991385. input_tokens=3203, output_tokens=873
00:11:33,587 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:33,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.610000000015134. input_tokens=3807, output_tokens=906
00:11:33,685 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:33,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.671000000002095. input_tokens=9892, output_tokens=1001
00:11:34,399 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:34,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.53100000001723. input_tokens=6244, output_tokens=859
00:11:35,657 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:35,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.797000000020489. input_tokens=7467, output_tokens=933
00:11:35,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:35,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.062999999994645. input_tokens=3923, output_tokens=799
00:11:37,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:37,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.375. input_tokens=9838, output_tokens=822
00:11:43,229 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:43,232 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.405999999988126. input_tokens=2652, output_tokens=794
00:11:43,934 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:43,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.98399999999674. input_tokens=3623, output_tokens=827
00:11:44,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:44,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.57799999997951. input_tokens=3030, output_tokens=827
00:11:46,797 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:46,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.85999999998603. input_tokens=3918, output_tokens=923
00:11:53,969 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:53,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.969000000011874. input_tokens=2122, output_tokens=546
00:11:54,730 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:54,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.75. input_tokens=2102, output_tokens=535
00:11:55,992 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:55,994 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.030999999988126. input_tokens=2225, output_tokens=653
00:11:56,86 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:56,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.078000000008615. input_tokens=3006, output_tokens=736
00:11:57,31 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:57,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.109000000025844. input_tokens=2692, output_tokens=819
00:11:57,296 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:57,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.327999999979511. input_tokens=4010, output_tokens=818
00:11:58,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:58,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.062999999994645. input_tokens=3289, output_tokens=877
00:11:58,127 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:58,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.10899999999674. input_tokens=3617, output_tokens=807
00:11:58,241 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:58,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.280999999988126. input_tokens=2277, output_tokens=743
00:11:58,610 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:58,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.671999999991385. input_tokens=2636, output_tokens=776
00:11:59,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:59,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.312999999994645. input_tokens=3956, output_tokens=879
00:11:59,466 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:59,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.51600000000326. input_tokens=2217, output_tokens=728
00:11:59,675 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:59,680 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.625. input_tokens=4870, output_tokens=871
00:11:59,752 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:59,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.844000000011874. input_tokens=6306, output_tokens=1004
00:11:59,778 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:59,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.828000000008615. input_tokens=7801, output_tokens=951
00:12:00,422 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:00,425 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.437999999994645. input_tokens=5216, output_tokens=943
00:12:00,495 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:00,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.5. input_tokens=3908, output_tokens=825
00:12:00,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:00,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.484000000025844. input_tokens=3556, output_tokens=809
00:12:00,620 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:00,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.60899999999674. input_tokens=3449, output_tokens=888
00:12:01,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:01,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.812999999994645. input_tokens=6470, output_tokens=902
00:12:02,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:02,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.155999999988126. input_tokens=8289, output_tokens=983
00:12:02,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:02,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.5. input_tokens=5920, output_tokens=996
00:12:03,2 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:03,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.953000000008615. input_tokens=4467, output_tokens=1009
00:12:03,462 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:03,462 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:03,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:03,474 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:03,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:03,495 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:03,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:03,504 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:03,523 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:03,524 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:03,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:03,531 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:03,548 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:03,549 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:04,878 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:04,878 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:04,982 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:04,983 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:05,118 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:05,119 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:05,145 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:05,146 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:05,241 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:05,242 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:05,316 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:05,317 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:05,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:05,353 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:07,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:07,600 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:07,681 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:07,682 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:07,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:07,899 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:07,903 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:07,905 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:08,209 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:08,209 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:09,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:09,327 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:11,372 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:11,373 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:13,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:13,255 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:13,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:13,410 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:14,212 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:14,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.969000000011874. input_tokens=2854, output_tokens=794
00:12:14,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:14,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.375. input_tokens=2340, output_tokens=793
00:12:14,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:14,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.60899999999674. input_tokens=2612, output_tokens=811
00:12:15,33 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:15,34 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:15,712 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:15,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.437999999994645. input_tokens=6250, output_tokens=1004
00:12:15,719 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:15,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.5. input_tokens=2971, output_tokens=823
00:12:18,38 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:18,39 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:20,143 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:20,144 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:22,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:22,427 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:24,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:24,135 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:24,484 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:24,484 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:27,148 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:27,148 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:30,69 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:30,71 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 8.53100000001723. input_tokens=2060, output_tokens=521
00:12:32,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:32,429 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:33,623 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:33,624 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:34,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:34,291 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:36,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:36,310 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:39,403 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:39,404 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:42,584 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:42,585 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:42,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:12:42,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 13.10899999999674. input_tokens=8939, output_tokens=913
00:12:43,752 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:43,752 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:44,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:44,435 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:46,498 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:46,499 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:52,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:52,801 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:53,868 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:53,869 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:12:54,554 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:12:54,554 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:13:03,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:13:03,84 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:13:03,996 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:13:03,997 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:13:04,686 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:13:04,687 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:13:09,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:13:09,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 13.14100000000326. input_tokens=6738, output_tokens=894
00:13:11,773 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:13:11,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 22.359000000025844. input_tokens=6962, output_tokens=809
00:13:14,229 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:13:14,230 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:13:14,230 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
00:13:14,242 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
00:13:14,242 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 4
00:13:14,823 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:13:14,824 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:13:14,824 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
00:13:14,828 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
00:13:14,828 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 10
00:13:25,357 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:13:25,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 12.25. input_tokens=7327, output_tokens=1008
00:13:25,404 datashaper.workflow.workflow INFO executing verb window
00:13:25,406 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
00:13:25,679 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
00:13:25,691 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
00:13:25,749 datashaper.workflow.workflow INFO executing verb unroll
00:13:25,775 datashaper.workflow.workflow INFO executing verb select
00:13:25,796 datashaper.workflow.workflow INFO executing verb rename
00:13:25,819 datashaper.workflow.workflow INFO executing verb join
00:13:25,846 datashaper.workflow.workflow INFO executing verb aggregate_override
00:13:25,871 datashaper.workflow.workflow INFO executing verb join
00:13:25,899 datashaper.workflow.workflow INFO executing verb rename
00:13:25,922 datashaper.workflow.workflow INFO executing verb convert
00:13:25,975 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
00:13:26,186 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
00:13:26,186 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
00:13:26,246 datashaper.workflow.workflow INFO executing verb rename
00:13:26,249 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
00:13:26,349 graphrag.index.cli INFO All workflows completed successfully.
00:13:43,964 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
00:13:43,967 graphrag.index.cli INFO Starting pipeline run for: 20241202-001343, dryrun=False
00:13:43,968 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
00:13:43,970 graphrag.index.create_pipeline_config INFO skipping workflows 
00:13:43,970 graphrag.index.run.run INFO Running pipeline
00:13:43,970 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
00:13:43,971 graphrag.index.input.load_input INFO loading input from root_dir=input
00:13:43,971 graphrag.index.input.load_input INFO using file storage for input
00:13:43,973 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
00:13:43,973 graphrag.index.input.text INFO found text files from input, found [('1_copy0.txt', {}), ('1_copy1.txt', {}), ('1_copy2.txt', {}), ('1_copy3.txt', {}), ('1_copy4.txt', {}), ('2_copy0.txt', {}), ('2_copy1.txt', {}), ('2_copy2.txt', {}), ('2_copy3.txt', {}), ('2_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
00:13:43,991 graphrag.index.input.text INFO Found 11 files, loading 11
00:13:43,993 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
00:13:43,993 graphrag.index.run.run INFO Final # of rows loaded: 11
00:13:44,115 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
00:13:44,121 datashaper.workflow.workflow INFO executing verb orderby
00:13:44,125 datashaper.workflow.workflow INFO executing verb zip
00:13:44,129 datashaper.workflow.workflow INFO executing verb aggregate_override
00:13:44,135 datashaper.workflow.workflow INFO executing verb chunk
00:13:44,433 datashaper.workflow.workflow INFO executing verb select
00:13:44,438 datashaper.workflow.workflow INFO executing verb unroll
00:13:44,444 datashaper.workflow.workflow INFO executing verb rename
00:13:44,449 datashaper.workflow.workflow INFO executing verb genid
00:13:44,461 datashaper.workflow.workflow INFO executing verb unzip
00:13:44,467 datashaper.workflow.workflow INFO executing verb copy
00:13:44,473 datashaper.workflow.workflow INFO executing verb filter
00:13:44,486 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
00:13:44,651 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
00:13:44,652 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:13:44,685 datashaper.workflow.workflow INFO executing verb entity_extract
00:13:44,698 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
00:13:45,108 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
00:13:45,108 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
00:13:45,515 datashaper.workflow.workflow INFO executing verb merge_graphs
00:13:45,661 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
00:13:45,866 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
00:13:45,866 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
00:13:45,895 datashaper.workflow.workflow INFO executing verb summarize_descriptions
00:13:48,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:13:48,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7030000000086147. input_tokens=486, output_tokens=189
00:13:48,769 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
00:13:48,922 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
00:13:48,922 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
00:13:48,951 datashaper.workflow.workflow INFO executing verb cluster_graph
00:13:49,428 datashaper.workflow.workflow INFO executing verb select
00:13:49,439 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
00:13:49,637 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
00:13:49,638 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:13:49,676 datashaper.workflow.workflow INFO executing verb unpack_graph
00:13:49,886 datashaper.workflow.workflow INFO executing verb rename
00:13:49,897 datashaper.workflow.workflow INFO executing verb select
00:13:49,909 datashaper.workflow.workflow INFO executing verb dedupe
00:13:49,922 datashaper.workflow.workflow INFO executing verb rename
00:13:49,933 datashaper.workflow.workflow INFO executing verb filter
00:13:49,965 datashaper.workflow.workflow INFO executing verb text_split
00:13:49,986 datashaper.workflow.workflow INFO executing verb drop
00:13:49,999 datashaper.workflow.workflow INFO executing verb merge
00:13:50,132 datashaper.workflow.workflow INFO executing verb text_embed
00:13:50,134 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
00:13:50,654 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
00:13:50,654 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
00:13:50,741 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 407 inputs via 407 snippets using 26 batches. max_batch_size=16, max_tokens=8191
00:13:51,713 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:13:51,747 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:13:51,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0940000000118744. input_tokens=1775, output_tokens=0
00:13:51,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1410000000032596. input_tokens=932, output_tokens=0
00:13:51,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:13:52,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2650000000139698. input_tokens=240, output_tokens=0
00:13:52,106 datashaper.workflow.workflow INFO executing verb drop
00:13:52,130 datashaper.workflow.workflow INFO executing verb filter
00:13:52,162 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
00:13:52,475 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
00:13:52,475 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:13:52,515 datashaper.workflow.workflow INFO executing verb layout_graph
00:13:53,673 datashaper.workflow.workflow INFO executing verb unpack_graph
00:13:53,951 datashaper.workflow.workflow INFO executing verb unpack_graph
00:13:54,190 datashaper.workflow.workflow INFO executing verb filter
00:13:54,245 datashaper.workflow.workflow INFO executing verb drop
00:13:54,260 datashaper.workflow.workflow INFO executing verb select
00:13:54,277 datashaper.workflow.workflow INFO executing verb rename
00:13:54,293 datashaper.workflow.workflow INFO executing verb join
00:13:54,319 datashaper.workflow.workflow INFO executing verb convert
00:13:54,371 datashaper.workflow.workflow INFO executing verb rename
00:13:54,374 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
00:13:54,576 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
00:13:54,583 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:13:54,626 datashaper.workflow.workflow INFO executing verb create_final_communities
00:13:55,129 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
00:13:55,332 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
00:13:55,332 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
00:13:55,349 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:13:55,397 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
00:13:55,671 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
00:13:55,681 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
00:13:55,893 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
00:13:55,894 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:13:55,903 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
00:13:55,962 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
00:13:56,43 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
00:13:56,121 datashaper.workflow.workflow INFO executing verb select
00:13:56,125 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
00:13:56,482 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
00:13:56,483 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
00:13:56,494 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
00:13:56,557 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
00:13:56,619 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
00:13:56,686 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
00:13:56,729 datashaper.workflow.workflow INFO executing verb prepare_community_reports
00:13:56,730 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 407
00:13:56,835 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 407
00:13:57,17 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 407
00:13:57,236 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 407
00:13:57,424 datashaper.workflow.workflow INFO executing verb create_community_reports
00:13:58,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:13:58,578 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:13:58,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:13:58,600 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:00,593 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:00,593 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:00,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:00,824 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:03,638 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:03,638 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:03,675 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:03,675 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:03,910 graphrag.index.cli INFO Logging enabled at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output\indexing-engine.log
00:14:03,917 graphrag.index.cli INFO Starting pipeline run for: 20241202-001403, dryrun=False
00:14:03,919 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 200000,
        "requests_per_minute": 500,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Others\\USC\\CSCI 566\\graphrag\\csci_566_project\\ragtest_poison\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 200000,
            "requests_per_minute": 500,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
00:14:03,924 graphrag.index.create_pipeline_config INFO skipping workflows 
00:14:03,925 graphrag.index.run.run INFO Running pipeline
00:14:03,925 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\output
00:14:03,926 graphrag.index.input.load_input INFO loading input from root_dir=input
00:14:03,926 graphrag.index.input.load_input INFO using file storage for input
00:14:03,929 graphrag.index.storage.file_pipeline_storage INFO search C:\Others\USC\CSCI 566\graphrag\csci_566_project\ragtest_poison\input for files matching .*\.txt$
00:14:03,931 graphrag.index.input.text INFO found text files from input, found [('1_copy0.txt', {}), ('1_copy1.txt', {}), ('1_copy2.txt', {}), ('1_copy3.txt', {}), ('1_copy4.txt', {}), ('3_copy0.txt', {}), ('3_copy1.txt', {}), ('3_copy2.txt', {}), ('3_copy3.txt', {}), ('3_copy4.txt', {}), ("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", {})]
00:14:03,964 graphrag.index.input.text INFO Found 11 files, loading 11
00:14:03,966 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
00:14:03,966 graphrag.index.run.run INFO Final # of rows loaded: 11
00:14:04,171 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
00:14:04,178 datashaper.workflow.workflow INFO executing verb orderby
00:14:04,183 datashaper.workflow.workflow INFO executing verb zip
00:14:04,189 datashaper.workflow.workflow INFO executing verb aggregate_override
00:14:04,198 datashaper.workflow.workflow INFO executing verb chunk
00:14:04,637 datashaper.workflow.workflow INFO executing verb select
00:14:04,644 datashaper.workflow.workflow INFO executing verb unroll
00:14:04,652 datashaper.workflow.workflow INFO executing verb rename
00:14:04,658 datashaper.workflow.workflow INFO executing verb genid
00:14:04,673 datashaper.workflow.workflow INFO executing verb unzip
00:14:04,681 datashaper.workflow.workflow INFO executing verb copy
00:14:04,689 datashaper.workflow.workflow INFO executing verb filter
00:14:04,709 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
00:14:04,905 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
00:14:04,911 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:14:04,946 datashaper.workflow.workflow INFO executing verb entity_extract
00:14:04,964 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
00:14:05,347 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=200000, RPM=500
00:14:05,347 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
00:14:07,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:07,909 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:08,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:08,529 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:09,729 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:14:09,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.062000000005355. input_tokens=2101, output_tokens=264
00:14:09,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:09,844 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:10,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:14:10,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.202999999979511. input_tokens=2652, output_tokens=782
00:14:11,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:11,412 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:12,214 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:14:12,216 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.860000000015134. input_tokens=4161, output_tokens=892
00:14:16,683 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:16,684 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:17,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:17,584 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:18,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:14:18,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.905999999988126. input_tokens=34, output_tokens=360
00:14:18,649 datashaper.workflow.workflow INFO executing verb merge_graphs
00:14:18,785 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
00:14:18,959 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
00:14:18,959 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
00:14:18,986 datashaper.workflow.workflow INFO executing verb summarize_descriptions
00:14:19,236 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:19,238 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:19,471 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:19,472 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:19,480 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:19,481 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:19,518 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:19,530 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:19,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:19,706 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:20,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:20,474 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:20,803 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:20,804 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:21,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:21,151 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:21,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:21,301 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:21,464 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:21,465 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:23,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:23,522 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:23,622 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:23,623 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:23,961 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:23,962 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:23,962 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:23,963 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:26,563 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:14:26,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 2 retries took 3.3429999999934807. input_tokens=963, output_tokens=298
00:14:26,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:26,887 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:27,751 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:27,752 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:28,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:28,55 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:28,106 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:28,106 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:28,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:28,606 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:29,21 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:29,21 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:36,771 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:36,771 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:37,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:37,96 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:37,165 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:37,165 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:37,333 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:37,334 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:37,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:14:37,909 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:14:38,632 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:14:38,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 4 retries took 2.4539999999979045. input_tokens=273, output_tokens=156
00:14:59,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:14:59,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 12.312999999994645. input_tokens=6780, output_tokens=869
00:15:01,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:15:01,445 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 13.53100000001723. input_tokens=9895, output_tokens=841
00:15:02,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,762 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,812 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,813 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,853 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,857 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,861 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,862 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,864 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,865 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,895 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,896 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,912 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,912 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,913 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,914 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,921 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,922 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,934 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,935 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,948 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,949 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,959 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,974 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,975 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,980 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,989 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:02,989 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:02,990 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:03,14 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:03,14 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:03,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:03,29 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:03,947 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:03,949 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,17 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,17 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,53 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,54 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,97 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,125 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,125 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,161 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,166 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,170 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,170 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,202 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,210 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,316 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,329 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,330 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,419 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,420 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,422 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,423 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,625 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,626 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,630 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,737 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,840 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,842 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,972 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:04,975 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:04,976 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:05,94 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:05,95 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,506 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,507 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,598 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,599 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,599 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,612 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,613 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,675 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,676 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,679 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,680 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,680 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,797 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,798 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,842 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,913 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,914 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,916 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,918 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:06,965 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:06,966 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:07,118 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:07,119 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:07,551 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:07,552 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:07,808 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:07,809 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:09,611 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:09,612 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:10,816 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:10,817 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:11,125 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:11,126 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:11,954 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:11,955 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:12,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:12,605 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:13,111 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:13,112 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:14,537 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:14,538 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:15,532 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:15,533 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:16,334 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:16,334 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:16,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:15:16,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.297000000020489. input_tokens=2660, output_tokens=835
00:15:17,609 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:17,609 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:19,62 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:19,63 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:19,514 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:19,515 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:20,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:20,502 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:21,54 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:21,55 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:22,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:22,80 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:22,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:22,325 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:23,618 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:15:23,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 9.578000000008615. input_tokens=2181, output_tokens=666
00:15:23,659 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:23,660 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:23,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:23,876 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:25,737 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:25,738 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:26,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:26,520 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:27,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:27,608 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:28,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:28,851 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:29,649 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:29,650 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:32,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:32,74 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:32,851 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:32,852 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:33,197 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:33,197 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:33,982 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:33,983 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:34,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:34,495 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:35,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:35,855 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:37,63 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:37,64 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:37,723 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:37,723 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:38,710 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:38,711 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:40,942 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:40,942 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:42,195 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:42,195 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:42,971 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:42,972 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:43,332 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:43,333 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:43,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:15:43,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 12.937000000005355. input_tokens=5516, output_tokens=862
00:15:44,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:44,106 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:44,644 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:44,645 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:45,986 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:45,987 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:47,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:47,174 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:47,430 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:15:47,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 6 retries took 7.76500000001397. input_tokens=2122, output_tokens=592
00:15:47,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:47,858 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:48,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:48,848 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:50,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:50,504 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:52,37 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:52,37 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:53,88 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:53,89 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:53,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:53,141 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:54,204 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:54,205 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:54,765 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:54,765 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:55,546 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:55,547 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:58,0 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:58,1 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:15:58,974 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:15:58,975 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:00,232 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:00,233 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:01,692 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:01,693 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:02,845 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:02,846 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:03,228 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:03,229 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:04,49 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:04,49 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:04,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:04,328 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:05,722 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:05,723 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:07,786 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:07,788 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:09,53 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:09,53 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:10,279 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:10,279 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:10,351 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:16:10,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 13.375. input_tokens=4970, output_tokens=910
00:16:11,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:11,354 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:12,873 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:12,874 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:13,353 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:13,354 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:13,354 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
00:16:13,366 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
00:16:13,366 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 57
00:16:15,197 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:15,198 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:15,198 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
00:16:15,202 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
00:16:15,202 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 44
00:16:15,839 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:15,839 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:15,839 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 196886, Requested 4488. Please try again in 412ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
00:16:15,844 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
00:16:15,844 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 53
00:16:17,136 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:17,137 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:18,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:18,578 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:19,193 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:19,194 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:19,195 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
00:16:19,200 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
00:16:19,200 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 54
00:16:19,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:16:19,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 14.469000000011874. input_tokens=2331, output_tokens=602
00:16:20,428 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:20,428 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:21,490 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:21,490 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:21,490 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 195363, Requested 5678. Please try again in 312ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
00:16:21,496 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
00:16:21,496 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 45
00:16:22,343 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:16:22,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 8.28200000000652. input_tokens=2550, output_tokens=662
00:16:25,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:25,579 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:27,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:27,136 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:28,586 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:28,587 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:29,815 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:29,816 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:29,816 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}
00:16:29,821 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
00:16:29,821 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 56
00:16:30,576 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:30,577 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:30,577 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\resources\chat\completions.py", line 1661, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1533, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\andyp\anaconda3\envs\graphrag-local\Lib\site-packages\openai\_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-WOx4zVSIr5TxCLinW96Lvbcd on tokens per min (TPM): Limit 200000, Used 193684, Requested 6540. Please try again in 67ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
00:16:30,581 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
00:16:30,581 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 51
00:16:35,628 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:16:35,631 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 11.78200000000652. input_tokens=5454, output_tokens=849
00:16:35,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:35,848 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:37,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:37,320 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:38,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:38,719 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:46,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:46,48 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:47,483 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:47,484 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:49,497 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:16:49,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 2 retries took 16.828000000008615. input_tokens=7367, output_tokens=875
00:16:56,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:16:56,256 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:16:56,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:16:56,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 15.21899999998277. input_tokens=9861, output_tokens=1031
00:16:59,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:16:59,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 11.094000000011874. input_tokens=4385, output_tokens=837
00:17:15,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:17:15,383 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 7 retries took 17.90700000000652. input_tokens=6882, output_tokens=826
00:17:18,843 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:17:18,846 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 12.578000000008615. input_tokens=4873, output_tokens=837
00:17:19,249 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,250 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,328 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,329 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,347 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,348 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,351 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,352 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,404 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,405 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,420 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,421 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,426 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,428 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,429 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,430 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,431 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,444 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,444 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,448 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,449 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,456 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,456 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,460 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,461 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,465 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,483 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,491 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,493 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,494 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,495 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,497 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,498 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,577 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,578 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,611 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,612 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,614 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,615 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,706 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,707 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:19,977 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:19,979 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:20,741 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:20,742 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:21,359 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:21,360 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:22,0 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:22,1 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:22,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:22,859 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:24,366 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:24,368 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:25,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:25,47 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:25,712 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:25,713 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:26,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:26,508 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:27,312 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:27,313 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:28,149 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:28,150 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:28,775 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:28,775 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:29,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:29,197 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:29,900 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:29,901 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:30,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:17:30,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.875. input_tokens=2692, output_tokens=777
00:17:30,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:30,919 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:31,672 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:31,673 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:32,477 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:32,478 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:32,990 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:32,991 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:33,774 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:33,775 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:34,588 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:34,588 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:35,393 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:35,394 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:35,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:17:35,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 12.094000000011874. input_tokens=2217, output_tokens=782
00:17:36,205 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:36,207 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:36,929 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:36,929 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:37,832 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:37,833 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:38,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:38,436 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:39,312 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:39,313 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:40,23 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:40,24 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:40,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:40,834 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:41,704 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:41,705 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:42,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:42,410 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:42,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:42,874 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:43,722 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:43,723 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
00:17:44,407 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:17:44,407 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
